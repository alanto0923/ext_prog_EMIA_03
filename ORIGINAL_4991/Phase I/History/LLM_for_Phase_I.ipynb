{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Am0RlC30KL"
      },
      "source": [
        "# Data Acquisition (Research Papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krQd3D9r0oAt",
        "outputId": "579c9047-755d-4274-9202-2fece4273164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
            "Requirement already satisfied: xmltodict in /opt/anaconda3/lib/python3.11/site-packages (0.14.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests xmltodict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBMVU7IF3xMX",
        "outputId": "b32d29f2-1c64-4e36-a44d-c69ee669fb20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
            "Requirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.11/site-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.8.4)\n",
            "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
            "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.11/site-packages (0.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (2.0.9)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
            "Requirement already satisfied: pyfume in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
            "Requirement already satisfied: simpful==2.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
            "Requirement already satisfied: fst-pso==1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
            "Requirement already satisfied: miniful in /opt/anaconda3/lib/python3.11/site-packages (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests PyPDF2 nltk spacy gensim textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tbtXhafG3zUI"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import xmltodict\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from gensim import corpora, models\n",
        "import sqlite3\n",
        "from textblob import TextBlob  # Import TextBlob at the top level\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Load spaCy model ONCE, at the top level:\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    os.system(\"python -m spacy download en_core_web_sm\") # Download if not found\n",
        "    nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search_keywords = [\"alpha factors\", 'alpha generation', 'alpha mining', 'factor investing', 'formulaic alphas', 'alpha portfolio construction', 'stock price prediction', 'artifical intelligence trading', 'AI stock selection', 'large language models finance', 'reinforcement learning portfolio', 'portfolio optimization', 'trading strategies', 'algorithmic trading', 'quantitative trading', 'mixture of experts trading']\n",
        "len(search_keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c_eI7j-4Dsm",
        "outputId": "318d3612-5976-49ce-f4d8-22e07c3f5762"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 235\u001b[0m\n\u001b[1;32m    232\u001b[0m all_papers \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Store all papers from all searches\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m search_keywords:\n\u001b[0;32m--> 235\u001b[0m     papers \u001b[38;5;241m=\u001b[39m search_arxiv_papers([keyword], max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Search for ONE keyword\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m papers:\n\u001b[1;32m    238\u001b[0m         all_papers\u001b[38;5;241m.\u001b[39mextend(papers)  \u001b[38;5;66;03m# Add the new papers to the combined list\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[11], line 35\u001b[0m, in \u001b[0;36msearch_arxiv_papers\u001b[0;34m(query, max_results, retry_count)\u001b[0m\n\u001b[1;32m     33\u001b[0m paper \u001b[38;5;241m=\u001b[39m parse_arxiv_entry(entry)\n\u001b[1;32m     34\u001b[0m pdf_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://arxiv.org/pdf/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 35\u001b[0m paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_path\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m download_pdf(pdf_url, paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_path\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     38\u001b[0m     paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_text_from_pdf(paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
            "Cell \u001b[0;32mIn[11], line 75\u001b[0m, in \u001b[0;36mdownload_pdf\u001b[0;34m(pdf_url, paper_id)\u001b[0m\n\u001b[1;32m     72\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(pdf_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdf_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(pdf_url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Stream for large files\u001b[39;00m\n\u001b[1;32m     76\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:724\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     gen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresolve_redirects(r, request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 724\u001b[0m     history \u001b[38;5;241m=\u001b[39m [resp \u001b[38;5;28;01mfor\u001b[39;00m resp \u001b[38;5;129;01min\u001b[39;00m gen]\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    726\u001b[0m     history \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:265\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[1;32m    266\u001b[0m         req,\n\u001b[1;32m    267\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    268\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    269\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[1;32m    270\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[1;32m    271\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    272\u001b[0m         allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs,\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcookies, prepared_request, resp\u001b[38;5;241m.\u001b[39mraw)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    679\u001b[0m     )\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:716\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 716\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    717\u001b[0m     conn,\n\u001b[1;32m    718\u001b[0m     method,\n\u001b[1;32m    719\u001b[0m     url,\n\u001b[1;32m    720\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    721\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    722\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    723\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    724\u001b[0m )\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:468\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    463\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    464\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 468\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:463\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:1390\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1390\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[1;32m   1391\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1392\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/http/client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import xmltodict\n",
        "import PyPDF2  # For PDF processing (install: pip install PyPDF2)\n",
        "import io # For in-memory file handling\n",
        "\n",
        "ARXIV_API_URL = \"http://export.arxiv.org/api/query\"\n",
        "\n",
        "def search_arxiv_papers(query, max_results=50, retry_count=3):\n",
        "    \"\"\"Searches arXiv and downloads PDFs.\"\"\"\n",
        "\n",
        "    params = {  # Put params here\n",
        "        \"search_query\": query,\n",
        "        \"start\": 0,\n",
        "        \"max_results\": max_results,\n",
        "        \"id_list\": \"\"\n",
        "    }\n",
        "    results = []  # Initialize results outside the try block\n",
        "\n",
        "    try:\n",
        "        for attempt in range(retry_count):\n",
        "            response = requests.get(ARXIV_API_URL, params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            try:\n",
        "                xml_dict = xmltodict.parse(response.content)\n",
        "                entries = xml_dict.get('feed', {}).get('entry', [])\n",
        "\n",
        "                # NOW you can iterate over entries:\n",
        "                for entry in entries:\n",
        "                    paper = parse_arxiv_entry(entry)\n",
        "                    pdf_url = f\"https://arxiv.org/pdf/{paper['id']}.pdf\"\n",
        "                    paper['pdf_path'] = download_pdf(pdf_url, paper['id'])\n",
        "\n",
        "                    if paper['pdf_path']:\n",
        "                        paper['full_text'] = extract_text_from_pdf(paper['pdf_path'])\n",
        "                    if paper.get('full_text'):\n",
        "\n",
        "                        paper['keywords'] = extract_keywords(paper['full_text'], method=\"spacy\") # Use spacy\n",
        "                        paper['summary'] = summarize_text(paper['full_text'])\n",
        "                        paper['sentiment'] = analyze_sentiment(paper['full_text'])\n",
        "\n",
        "                        print(f\"Keywords: {paper['keywords']}\")\n",
        "                        print(f\"Summary: {paper['summary']}\")\n",
        "                        print(f\"Sentiment: {paper['sentiment']}\")\n",
        "\n",
        "                        store_in_db(paper) # Store the data in the database\n",
        "\n",
        "                    results.append(paper)\n",
        "\n",
        "                return results  # Return results after successful processing\n",
        "\n",
        "            except (xmltodict.expat.ExpatError, KeyError) as e:\n",
        "                print(f\"Error parsing XML response: {e}. Retrying...\")\n",
        "                time.sleep(2**attempt)\n",
        "                continue  # Retry the request\n",
        "\n",
        "        print(\"Error: Failed to get valid response after multiple retries.\")\n",
        "        return [] # Return empty list if retries fail\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error searching arXiv: {e}\")\n",
        "        return []\n",
        "\n",
        "def download_pdf(pdf_url, paper_id):\n",
        "    \"\"\"Downloads a PDF given its URL and saves it locally.\"\"\"\n",
        "    try:\n",
        "      # Create directory if doesn't exist\n",
        "      pdf_dir = \"arxiv_pdfs\"\n",
        "      os.makedirs(pdf_dir, exist_ok=True)\n",
        "      file_path = os.path.join(pdf_dir, f\"{paper_id}.pdf\")\n",
        "\n",
        "      response = requests.get(pdf_url, stream=True)  # Stream for large files\n",
        "      response.raise_for_status()\n",
        "\n",
        "      with open(file_path, \"wb\") as f:\n",
        "          for chunk in response.iter_content(chunk_size=8192):\n",
        "              f.write(chunk)\n",
        "      print(f\"Downloaded PDF: {file_path}\")\n",
        "      return file_path\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text content from a PDF.\"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text()\n",
        "            return text\n",
        "    except (FileNotFoundError, PyPDF2.errors.PdfReadError) as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_arxiv_entry(entry):\n",
        "    \"\"\"Parses an individual arXiv entry (from XML) into a dictionary.\"\"\"\n",
        "    paper = {}\n",
        "    paper['id'] = entry.get('id', '').split('/')[-1] if entry.get('id') else None # Extract ID\n",
        "    paper['title'] = entry.get('title', '').replace('\\n', ' ') # Remove newline\n",
        "    paper['abstract'] = entry.get('summary', '').replace('\\n', ' ') # Remove newline\n",
        "    paper['authors'] = [author.get('name') for author in entry.get('author', []) if isinstance(author, dict)] # Handle multiple authors\n",
        "    paper['categories'] = [cat.get('term') for cat in entry.get('category', []) if isinstance(cat, dict)]\n",
        "    paper['journal-ref'] = entry.get('journal-ref')\n",
        "    paper['doi'] = entry.get('doi')\n",
        "    paper['submitted'] = entry.get('published') # Use published for submission date\n",
        "\n",
        "    return paper\n",
        "\n",
        "def extract_keywords(text, method=\"nltk\"):  # Added method parameter\n",
        "    \"\"\"Extracts keywords from text (NLTK or spaCy).\"\"\"\n",
        "    if text is None:\n",
        "        return []\n",
        "\n",
        "    text = text.lower() # Lowercasing for both methods\n",
        "    if method == \"nltk\":\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_words = [w for w in word_tokens if not w in stop_words and w.isalnum()]\n",
        "        return filtered_words[:20]  # Basic NLTK keyword extraction\n",
        "\n",
        "    elif method == \"spacy\":\n",
        "        doc = nlp(text)\n",
        "        keywords = [token.lemma_ for token in doc if token.pos_ in (\"NOUN\", \"PROPN\", \"ADJ\") and not token.is_stop and token.is_alpha]\n",
        "        return keywords[:20]  # More advanced spaCy keyword extraction\n",
        "\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "def summarize_text(text, num_sentences=3):\n",
        "    \"\"\"Summarizes text (basic example using sentence splitting).\"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    if len(sentences) <= num_sentences:\n",
        "        return text\n",
        "\n",
        "    # Very basic summarization: take the first num_sentences\n",
        "    summary = \"\".join([sent.text for sent in sentences[:num_sentences]])\n",
        "    return summary\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"Analyzes sentiment (basic example using TextBlob).\"\"\"\n",
        "    if text is None:\n",
        "        return \"N/A\" # Handle None case\n",
        "\n",
        "    from textblob import TextBlob # Install: pip install textblob\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity  # Returns a score between -1 and 1\n",
        "\n",
        "def create_topics(texts, num_topics=5):\n",
        "    \"\"\"Creates topics using LDA.\"\"\"\n",
        "    if not texts:\n",
        "        return []\n",
        "\n",
        "    tokenized_texts = [[word for word in text.lower().split() if word.isalnum() and word not in stopwords.words('english')] for text in texts if text]\n",
        "\n",
        "    dictionary = corpora.Dictionary(tokenized_texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "    topics = lda_model.show_topics()\n",
        "    return topics\n",
        "\n",
        "def store_in_db(paper_data):\n",
        "    conn = sqlite3.connect('arxiv_papers.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    # Define the columns (important for dynamic handling)\n",
        "    columns = [\n",
        "        'id', 'title', 'abstract', 'authors', 'categories', 'journal_ref', 'doi',\n",
        "        'submitted', 'pdf_path', 'full_text', 'keywords', 'summary', 'sentiment'\n",
        "    ]\n",
        "\n",
        "    # Create the table if it doesn't exist (only needs to be done once)\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS papers (\n",
        "            id TEXT PRIMARY KEY,\n",
        "            title TEXT,\n",
        "            abstract TEXT,\n",
        "            authors TEXT,\n",
        "            categories TEXT,\n",
        "            journal_ref TEXT,\n",
        "            doi TEXT,\n",
        "            submitted TEXT,\n",
        "            pdf_path TEXT,\n",
        "            full_text TEXT,\n",
        "            keywords TEXT,\n",
        "            summary TEXT,\n",
        "            sentiment REAL\n",
        "        )\n",
        "    ''')\n",
        "\n",
        "    try:\n",
        "        # 1. Build the VALUES part of the query dynamically\n",
        "        values_placeholders = \", \".join([\"?\"] * len(columns))  # \"?, ?, ..., ?\"\n",
        "\n",
        "        # 2. Extract the values in the correct order, handling missing data\n",
        "        values = []\n",
        "        for col in columns:\n",
        "            value = paper_data.get(col)\n",
        "            if col in ('authors', 'categories', 'keywords') and value is not None: #For lists\n",
        "              value = json.dumps(value)\n",
        "            elif value is None:\n",
        "              value = 'N/A' # Handle None case\n",
        "            values.append(value)\n",
        "\n",
        "        # 3. Construct and execute the query\n",
        "        query = f\"INSERT INTO papers ({', '.join(columns)}) VALUES ({values_placeholders})\"\n",
        "        cursor.execute(query, tuple(values))  # Important: tuple(values)\n",
        "\n",
        "        conn.commit()\n",
        "    except sqlite3.IntegrityError:\n",
        "        print(f\"Paper with id {paper_data.get('id')} already exists. Skipping.\")\n",
        "    except sqlite3.Error as e:  # Catch other potential database errors\n",
        "        print(f\"Database error: {e}\")\n",
        "        conn.rollback() # Rollback in case of error\n",
        "\n",
        "    conn.close()\n",
        "\n",
        "search_keywords = [\"alpha factors\", 'alpha generation', 'alpha mining', 'factor investing', 'formulaic alphas', 'alpha portfolio construction', 'stock price prediction', 'artifical intelligence trading', 'AI stock selection', 'large language models finance', 'reinforcement learning portfolio', 'portfolio optimization', 'trading strategies', 'algorithmic trading', 'quantitative trading', 'mixture of experts trading']\n",
        "# search_keywords = [\"financial time series prediction\", 'mixture of experts', 'portfolio optimization', 'finance', 'stock selection', 'factor investing', 'deep reinforcement learning', 'trading', 'large language models', 'alpha generation', 'alpha mining']\n",
        "\n",
        "all_papers = []  # Store all papers from all searches\n",
        "\n",
        "for keyword in search_keywords:\n",
        "    papers = search_arxiv_papers([keyword], max_results=5)  # Search for ONE keyword\n",
        "    \n",
        "    if papers:\n",
        "        all_papers.extend(papers)  # Add the new papers to the combined list\n",
        "        for paper in papers: # process each paper\n",
        "            print(json.dumps(paper, indent=4))\n",
        "            if paper.get('full_text'):\n",
        "                print(f\"Full Text (First 200 characters of {len(paper['full_text'])}):\")\n",
        "                print(paper['full_text'][:200] + \"...\")  # Or even less\n",
        "                print()\n",
        "                print('Processing:')\n",
        "                text_file_path = os.path.join(\"arxiv_texts\", f\"{paper['id']}.txt\")  # New directory\n",
        "                os.makedirs(\"arxiv_texts\", exist_ok=True) # Create directory if doesn't exist\n",
        "                with open(text_file_path, \"w\", encoding=\"utf-8\") as text_file:  # UTF-8 encoding\n",
        "                    text_file.write(paper['full_text'])\n",
        "                print(f\"Full text saved to: {text_file_path}\")\n",
        "                # ... (Process paper data, including full text)\n",
        "\n",
        "else:\n",
        "    print(\"No papers found.\")\n",
        "\n",
        "# Example of topic modeling (after searching for papers):\n",
        "texts = [paper.get('full_text') for paper in papers if paper.get('full_text')]\n",
        "topics = create_topics(texts)\n",
        "print(\"\\nTopics:\")\n",
        "for topic in topics:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(all_papers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# First LLM Agent for Phase I (displayed a comprehensive set of alpha factors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
            "Requirement already satisfied: xmltodict in /opt/anaconda3/lib/python3.11/site-packages (0.14.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests xmltodict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.32.3)\n",
            "Requirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.11/site-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.8.4)\n",
            "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.11/site-packages (4.3.0)\n",
            "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.11/site-packages (0.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.24.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: FuzzyTM>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from gensim) (2.0.9)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
            "Requirement already satisfied: pyfume in /opt/anaconda3/lib/python3.11/site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
            "Requirement already satisfied: simpful==2.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
            "Requirement already satisfied: fst-pso==1.8.1 in /opt/anaconda3/lib/python3.11/site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
            "Requirement already satisfied: miniful in /opt/anaconda3/lib/python3.11/site-packages (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests PyPDF2 nltk spacy gensim textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tf-keras\n",
            "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: pip is looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /opt/anaconda3/lib/python3.11/site-packages (from tf-keras) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (68.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.9.0)\n",
            "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow<2.19,>=2.18->tf-keras)\n",
            "  Downloading numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m195.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.41.2)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.3.5)\n",
            "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
            "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, tf-keras\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "streamlit 1.30.0 requires numpy<2,>=1.19.3, but you have numpy 2.0.2 which is incompatible.\n",
            "streamlit 1.30.0 requires packaging<24,>=16.8, but you have packaging 24.2 which is incompatible.\n",
            "langchain 0.3.19 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
            "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.0.2 which is incompatible.\n",
            "mxnet 1.6.0 requires numpy<2.0.0,>1.16.0, but you have numpy 2.0.2 which is incompatible.\n",
            "rqfactor 1.3.18 requires markupsafe==2.0.1, but you have markupsafe 3.0.2 which is incompatible.\n",
            "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.2 which is incompatible.\n",
            "langchain-community 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "numba 0.59.0 requires numpy<1.27,>=1.22, but you have numpy 2.0.2 which is incompatible.\n",
            "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 2.0.2 which is incompatible.\n",
            "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.0.2 tf-keras-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import xmltodict\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from gensim import corpora, models\n",
        "import sqlite3\n",
        "from textblob import TextBlob  # Import TextBlob at the top level\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Load spaCy model ONCE, at the top level:\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    os.system(\"python -m spacy download en_core_web_sm\") # Download if not found\n",
        "    nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index loaded from storage.\n",
            "Raw Response (Before Cleanup): ```\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Momentum\",\n",
            "      \"name\": \"Price Momentum (14 days)\",\n",
            "      \"code\": \"((CLOSE - DELAY(CLOSE, 14)) / DELAY(CLOSE, 14))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Mean Reversion (20 days)\",\n",
            "      \"code\": \"(MEAN(CLOSE, 20) - CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"20-Day Volatility\",\n",
            "      \"code\": \"STD(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Fundamental\",\n",
            "      \"name\": \"Price-to-Earnings Ratio (P/E)\",\n",
            "      \"code\": \"(CLOSE / EPS)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Trading Volume\",\n",
            "      \"code\": \"VOLUME\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Quality\",\n",
            "      \"name\": \"Gross Profit Margin\",\n",
            "      \"code\": \"(GROSS_PROFIT / REVENUE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"GROWTH\",\n",
            "      \"name\": \"Earnings Growth Rate\",\n",
            "      \"code\": \"(EPS / DELAY(EPS,1) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Technical\",\n",
            "      \"name\": \"Moving Average (MA)\",\n",
            "      \"code\": \"SMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"GDP Growth Rate\",\n",
            "      \"code\": \"GDP - DELAY(GDP, 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Momentum\",\n",
            "      \"name\": \"Relative Strength Index (RSI)\",\n",
            "      \"code\": \"(100 - (100 / (1 + RS)) * (CLOSE / PREV_CLOSE))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Bollinger Bands\",\n",
            "      \"code\": \"STDEV(CLOSE, 20) + (MEAN(CLOSE, 20) * 2)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Standard Deviation of Returns\",\n",
            "      \"code\": \"(STD(RETURN, 20))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Fundamental\",\n",
            "      \"name\": \"Price-to-Book Ratio (P/B)\",\n",
            "      \"code\": \"(CLOSE / BOOK_VALUE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Average True Range (ATR)\",\n",
            "      \"code\": \"(ATR(CLOSE, 20))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Quality\",\n",
            "      \"name\": \"Dividend Yield\",\n",
            "      \"code\": \"(DIVIDEND / CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"GROWTH\",\n",
            "      \"name\": \"Price-to-Book Growth Rate\",\n",
            "      \"code\": \"(BOOK_VALUE / CLOSE) - (BOOK_VALUE / DELAY(BOOK_VALUE, 1))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Technical\",\n",
            "      \"name\": \"Bollinger Bands Momentum\",\n",
            "      \"code\": \"RS(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Inflation Rate\",\n",
            "      \"code\": \"(INFLATION_RATE * 100)\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import asyncio\n",
        "import json\n",
        "from llama_index.core import Settings, StorageContext, load_index_from_storage  # Import missing classes\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex, Document, SimpleDirectoryReader, Settings\n",
        "import fitz\n",
        "from PIL import Image\n",
        "import re\n",
        "import io\n",
        "import os  # Import os for directory checking\n",
        "import pandas as pd #For table handling if needed\n",
        "import glob\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from typing import List, Dict\n",
        "# ... other imports (fitz, vector database library, etc.)\n",
        "\n",
        "\n",
        "# Set global settings\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "Settings.llm = Ollama(model=\"llama3.2\", request_timeout=720.0)\n",
        "\n",
        "text_embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "\n",
        "def process_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    document_data = {\n",
        "        \"text\": \"\",\n",
        "        \"images\": [],\n",
        "        }\n",
        "\n",
        "    for page in doc:\n",
        "        document_data[\"text\"] += page.get_text()\n",
        "    \n",
        "    for page in doc: #Iterate through all pages.\n",
        "        for image in page.get_images(): # Get images without width/height\n",
        "            xref = image[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            if base_image: # Check if image extracted\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image_ext = base_image[\"ext\"]\n",
        "\n",
        "            try:\n",
        "                image = Image.open(io.BytesIO(image_bytes)) # Use PIL to resize\n",
        "                new_image = image.resize((2000, 2000)) # Resize with PIL\n",
        "                # Convert the resized image back to bytes (if needed for later use)\n",
        "                image_bytes_resized = io.BytesIO()\n",
        "                new_image.save(image_bytes_resized, format=image_ext.upper()) # Save in original format\n",
        "                image_bytes_resized = image_bytes_resized.getvalue()\n",
        "                document_data[\"images\"].append(image_bytes_resized) # Append image bytes\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing or resizing image: {e}\")\n",
        "                continue # Skip to the next image if there's an issue\n",
        "    return document_data\n",
        "    \n",
        "async def retrieve_documents(query, query_engine): # Add query_engine parameter\n",
        "    pdf_directory = \"arxiv_pdfs\"\n",
        "    pdf_paths = glob.glob(os.path.join(pdf_directory, \"*.pdf\"))\n",
        "    # pdf_paths = [\"arxiv_pdfs/2409.06289v1.pdf\"]\n",
        "    # pdf_paths = [\"arxiv_pdfs/2103.16196v2.pdf\", \"arxiv_pdfs/2308.00016v1.pdf\"] # Replace with your PDF file paths\n",
        "    documents = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        data = process_pdf(pdf_path) # Process each pdf file\n",
        "        # Create LlamaIndex Documents\n",
        "        text_document = Document(text=data[\"text\"], metadata={\"source\": pdf_path})\n",
        "        documents.append(text_document)\n",
        "        # Handle images\n",
        "        for image_bytes in data[\"images\"]:\n",
        "            image_document = Document(text=\"Image\", metadata={\"image_bytes\": image_bytes, \"source\": pdf_path})\n",
        "            documents.append(image_document)\n",
        "\n",
        "    response = await query_engine.aquery(query)\n",
        "    return response.source_nodes\n",
        "\n",
        "\n",
        "def get_multimodal_embeddings(data):\n",
        "    text_emb = text_embedding_model.encode(data[\"text\"])\n",
        "    image_embs = []\n",
        "    for image in data[\"images\"]:\n",
        "        inputs = clip_processor(images=[image], return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.get_image_features(**inputs)\n",
        "        image_emb = image_features.cpu().numpy()\n",
        "        image_embs.append(image_emb)\n",
        "    return text_emb, image_embs #, table_embs, figure_embs\n",
        "\n",
        "async def retrieving_documents_and_creating_multimodal_input(query, query_engine):\n",
        "    # 1. Retrieve relevant documents\n",
        "    retrieved_documents = await retrieve_documents(query, query_engine)\n",
        "\n",
        "    # 2. Process documents and create multimodal input\n",
        "    multimodal_input = \"\"\n",
        "    for source_node in retrieved_documents:\n",
        "        doc = source_node.node.text\n",
        "        multimodal_input += doc\n",
        "        if \"images\" in source_node.node.metadata:\n",
        "            for image_bytes in source_node.node.metadata[\"images\"]:\n",
        "                try:\n",
        "                    image = Image.open(io.BytesIO(image_bytes))\n",
        "                    image_summary = f\"Image from document: {source_node.node.text[:50]}...\"\n",
        "                    multimodal_input += f\"Image: {image_summary}\\n\"\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image for summary: {e}\")\n",
        "    return multimodal_input\n",
        "\n",
        "async def generate_seed_alphas(query, query_engine):\n",
        "\n",
        "    multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    \n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    # Query the LLM\n",
        "    prompt = f\"\"\"Generate *50 unqiue seed alphas* related to: {query}. Categorize them into financial domains (Momentum, Mean Reversion, Volatility, Fundamental, Liquidity, Quality, Growth, Technical, Micro Economics etc.) and provide the alpha name and code.  Focus on alphas suitable for daily stock market data.\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Momentum\",\n",
        "                \"name\": \"Price Momentum (14 days)\",\n",
        "                \"code\": \"((CLOSE - DELAY(CLOSE, 14)) / DELAY(CLOSE, 14))\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Mean Reversion\",\n",
        "                \"name\": \"Mean Reversion (20 days)\",\n",
        "                \"code\": \"(MEAN(CLOSE, 20) - CLOSE)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Volatility\",\n",
        "                \"name\": \"20-Day Volatility\",\n",
        "                \"code\": \"STD(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Fundamental\",\n",
        "                \"name\": \"Price-to-Earnings Ratio (P/E)\",\n",
        "                \"code\": \"(CLOSE / EPS)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Liquidity\",\n",
        "                \"name\": \"Trading Volume\",\n",
        "                \"code\": \"VOLUME\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Quality\",\n",
        "                \"name\": \"Gross Profit Margin\",\n",
        "                \"code\": \"(GROSS_PROFIT / REVENUE)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"GROWTH\",\n",
        "                \"name\": \"Earnings Growth Rate\",\n",
        "                \"code\": \"(EPS / DELAY(EPS,1) - 1)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Technical\",\n",
        "                \"name\": \"Moving Average (MA)\",\n",
        "                \"code\": \"SMA(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Macro Economics\",\n",
        "                \"name\": \"GDP Growth Rate\",\n",
        "                \"code\": \"GDP - DELAY (GDP, n)\"\n",
        "                }},\n",
        "                // ... more examples (at least 8-10 per domain if possible)\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "\n",
        "            *It is absolutely crucial that the response is valid JSON and nothing else.*  Do not include any explanatory text outside the JSON object.  If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Raw Response (Before Cleanup): {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "async def main():\n",
        "    data_dir = \"arxiv_pdfs\"  # Define the data directory\n",
        "\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Error: Directory '{data_dir}' does not exist. Create it and add your PDF files.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
        "        index = load_index_from_storage(storage_context)\n",
        "        query_engine = index.as_query_engine()\n",
        "        print(\"Index loaded from storage.\")\n",
        "    except Exception:\n",
        "        documents = SimpleDirectoryReader(data_dir).load_data()\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        query_engine = index.as_query_engine()\n",
        "        index.storage_context.persist(\"storage\")\n",
        "        print(\"New index created and persisted.\")\n",
        "\n",
        "    json_text = await generate_seed_alphas(\"research on momentum strategies\", query_engine)\n",
        "    return json_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    json_text = await main() # Use asyncio.run to execute the async main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(json_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alphas': [{'domain': 'Momentum', 'name': 'Price Momentum (14 days)', 'code': '((CLOSE - DELAY(CLOSE, 14)) / DELAY(CLOSE, 14))'}, {'domain': 'Mean Reversion', 'name': 'Mean Reversion (20 days)', 'code': '(MEAN(CLOSE, 20) - CLOSE)'}, {'domain': 'Volatility', 'name': '20-Day Volatility', 'code': 'STD(CLOSE, 20)'}, {'domain': 'Fundamental', 'name': 'Price-to-Earnings Ratio (P/E)', 'code': '(CLOSE / EPS)'}, {'domain': 'Liquidity', 'name': 'Trading Volume', 'code': 'VOLUME'}, {'domain': 'Quality', 'name': 'Gross Profit Margin', 'code': '(GROSS_PROFIT / REVENUE)'}, {'domain': 'GROWTH', 'name': 'Earnings Growth Rate', 'code': '(EPS / DELAY(EPS,1) - 1)'}, {'domain': 'Technical', 'name': 'Moving Average (MA)', 'code': 'SMA(CLOSE, 20)'}, {'domain': 'Macro Economics', 'name': 'GDP Growth Rate', 'code': 'GDP - DELAY(GDP, 1)'}, {'domain': 'Momentum', 'name': 'Relative Strength Index (RSI)', 'code': '(100 - (100 / (1 + RS)) * (CLOSE / PREV_CLOSE))'}, {'domain': 'Mean Reversion', 'name': 'Bollinger Bands', 'code': 'STDEV(CLOSE, 20) + (MEAN(CLOSE, 20) * 2)'}, {'domain': 'Volatility', 'name': 'Standard Deviation of Returns', 'code': '(STD(RETURN, 20))'}, {'domain': 'Fundamental', 'name': 'Price-to-Book Ratio (P/B)', 'code': '(CLOSE / BOOK_VALUE)'}, {'domain': 'Liquidity', 'name': 'Average True Range (ATR)', 'code': '(ATR(CLOSE, 20))'}, {'domain': 'Quality', 'name': 'Dividend Yield', 'code': '(DIVIDEND / CLOSE)'}, {'domain': 'GROWTH', 'name': 'Price-to-Book Growth Rate', 'code': '(BOOK_VALUE / CLOSE) - (BOOK_VALUE / DELAY(BOOK_VALUE, 1))'}, {'domain': 'Technical', 'name': 'Bollinger Bands Momentum', 'code': 'RS(CLOSE, 20)'}, {'domain': 'Macro Economics', 'name': 'Inflation Rate', 'code': '(INFLATION_RATE * 100)'}]}\n"
          ]
        }
      ],
      "source": [
        "print(json_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>domain</th>\n",
              "      <th>name</th>\n",
              "      <th>code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Price Momentum (14 days)</td>\n",
              "      <td>((CLOSE - DELAY(CLOSE, 14)) / DELAY(CLOSE, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Mean Reversion (20 days)</td>\n",
              "      <td>(MEAN(CLOSE, 20) - CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>20-Day Volatility</td>\n",
              "      <td>STD(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Price-to-Earnings Ratio (P/E)</td>\n",
              "      <td>(CLOSE / EPS)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Trading Volume</td>\n",
              "      <td>VOLUME</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Quality</td>\n",
              "      <td>Gross Profit Margin</td>\n",
              "      <td>(GROSS_PROFIT / REVENUE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GROWTH</td>\n",
              "      <td>Earnings Growth Rate</td>\n",
              "      <td>(EPS / DELAY(EPS,1) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Moving Average (MA)</td>\n",
              "      <td>SMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>GDP Growth Rate</td>\n",
              "      <td>GDP - DELAY(GDP, 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Relative Strength Index (RSI)</td>\n",
              "      <td>(100 - (100 / (1 + RS)) * (CLOSE / PREV_CLOSE))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Bollinger Bands</td>\n",
              "      <td>STDEV(CLOSE, 20) + (MEAN(CLOSE, 20) * 2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Standard Deviation of Returns</td>\n",
              "      <td>(STD(RETURN, 20))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Price-to-Book Ratio (P/B)</td>\n",
              "      <td>(CLOSE / BOOK_VALUE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Average True Range (ATR)</td>\n",
              "      <td>(ATR(CLOSE, 20))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Quality</td>\n",
              "      <td>Dividend Yield</td>\n",
              "      <td>(DIVIDEND / CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>GROWTH</td>\n",
              "      <td>Price-to-Book Growth Rate</td>\n",
              "      <td>(BOOK_VALUE / CLOSE) - (BOOK_VALUE / DELAY(BOO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Bollinger Bands Momentum</td>\n",
              "      <td>RS(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Inflation Rate</td>\n",
              "      <td>(INFLATION_RATE * 100)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             domain                           name  \\\n",
              "0          Momentum       Price Momentum (14 days)   \n",
              "1    Mean Reversion       Mean Reversion (20 days)   \n",
              "2        Volatility              20-Day Volatility   \n",
              "3       Fundamental  Price-to-Earnings Ratio (P/E)   \n",
              "4         Liquidity                 Trading Volume   \n",
              "5           Quality            Gross Profit Margin   \n",
              "6            GROWTH           Earnings Growth Rate   \n",
              "7         Technical            Moving Average (MA)   \n",
              "8   Macro Economics                GDP Growth Rate   \n",
              "9          Momentum  Relative Strength Index (RSI)   \n",
              "10   Mean Reversion                Bollinger Bands   \n",
              "11       Volatility  Standard Deviation of Returns   \n",
              "12      Fundamental      Price-to-Book Ratio (P/B)   \n",
              "13        Liquidity       Average True Range (ATR)   \n",
              "14          Quality                 Dividend Yield   \n",
              "15           GROWTH      Price-to-Book Growth Rate   \n",
              "16        Technical       Bollinger Bands Momentum   \n",
              "17  Macro Economics                 Inflation Rate   \n",
              "\n",
              "                                                 code  \n",
              "0     ((CLOSE - DELAY(CLOSE, 14)) / DELAY(CLOSE, 14))  \n",
              "1                           (MEAN(CLOSE, 20) - CLOSE)  \n",
              "2                                      STD(CLOSE, 20)  \n",
              "3                                       (CLOSE / EPS)  \n",
              "4                                              VOLUME  \n",
              "5                            (GROSS_PROFIT / REVENUE)  \n",
              "6                            (EPS / DELAY(EPS,1) - 1)  \n",
              "7                                      SMA(CLOSE, 20)  \n",
              "8                                 GDP - DELAY(GDP, 1)  \n",
              "9     (100 - (100 / (1 + RS)) * (CLOSE / PREV_CLOSE))  \n",
              "10           STDEV(CLOSE, 20) + (MEAN(CLOSE, 20) * 2)  \n",
              "11                                  (STD(RETURN, 20))  \n",
              "12                               (CLOSE / BOOK_VALUE)  \n",
              "13                                   (ATR(CLOSE, 20))  \n",
              "14                                 (DIVIDEND / CLOSE)  \n",
              "15  (BOOK_VALUE / CLOSE) - (BOOK_VALUE / DELAY(BOO...  \n",
              "16                                      RS(CLOSE, 20)  \n",
              "17                             (INFLATION_RATE * 100)  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "original_dfs = []\n",
        "\n",
        "\n",
        "alphas = json_text[\"alphas\"]\n",
        "original_df_new = pd.DataFrame(alphas)  # Directly create DataFrame from the list of dictionaries\n",
        "original_dfs.append(original_df_new)\n",
        "\n",
        "if original_dfs:\n",
        "    original_combined_df = pd.concat(original_dfs, ignore_index=True)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "original_combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_combined_df.to_csv(\"testing.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Second LLM Agent for Phase I (asked for each financial domain each time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (2.32.3)\n",
            "Requirement already satisfied: xmltodict in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (0.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests) (3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests xmltodict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (2.32.3)\n",
            "Requirement already satisfied: PyPDF2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (3.8.3)\n",
            "Requirement already satisfied: gensim in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (3.8.3)\n",
            "Requirement already satisfied: textblob in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (0.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests) (3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions>=3.10.0.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from PyPDF2) (4.12.2)\n",
            "Requirement already satisfied: tqdm in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: click in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: joblib in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: setuptools in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (58.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.16.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests PyPDF2 nltk spacy gensim textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import xmltodict\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from gensim import corpora, models\n",
        "import sqlite3\n",
        "from textblob import TextBlob  # Import TextBlob at the top level\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Load spaCy model ONCE, at the top level:\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    os.system(\"python -m spacy download en_core_web_sm\") # Download if not found\n",
        "    nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (0.12.17)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.17 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.12.17)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.6.4)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.4.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.4.5)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.3.19)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.62.0)\n",
            "Requirement already satisfied: dataclasses-json in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.6.7)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (4.67.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.4.51)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.0.8)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (2.10.6)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.9.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (2025.2.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (2.32.3)\n",
            "Requirement already satisfied: numpy in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (10.2.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.9.0)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (4.12.2)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.2.18)\n",
            "Requirement already satisfied: wrapt in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (1.12.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (6.0.2)\n",
            "Requirement already satisfied: httpx in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (0.28.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (3.2.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.17->llama-index) (3.11.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (21.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.18.3)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (4.0.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (0.2.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (6.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.3.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.8 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.12)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-cloud<0.2.0,>=0.1.8->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: idna in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (3.2)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.0.2)\n",
            "Requirement already satisfied: anyio in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (4.8.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (0.14.0)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.0)\n",
            "Requirement already satisfied: pandas in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.3.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.1)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.1)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.1.7)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-cloud-services>=0.6.1->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: joblib in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk>3.8.1->llama-index) (1.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.0.4)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (2.27.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.26.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.17->llama-index) (1.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.17->llama-index) (0.4.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.17->llama-index) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.16.0)\n",
            "Requirement already satisfied: llama-index-core in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (0.12.17)\n",
            "Requirement already satisfied: llama-index-readers-file in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (0.4.5)\n",
            "Requirement already satisfied: llama-index-llms-ollama in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (0.5.2)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (0.5.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (2.32.3)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.2.18)\n",
            "Requirement already satisfied: numpy in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (10.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (6.0.2)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: dataclasses-json in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.6.7)\n",
            "Requirement already satisfied: wrapt in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.12.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (3.11.12)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.2.2)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.4.51)\n",
            "Requirement already satisfied: httpx in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.28.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (2025.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (4.67.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (2.10.6)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (3.9.1)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: networkx>=3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (3.2.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-core) (9.0.0)\n",
            "Requirement already satisfied: pandas in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file) (1.3.4)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file) (0.0.26)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file) (5.3.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-readers-file) (4.13.1)\n",
            "Requirement already satisfied: ollama>=0.4.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-llms-ollama) (0.4.7)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-embeddings-huggingface) (3.4.1)\n",
            "Requirement already satisfied: huggingface-hub[inference]>=0.19.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from llama-index-embeddings-huggingface) (0.28.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (21.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.6)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.2.1)\n",
            "Requirement already satisfied: filelock in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2)\n",
            "Requirement already satisfied: joblib in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
            "Requirement already satisfied: click in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core) (1.0.2)\n",
            "Requirement already satisfied: certifi in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core) (4.8.0)\n",
            "Requirement already satisfied: idna in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpx->llama-index-core) (3.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core) (2.27.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core) (1.26.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core) (2.0.4)\n",
            "Requirement already satisfied: scipy in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.4.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.48.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (1.1.1)\n",
            "Requirement already satisfied: sympy in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.9)\n",
            "Requirement already satisfied: jinja2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core) (0.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx->llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from anyio->httpx->llama-index-core) (1.0.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pandas->llama-index-readers-file) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from pandas->llama-index-readers-file) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->llama-index-readers-file) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/alanto/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama imported successfully!\n",
            "Ollama LLM instantiated!\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "print(\"Ollama imported successfully!\")  # If this prints, the import works\n",
        "\n",
        "# If the import works, then try a minimal Ollama interaction:\n",
        "\n",
        "# You can type this command (in the terminal of your computer) to use this LLM : ollama run llama3.2\n",
        "try:\n",
        "  llm = Ollama(model=\"llama3.2\") # or another model available to you\n",
        "  print(\"Ollama LLM instantiated!\")\n",
        "except Exception as e:\n",
        "  print(f\"Error instantiating Ollama: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index loaded from storage.\n",
            "Output from LLM: ```\n",
            "{\n",
            "    \"alphas\": [\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"Price Momentum\",\n",
            "            \"code\": \"(CLOSE - DELAY(CLOSE, 14))\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"Volume Momentum\",\n",
            "            \"code\": \"(VOLUME - DELAY(VOLUME, 14))\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"RSI Momentum\",\n",
            "            \"code\": \"(RSI - DELAY(RSI, 14))\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"Stochastic Momentum\",\n",
            "            \"code\": \"(STOCHASTIC - DELAY(STOCHASTIC, 14))\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"Moving Average Momentum\",\n",
            "            \"code\": \"(SMA - DELAY(SMA, 14))\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"Bollinger Bands Momentum\",\n",
            "            \"code\": \"(BBAND - DELAY(BBAND, 14))\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"Force Index Momentum\",\n",
            "            \"code\": \"(FORCEINDEX - DELAY(FORCEINDEX, 14))\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"Rate of Change Momentum\",\n",
            "            \"code\": \"(RATEOFCHANGE - DELAY(RATEOFCHANGE, 14))\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Momentum\",\n",
            "            \"name\": \"Money Flow Index Momentum\",\n",
            "            \"code\": \"(MFI - DELAY(MFI, 14))\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "Output from LLM: ```\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Mean Reversion (20 days)\",\n",
            "      \"code\": \"(MEAN(CLOSE, 20) - CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Z-score Mean Reversion\",\n",
            "      \"code\": \"(CLOSE - MEAN(CLOSE, 20)) / STD(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Bollinger Bands\",\n",
            "      \"code\": \"(CLOSE - LOWER_BAND) / (UPPER_BAND - LOWER_BAND)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Exponential Moving Average Crossover\",\n",
            "      \"code\": \"IF(CLOSE > MA(20, CLOSE), 1, 0)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Relative Strength Index (RSI)\",\n",
            "      \"code\": \"(100 - (100 / (1 + RS(CLOSE)))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Momentum Indicator\",\n",
            "      \"code\": \"IF(CLOSE > CLOSE[1], 1, 0)\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "Output from LLM: ```\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Standard Deviation\",\n",
            "      \"code\": \"STD(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Average True Range (ATR)\",\n",
            "      \"code\": \"ATR(14)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Bollinger Band Width\",\n",
            "      \"code\": \"(UPPER_BAND - LOWER_BAND) / SMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Relative Strength Index (RSI)\",\n",
            "      \"code\": \"RSI(Close, 14)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Stochastic Oscillator\",\n",
            "      \"code\": \"STOCH(CLOSE, 14, 3, 3)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Keltner Channel Width\",\n",
            "      \"code\": \"(MAX(CLOSE) + MIN(CLOSE)) / SMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Bollinger Band Index\",\n",
            "      \"code\": \"SMA(CLOSE, 20) * (1 + ((UPPER_BAND - LOWER_BAND) / SMA(CLOSE, 20)))\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "Output from LLM: ```\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Fundamental\",\n",
            "      \"name\": \"Price-to-Earnings Ratio (P/E)\",\n",
            "      \"code\": \"(CLOSE / EPS)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Fundamental\",\n",
            "      \"name\": \"Price-to-Book Ratio (P/B)\",\n",
            "      \"code\": \"(CLOSE / BOOK_VALUE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Fundamental\",\n",
            "      \"name\": \"Dividend Yield\",\n",
            "      \"code\": \"(DIVIDENDS / CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Fundamental\",\n",
            "      \"name\": \"Price-to-Sales Ratio (P/S)\",\n",
            "      \"code\": \"(CLOSE / SALES)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Fundamental\",\n",
            "      \"name\": \"Return on Equity (ROE)\",\n",
            "      \"code\": \"(NET_INCOME / TOTAL_EQUITY)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Fundamental\",\n",
            "      \"name\": \"Free Cash Flow Yield\",\n",
            "      \"code\": \"(FREE_CASH_FLOW / STOCK_PRICE)\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "Output from LLM: ```\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Trading Volume\",\n",
            "      \"code\": \"VOLUME\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Average Trading Volume\",\n",
            "      \"code\": \"(HIGH - LOW)/ CLOSE\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Volume Rate of Change (VROC)\",\n",
            "      \"code\": \"(VOLUME - DELAY(VOLUME, 14)) / DELAY(VOLUME, 14)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Bid-Ask Spread (BAS)\",\n",
            "      \"code\": \"HIGH - LOW\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Implied Volatility (IV)\",\n",
            "      \"code\": \"\\u03C0V\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Average True Range (ATR)\",\n",
            "      \"code\": \"(HIGH - LOW)/ (HIGH + LOW - CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Order Book Depth\",\n",
            "      \"code\": \"(ORDERS - BIDS)/(ORDERS + BIDS)\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "Output from LLM: ```\n",
            "{\n",
            "    \"alphas\": [\n",
            "        {\n",
            "            \"domain\": \"Quality\",\n",
            "            \"name\": \"Gross Profit Margin\",\n",
            "            \"code\": \"(GROSS_PROFIT / REVENUE)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Quality\",\n",
            "            \"name\": \"Operating Profit Margin\",\n",
            "            \"code\": \"(OPERATING_INCOME / REVENUE)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Quality\",\n",
            "            \"name\": \"Net Profit Margin\",\n",
            "            \"code\": \"(NET_INCOME / REVENUE)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Quality\",\n",
            "            \"name\": \"Return on Equity (ROE)\",\n",
            "            \"code\": \"(NET_INCOME / TOTAL_EQUITY)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Quality\",\n",
            "            \"name\": \"Debt-to-Equity Ratio\",\n",
            "            \"code\": \"(DEBT / TOTAL_EQUITY)\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "Output from LLM: ```\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Growth\",\n",
            "      \"name\": \"Earnings Growth Rate\",\n",
            "      \"code\": \"(EPS / DELAY(EPS,1) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Growth\",\n",
            "      \"name\": \"Revenue Growth Rate\",\n",
            "      \"code\": \"(REVENUE / DELAY(REVENUE, 1) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Growth\",\n",
            "      \"name\": \"EBITDA Growth Rate\",\n",
            "      \"code\": \"(EBITDA / DELAY(EBITDA, 1) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Growth\",\n",
            "      \"name\": \"Free Cash Flow Growth Rate\",\n",
            "      \"code\": \"(FCF / DELAY(FCF, 1) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Growth\",\n",
            "      \"name\": \"Operating Income Growth Rate\",\n",
            "      \"code\": \"(OI / DELAY(OI, 1) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Growth\",\n",
            "      \"name\": \"Net Income Growth Rate\",\n",
            "      \"code\": \"(NI / DELAY(NI, 1) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Growth\",\n",
            "      \"name\": \"Price-to-Earnings Ratio (P/E) Change\",\n",
            "      \"code\": \"((P/E) / DELAY(P/E, 1)) - 1\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Growth\",\n",
            "      \"name\": \"Dividend Growth Rate\",\n",
            "      \"code\": \"(DIVIDEND / DELAY(DIVIDEND, 1) - 1)\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "Output from LLM: ```\n",
            "{\n",
            "    \"alphas\": [\n",
            "        {\n",
            "            \"domain\": \"Technical\",\n",
            "            \"name\": \"Moving Average (MA)\",\n",
            "            \"code\": \"SMA(CLOSE, 20)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Technical\",\n",
            "            \"name\": \"Exponential Moving Average (EMA)\",\n",
            "            \"code\": \"EMA(CLOSE, 20)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Technical\",\n",
            "            \"name\": \"Relative Strength Index (RSI)\",\n",
            "            \"code\": \"RSI(14)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Technical\",\n",
            "            \"name\": \"Bollinger Bands\",\n",
            "            \"code\": \"BBAND(CLOSE, 20, 2)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Technical\",\n",
            "            \"name\": \"On Balance Volume (OBV)\",\n",
            "            \"code\": \"OBV(CLOSE, 'Volume')\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Technical\",\n",
            "            \"name\": \"Momentum Indicator\",\n",
            "            \"code\": \"MOIN(CLOSE, 20)\"\n",
            "        },\n",
            "        {\n",
            "            \"domain\": \"Technical\",\n",
            "            \"name\": \"Stochastic Oscillator\",\n",
            "            \"code\": \"STOS(CLOSE, 14, 3, 3)\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "```\n",
            "\n",
            "Output from LLM: ```json\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"GDP Growth Rate\",\n",
            "      \"code\": \"GDP - DELAY (GDP, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Inflation Rate\",\n",
            "      \"code\": \"CPI - DELAY (GDP, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Unemployment Rate\",\n",
            "      \"code\": \"UNEMPLOYMENT_RATE - DELAY(UNEMPLOYMENT_RATE, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Consumer Price Index (CPI)\",\n",
            "      \"code\": \"CPI - DELAY (GDP, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Interest Rate\",\n",
            "      \"code\": \"INTEREST_RATE - DELAY (GDP, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Exchange Rate\",\n",
            "      \"code\": \"EXCHANGE_RATE - DELAY (GDP, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Unemployment Expectations Index\",\n",
            "      \"code\": \"UNEMPLOYMENT_EXPECTATIONS_INDEX - DELAY(UNEMPLOYMENT_RATE, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Inflation Expectations Index\",\n",
            "      \"code\": \"INFLATION_EXPECTATIONS_INDEX - DELAY(CPI, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"GDP Deflator\",\n",
            "      \"code\": \"GDP_DEFULATOR - DELAY(GDP, n)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Macro Economics\",\n",
            "      \"name\": \"Interest Rate Volatility\",\n",
            "      \"code\": \"INTEREST_RATE_VOLATILITY - DELAY(INTEREST_RATE, n)\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import asyncio\n",
        "import json\n",
        "from llama_index.core import Settings, StorageContext, load_index_from_storage  # Import missing classes\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex, Document, SimpleDirectoryReader, Settings\n",
        "import fitz\n",
        "from PIL import Image\n",
        "import re\n",
        "import io\n",
        "import os  # Import os for directory checking\n",
        "import pandas as pd #For table handling if needed\n",
        "import glob\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "# Set global settings\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "Settings.llm = Ollama(model=\"llama3.2\", request_timeout=720.0)\n",
        "\n",
        "text_embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "\n",
        "def process_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    document_data = {\n",
        "        \"text\": \"\",\n",
        "        \"images\": [],\n",
        "        }\n",
        "\n",
        "    for page in doc:\n",
        "        document_data[\"text\"] += page.get_text()\n",
        "    \n",
        "    for page in doc: #Iterate through all pages.\n",
        "        for image in page.get_images(): # Get images without width/height\n",
        "            xref = image[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            if base_image: # Check if image extracted\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image_ext = base_image[\"ext\"]\n",
        "\n",
        "            try:\n",
        "                image = Image.open(io.BytesIO(image_bytes)) # Use PIL to resize\n",
        "                new_image = image.resize((2000, 2000)) # Resize with PIL\n",
        "                # Convert the resized image back to bytes (if needed for later use)\n",
        "                image_bytes_resized = io.BytesIO()\n",
        "                new_image.save(image_bytes_resized, format=image_ext.upper()) # Save in original format\n",
        "                image_bytes_resized = image_bytes_resized.getvalue()\n",
        "                document_data[\"images\"].append(image_bytes_resized) # Append image bytes\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing or resizing image: {e}\")\n",
        "                continue # Skip to the next image if there's an issue\n",
        "    return document_data\n",
        "    \n",
        "async def retrieve_documents(query, query_engine): # Add query_engine parameter\n",
        "    pdf_directory = \"arxiv_pdfs\"\n",
        "    pdf_paths = glob.glob(os.path.join(pdf_directory, \"*.pdf\"))\n",
        "    # pdf_paths = [\"arxiv_pdfs/2409.06289v1.pdf\"]\n",
        "    # pdf_paths = [\"arxiv_pdfs/2103.16196v2.pdf\", \"arxiv_pdfs/2308.00016v1.pdf\"] # Replace with your PDF file paths\n",
        "    documents = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        data = process_pdf(pdf_path) # Process each pdf file\n",
        "        # Create LlamaIndex Documents\n",
        "        text_document = Document(text=data[\"text\"], metadata={\"source\": pdf_path})\n",
        "        documents.append(text_document)\n",
        "        # Handle images\n",
        "        for image_bytes in data[\"images\"]:\n",
        "            image_document = Document(text=\"Image\", metadata={\"image_bytes\": image_bytes, \"source\": pdf_path})\n",
        "            documents.append(image_document)\n",
        "\n",
        "    response = await query_engine.aquery(query)\n",
        "    return response.source_nodes\n",
        "\n",
        "\n",
        "def get_multimodal_embeddings(data):\n",
        "    text_emb = text_embedding_model.encode(data[\"text\"])\n",
        "    image_embs = []\n",
        "    for image in data[\"images\"]:\n",
        "        inputs = clip_processor(images=[image], return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.get_image_features(**inputs)\n",
        "        image_emb = image_features.cpu().numpy()\n",
        "        image_embs.append(image_emb)\n",
        "    return text_emb, image_embs #, table_embs, figure_embs\n",
        "\n",
        "async def retrieving_documents_and_creating_multimodal_input(query, query_engine):\n",
        "    # 1. Retrieve relevant documents\n",
        "    retrieved_documents = await retrieve_documents(query, query_engine)\n",
        "\n",
        "    # 2. Process documents and create multimodal input\n",
        "    multimodal_input = \"\"\n",
        "    for source_node in retrieved_documents:\n",
        "        doc = source_node.node.text\n",
        "        multimodal_input += doc\n",
        "        if \"images\" in source_node.node.metadata:\n",
        "            for image_bytes in source_node.node.metadata[\"images\"]:\n",
        "                try:\n",
        "                    image = Image.open(io.BytesIO(image_bytes))\n",
        "                    image_summary = f\"Image from document: {source_node.node.text[:50]}...\"\n",
        "                    multimodal_input += f\"Image: {image_summary}\\n\"\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image for summary: {e}\")\n",
        "    return multimodal_input\n",
        "\n",
        "async def generate_alphas_for_momentum(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "\n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Momentum*. Provide the alpha name and code. Focus on alphas suitable for daily stock market data. *The domain of all alpha factors must be \"Momentum\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Momentum\",\n",
        "                \"name\": \"Price Momentum\",\n",
        "                \"code\": \"(CLOSE - DELAY(CLOSE, 14)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Momentum\",\n",
        "                \"name\": \"Volume Momentum\",\n",
        "                \"code\": \"(VOLUME - DELAY(VOLUME, 14))\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Momentum\",\n",
        "                \"name\": \"RSI Momentum\",\n",
        "                \"code\": \"(RSI - DELAY(RSI, 14))\"\n",
        "                }}\n",
        "                // ... more examples (at least 8-10 if possible)\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.* Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.* If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "async def generate_alphas_for_mean_reversion(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "\n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Mean Reversion*. Provide the alpha name and code. Focus on alphas suitable for daily stock market data. *The domain of all alpha factors must be \"Mean Reversion\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Mean Reversion\",\n",
        "                \"name\": \"Mean Reversion (20 days)\",\n",
        "                \"code\": \"(MEAN(CLOSE, 20) - CLOSE)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Mean Reversion\",\n",
        "                \"name\": \"Z-score Mean Reversion\",\n",
        "                \"code\": \"(CLOSE - MEAN(CLOSE, 20)) / STD(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Mean Reversion\",\n",
        "                \"name\": \"Bollinger Bands\",\n",
        "                \"code\": \"(CLOSE - LOWER_BAND) / (UPPER_BAND - LOWER_BAND)\"\n",
        "                }},\n",
        "                // ... more examples\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.* Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.* If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "async def generate_alphas_for_volatility(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    \n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Volatility*.  Provide the alpha name and code.  Focus on alphas suitable for daily stock market data.*The domain name of all alpha factors must be \"Volatility\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Volatility\",\n",
        "                \"name\": \"Standard Deviation\",\n",
        "                \"code\": \"STD(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Volatility\",\n",
        "                \"name\": \"Average True Range (ATR)\",\n",
        "                \"code\": \"ATR(14)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Volatility\",\n",
        "                \"name\": \"Bollinger Band Width\",\n",
        "                \"code\": \"(UPPER_BAND - LOWER_BAND) / SMA(CLOSE, 20)\"\n",
        "                }},\n",
        "                // ... more examples\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.* Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.* If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "async def generate_alphas_for_funadmental(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    \n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Fundamental*.  Provide the alpha name and code.  Focus on alphas suitable for daily stock market data.*The domain name of all alpha factors must be \"Fundamental\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Fundamental\",\n",
        "                \"name\": \"Price-to-Earnings Ratio (P/E)\",\n",
        "                \"code\": \"(CLOSE / EPS)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Fundamental\",\n",
        "                \"name\": \"Price-to-Book Ratio (P/E)\",\n",
        "                \"code\": \"(CLOSE / BOOK_VALUE)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Fundamental\",\n",
        "                \"name\": \"Dividend Yield\",\n",
        "                \"code\": \"(DIVIDENDS / CLOSE)\"\n",
        "                }},\n",
        "                // ... more examples\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.* Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.* If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "async def generate_alphas_for_liquidity(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    \n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Liquidity*.  Provide the alpha name and code.  Focus on alphas suitable for daily stock market data.*The domain name of all alpha factors must be \"Liquidity\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Liquidity\",\n",
        "                \"name\": \"Trading Volume\",\n",
        "                \"code\": \"VOLUME\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Liquidity\",\n",
        "                \"name\": \"Average Trading Volume\",\n",
        "                \"code\": \"(HIGH - LOW)/ CLOSE\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Liquidity\",\n",
        "                \"name\": \"Volume Rate of Change (VROC)\",\n",
        "                \"code\": \"(VOLUME - DELAY(VOLUME, 14)) / DELAY(VOLUME, 14)\"\n",
        "                }},\n",
        "                // ... more examples \n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.* Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.* If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "\n",
        "async def generate_alphas_for_quality(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    \n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Quality*.  Provide the alpha name and code.  Focus on alphas suitable for daily stock market data.*The domain name of all alpha factors must be \"Quality\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Quality\",\n",
        "                \"name\": \"Gross Profit Margin\",\n",
        "                \"code\": \"(GROSS_PROFIT / REVENUE)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Quality\",\n",
        "                \"name\": \"Operating Profit Margin\",\n",
        "                \"code\": \"(OPERATING_INCOME / REVENUE)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Quality\",\n",
        "                \"name\": \"Net Profit Margin\",\n",
        "                \"code\": \"(NET_INCOME / REVENUE)\"\n",
        "                }},\n",
        "                // ... more examples\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.* Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.* If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "async def generate_alphas_for_growth(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    \n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "    \n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Growth*.  Provide the alpha name and code.  Focus on alphas suitable for daily stock market data.*The domain name of all alpha factors must be \"Growth\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Growth\",\n",
        "                \"name\": \"Earnings Growth Rate\",\n",
        "                \"code\": \"(EPS / DELAY(EPS,1) - 1)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Growth\",\n",
        "                \"name\": \"Revenue Growth Rate\",\n",
        "                \"code\": \"(REVENUE / DELAY(REVENUE, 1) - 1)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Growth\",\n",
        "                \"name\": \"EBITDA Growth Rate\",\n",
        "                \"code\": \"(EBITDA / DELAY(EBITDA, 1) - 1)\"\n",
        "                }},\n",
        "                // ... more examples \n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.* Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.* If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "\n",
        "async def generate_alphas_for_technical(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    \n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Technical*.  Provide the alpha name and code.  Focus on alphas suitable for daily stock market data.*The domain name of all alpha factors must be \"Technical\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Technical\",\n",
        "                \"name\": \"Moving Average (MA)\",\n",
        "                \"code\": \"SMA(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Technical\",\n",
        "                \"name\": \"Exponential Moving Average (EMA)\",\n",
        "                \"code\": \"EMA(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Technical\",\n",
        "                \"name\": \"Relative Strength Index (RSI)\",\n",
        "                \"code\": \"RSI(14)\"\n",
        "                }},\n",
        "                // ... more examples\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.*  Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.*  If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e:\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "async def generate_alphas_for_macro(query, query_engine, multimodal_input):\n",
        "\n",
        "    # multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    \n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects.\"),\n",
        "    ]\n",
        "    output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "    format_instructions = output_parser.get_format_instructions()\n",
        "    \n",
        "    # 3. Query the LLM\n",
        "    prompt = f\"\"\"Generate *unique seed alphas* related to: *Macro Economics*.  Provide the alpha name and code.  Focus on alphas suitable for daily stock market data.*The domain name of all alpha factors must be \"Macro Economics\".*\n",
        "\n",
        "            Return the result as a *valid JSON object (dictionary)*.  The JSON object *must* have the following structure:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Macro Economics\",\n",
        "                \"name\": \"GDP Growth Rate\",\n",
        "                \"code\": \"GDP - DELAY (GDP, n)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Macro Economics\",\n",
        "                \"name\": \"Inflation Rate\",\n",
        "                \"code\": \"CPI - DELAY (GDP, n)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Macro Economics\",\n",
        "                \"name\": \"Unemployment Rate\",\n",
        "                \"code\": \"UNEMPLOYMENT_RATE - DELAY(UNEMPLOYMENT_RATE, n)\"\n",
        "                }},\n",
        "                // ... more examples (at least 8-10 per domain if possible)\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            *It is absolutely crucial that the response is valid JSON, and nothing else.* Do not include other alpha factors' domains. *Do not include any explanatory text outside the JSON object.* If you cannot generate any alphas, return an empty JSON object: `{{ \"alphas\": [] }}`.\n",
        "            Make sure all the keys (domain, name, code) are enclosed in double quotes. \n",
        "            {multimodal_input}\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = await Settings.llm.acomplete(prompt + multimodal_input)\n",
        "        completion_text = response.text\n",
        "        print(f\"Output from LLM: {completion_text}\")\n",
        "        print()\n",
        "        try:\n",
        "            parsed_output = output_parser.parse(completion_text)\n",
        "            return parsed_output\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM output: {parse_error}\")\n",
        "            return {\"alphas\": []} #return empty json on error\n",
        "    except Exception as e\n",
        "        print(f\"LLM or other Error: {e}\")\n",
        "        return None\n",
        "    \n",
        "async def generate_all_domain_alphas(query, query_engine, multimodal_input):\n",
        "    # domains = [\"Momentum\", \"Mean Reversion\",\"Volatility\", \"Fundamental\", \"Liquidity\", \"Quality\", \"Growth\", \"Technical\", \"Micro Economics\"]\n",
        "    all_alphas = []\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_momentum(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_mean_reversion(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_volatility(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_funadmental(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_liquidity(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_quality(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_growth(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_technical(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "\n",
        "    domain_alphas_json = await generate_alphas_for_macro(query, query_engine, multimodal_input)\n",
        "    all_alphas.append(domain_alphas_json)\n",
        "    \n",
        "    return all_alphas\n",
        "\n",
        "\n",
        "async def main():\n",
        "    data_dir = \"arxiv_pdfs\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Error: Directory '{data_dir}' does not exist. Create it and add your PDF files.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=\"storage\")\n",
        "        index = load_index_from_storage(storage_context)\n",
        "        query_engine = index.as_query_engine()\n",
        "        print(\"Index loaded from storage.\")\n",
        "    except Exception:\n",
        "        documents = SimpleDirectoryReader(data_dir).load_data()\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        query_engine = index.as_query_engine()\n",
        "        index.storage_context.persist(\"storage\")\n",
        "        print(\"New index created and persisted.\")\n",
        "\n",
        "    query = \"research on momentum strategies\"\n",
        "    # query = \"research on alpha factors\"\n",
        "    multimodal_input = await retrieving_documents_and_creating_multimodal_input(query, query_engine)\n",
        "    all_alphas = await generate_all_domain_alphas(query, query_engine, multimodal_input)\n",
        "    return all_alphas\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    json_new_text = await main() # Use asyncio.run to execute the async main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>domain</th>\n",
              "      <th>name</th>\n",
              "      <th>code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Price Momentum</td>\n",
              "      <td>(CLOSE - DELAY(CLOSE, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Volume Momentum</td>\n",
              "      <td>(VOLUME - DELAY(VOLUME, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>RSI Momentum</td>\n",
              "      <td>(RSI - DELAY(RSI, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Stochastic Momentum</td>\n",
              "      <td>(STOCHASTIC - DELAY(STOCHASTIC, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Moving Average Momentum</td>\n",
              "      <td>(SMA - DELAY(SMA, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Bollinger Bands Momentum</td>\n",
              "      <td>(BBAND - DELAY(BBAND, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Force Index Momentum</td>\n",
              "      <td>(FORCEINDEX - DELAY(FORCEINDEX, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Rate of Change Momentum</td>\n",
              "      <td>(RATEOFCHANGE - DELAY(RATEOFCHANGE, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Money Flow Index Momentum</td>\n",
              "      <td>(MFI - DELAY(MFI, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Mean Reversion (20 days)</td>\n",
              "      <td>(MEAN(CLOSE, 20) - CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Z-score Mean Reversion</td>\n",
              "      <td>(CLOSE - MEAN(CLOSE, 20)) / STD(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Bollinger Bands</td>\n",
              "      <td>(CLOSE - LOWER_BAND) / (UPPER_BAND - LOWER_BAND)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Exponential Moving Average Crossover</td>\n",
              "      <td>IF(CLOSE &gt; MA(20, CLOSE), 1, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Relative Strength Index (RSI)</td>\n",
              "      <td>(100 - (100 / (1 + RS(CLOSE)))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Momentum Indicator</td>\n",
              "      <td>IF(CLOSE &gt; CLOSE[1], 1, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Standard Deviation</td>\n",
              "      <td>STD(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Average True Range (ATR)</td>\n",
              "      <td>ATR(14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Bollinger Band Width</td>\n",
              "      <td>(UPPER_BAND - LOWER_BAND) / SMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Relative Strength Index (RSI)</td>\n",
              "      <td>RSI(Close, 14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Stochastic Oscillator</td>\n",
              "      <td>STOCH(CLOSE, 14, 3, 3)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Keltner Channel Width</td>\n",
              "      <td>(MAX(CLOSE) + MIN(CLOSE)) / SMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Bollinger Band Index</td>\n",
              "      <td>SMA(CLOSE, 20) * (1 + ((UPPER_BAND - LOWER_BAN...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Price-to-Earnings Ratio (P/E)</td>\n",
              "      <td>(CLOSE / EPS)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Price-to-Book Ratio (P/B)</td>\n",
              "      <td>(CLOSE / BOOK_VALUE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Dividend Yield</td>\n",
              "      <td>(DIVIDENDS / CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Price-to-Sales Ratio (P/S)</td>\n",
              "      <td>(CLOSE / SALES)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Return on Equity (ROE)</td>\n",
              "      <td>(NET_INCOME / TOTAL_EQUITY)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Fundamental</td>\n",
              "      <td>Free Cash Flow Yield</td>\n",
              "      <td>(FREE_CASH_FLOW / STOCK_PRICE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Trading Volume</td>\n",
              "      <td>VOLUME</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Average Trading Volume</td>\n",
              "      <td>(HIGH - LOW)/ CLOSE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Volume Rate of Change (VROC)</td>\n",
              "      <td>(VOLUME - DELAY(VOLUME, 14)) / DELAY(VOLUME, 14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Bid-Ask Spread (BAS)</td>\n",
              "      <td>HIGH - LOW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Implied Volatility (IV)</td>\n",
              "      <td>πV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Average True Range (ATR)</td>\n",
              "      <td>(HIGH - LOW)/ (HIGH + LOW - CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Order Book Depth</td>\n",
              "      <td>(ORDERS - BIDS)/(ORDERS + BIDS)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Quality</td>\n",
              "      <td>Gross Profit Margin</td>\n",
              "      <td>(GROSS_PROFIT / REVENUE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Quality</td>\n",
              "      <td>Operating Profit Margin</td>\n",
              "      <td>(OPERATING_INCOME / REVENUE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Quality</td>\n",
              "      <td>Net Profit Margin</td>\n",
              "      <td>(NET_INCOME / REVENUE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Quality</td>\n",
              "      <td>Return on Equity (ROE)</td>\n",
              "      <td>(NET_INCOME / TOTAL_EQUITY)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Quality</td>\n",
              "      <td>Debt-to-Equity Ratio</td>\n",
              "      <td>(DEBT / TOTAL_EQUITY)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Growth</td>\n",
              "      <td>Earnings Growth Rate</td>\n",
              "      <td>(EPS / DELAY(EPS,1) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Growth</td>\n",
              "      <td>Revenue Growth Rate</td>\n",
              "      <td>(REVENUE / DELAY(REVENUE, 1) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Growth</td>\n",
              "      <td>EBITDA Growth Rate</td>\n",
              "      <td>(EBITDA / DELAY(EBITDA, 1) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Growth</td>\n",
              "      <td>Free Cash Flow Growth Rate</td>\n",
              "      <td>(FCF / DELAY(FCF, 1) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Growth</td>\n",
              "      <td>Operating Income Growth Rate</td>\n",
              "      <td>(OI / DELAY(OI, 1) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>Growth</td>\n",
              "      <td>Net Income Growth Rate</td>\n",
              "      <td>(NI / DELAY(NI, 1) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Growth</td>\n",
              "      <td>Price-to-Earnings Ratio (P/E) Change</td>\n",
              "      <td>((P/E) / DELAY(P/E, 1)) - 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Growth</td>\n",
              "      <td>Dividend Growth Rate</td>\n",
              "      <td>(DIVIDEND / DELAY(DIVIDEND, 1) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Moving Average (MA)</td>\n",
              "      <td>SMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Exponential Moving Average (EMA)</td>\n",
              "      <td>EMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Relative Strength Index (RSI)</td>\n",
              "      <td>RSI(14)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Bollinger Bands</td>\n",
              "      <td>BBAND(CLOSE, 20, 2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Technical</td>\n",
              "      <td>On Balance Volume (OBV)</td>\n",
              "      <td>OBV(CLOSE, 'Volume')</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Momentum Indicator</td>\n",
              "      <td>MOIN(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Stochastic Oscillator</td>\n",
              "      <td>STOS(CLOSE, 14, 3, 3)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>GDP Growth Rate</td>\n",
              "      <td>GDP - DELAY (GDP, n)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Inflation Rate</td>\n",
              "      <td>CPI - DELAY (GDP, n)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Unemployment Rate</td>\n",
              "      <td>UNEMPLOYMENT_RATE - DELAY(UNEMPLOYMENT_RATE, n)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Consumer Price Index (CPI)</td>\n",
              "      <td>CPI - DELAY (GDP, n)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Interest Rate</td>\n",
              "      <td>INTEREST_RATE - DELAY (GDP, n)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Exchange Rate</td>\n",
              "      <td>EXCHANGE_RATE - DELAY (GDP, n)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Unemployment Expectations Index</td>\n",
              "      <td>UNEMPLOYMENT_EXPECTATIONS_INDEX - DELAY(UNEMPL...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Inflation Expectations Index</td>\n",
              "      <td>INFLATION_EXPECTATIONS_INDEX - DELAY(CPI, n)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>GDP Deflator</td>\n",
              "      <td>GDP_DEFULATOR - DELAY(GDP, n)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>Macro Economics</td>\n",
              "      <td>Interest Rate Volatility</td>\n",
              "      <td>INTEREST_RATE_VOLATILITY - DELAY(INTEREST_RATE...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             domain                                  name  \\\n",
              "0          Momentum                        Price Momentum   \n",
              "1          Momentum                       Volume Momentum   \n",
              "2          Momentum                          RSI Momentum   \n",
              "3          Momentum                   Stochastic Momentum   \n",
              "4          Momentum               Moving Average Momentum   \n",
              "5          Momentum              Bollinger Bands Momentum   \n",
              "6          Momentum                  Force Index Momentum   \n",
              "7          Momentum               Rate of Change Momentum   \n",
              "8          Momentum             Money Flow Index Momentum   \n",
              "9    Mean Reversion              Mean Reversion (20 days)   \n",
              "10   Mean Reversion                Z-score Mean Reversion   \n",
              "11   Mean Reversion                       Bollinger Bands   \n",
              "12   Mean Reversion  Exponential Moving Average Crossover   \n",
              "13   Mean Reversion         Relative Strength Index (RSI)   \n",
              "14   Mean Reversion                    Momentum Indicator   \n",
              "15       Volatility                    Standard Deviation   \n",
              "16       Volatility              Average True Range (ATR)   \n",
              "17       Volatility                  Bollinger Band Width   \n",
              "18       Volatility         Relative Strength Index (RSI)   \n",
              "19       Volatility                 Stochastic Oscillator   \n",
              "20       Volatility                 Keltner Channel Width   \n",
              "21       Volatility                  Bollinger Band Index   \n",
              "22      Fundamental         Price-to-Earnings Ratio (P/E)   \n",
              "23      Fundamental             Price-to-Book Ratio (P/B)   \n",
              "24      Fundamental                        Dividend Yield   \n",
              "25      Fundamental            Price-to-Sales Ratio (P/S)   \n",
              "26      Fundamental                Return on Equity (ROE)   \n",
              "27      Fundamental                  Free Cash Flow Yield   \n",
              "28        Liquidity                        Trading Volume   \n",
              "29        Liquidity                Average Trading Volume   \n",
              "30        Liquidity          Volume Rate of Change (VROC)   \n",
              "31        Liquidity                  Bid-Ask Spread (BAS)   \n",
              "32        Liquidity               Implied Volatility (IV)   \n",
              "33        Liquidity              Average True Range (ATR)   \n",
              "34        Liquidity                      Order Book Depth   \n",
              "35          Quality                   Gross Profit Margin   \n",
              "36          Quality               Operating Profit Margin   \n",
              "37          Quality                     Net Profit Margin   \n",
              "38          Quality                Return on Equity (ROE)   \n",
              "39          Quality                  Debt-to-Equity Ratio   \n",
              "40           Growth                  Earnings Growth Rate   \n",
              "41           Growth                   Revenue Growth Rate   \n",
              "42           Growth                    EBITDA Growth Rate   \n",
              "43           Growth            Free Cash Flow Growth Rate   \n",
              "44           Growth          Operating Income Growth Rate   \n",
              "45           Growth                Net Income Growth Rate   \n",
              "46           Growth  Price-to-Earnings Ratio (P/E) Change   \n",
              "47           Growth                  Dividend Growth Rate   \n",
              "48        Technical                   Moving Average (MA)   \n",
              "49        Technical      Exponential Moving Average (EMA)   \n",
              "50        Technical         Relative Strength Index (RSI)   \n",
              "51        Technical                       Bollinger Bands   \n",
              "52        Technical               On Balance Volume (OBV)   \n",
              "53        Technical                    Momentum Indicator   \n",
              "54        Technical                 Stochastic Oscillator   \n",
              "55  Macro Economics                       GDP Growth Rate   \n",
              "56  Macro Economics                        Inflation Rate   \n",
              "57  Macro Economics                     Unemployment Rate   \n",
              "58  Macro Economics            Consumer Price Index (CPI)   \n",
              "59  Macro Economics                         Interest Rate   \n",
              "60  Macro Economics                         Exchange Rate   \n",
              "61  Macro Economics       Unemployment Expectations Index   \n",
              "62  Macro Economics          Inflation Expectations Index   \n",
              "63  Macro Economics                          GDP Deflator   \n",
              "64  Macro Economics              Interest Rate Volatility   \n",
              "\n",
              "                                                 code  \n",
              "0                          (CLOSE - DELAY(CLOSE, 14))  \n",
              "1                        (VOLUME - DELAY(VOLUME, 14))  \n",
              "2                              (RSI - DELAY(RSI, 14))  \n",
              "3                (STOCHASTIC - DELAY(STOCHASTIC, 14))  \n",
              "4                              (SMA - DELAY(SMA, 14))  \n",
              "5                          (BBAND - DELAY(BBAND, 14))  \n",
              "6                (FORCEINDEX - DELAY(FORCEINDEX, 14))  \n",
              "7            (RATEOFCHANGE - DELAY(RATEOFCHANGE, 14))  \n",
              "8                              (MFI - DELAY(MFI, 14))  \n",
              "9                           (MEAN(CLOSE, 20) - CLOSE)  \n",
              "10         (CLOSE - MEAN(CLOSE, 20)) / STD(CLOSE, 20)  \n",
              "11   (CLOSE - LOWER_BAND) / (UPPER_BAND - LOWER_BAND)  \n",
              "12                    IF(CLOSE > MA(20, CLOSE), 1, 0)  \n",
              "13                     (100 - (100 / (1 + RS(CLOSE)))  \n",
              "14                         IF(CLOSE > CLOSE[1], 1, 0)  \n",
              "15                                     STD(CLOSE, 20)  \n",
              "16                                            ATR(14)  \n",
              "17         (UPPER_BAND - LOWER_BAND) / SMA(CLOSE, 20)  \n",
              "18                                     RSI(Close, 14)  \n",
              "19                             STOCH(CLOSE, 14, 3, 3)  \n",
              "20         (MAX(CLOSE) + MIN(CLOSE)) / SMA(CLOSE, 20)  \n",
              "21  SMA(CLOSE, 20) * (1 + ((UPPER_BAND - LOWER_BAN...  \n",
              "22                                      (CLOSE / EPS)  \n",
              "23                               (CLOSE / BOOK_VALUE)  \n",
              "24                                (DIVIDENDS / CLOSE)  \n",
              "25                                    (CLOSE / SALES)  \n",
              "26                        (NET_INCOME / TOTAL_EQUITY)  \n",
              "27                     (FREE_CASH_FLOW / STOCK_PRICE)  \n",
              "28                                             VOLUME  \n",
              "29                                (HIGH - LOW)/ CLOSE  \n",
              "30   (VOLUME - DELAY(VOLUME, 14)) / DELAY(VOLUME, 14)  \n",
              "31                                         HIGH - LOW  \n",
              "32                                                 πV  \n",
              "33                 (HIGH - LOW)/ (HIGH + LOW - CLOSE)  \n",
              "34                    (ORDERS - BIDS)/(ORDERS + BIDS)  \n",
              "35                           (GROSS_PROFIT / REVENUE)  \n",
              "36                       (OPERATING_INCOME / REVENUE)  \n",
              "37                             (NET_INCOME / REVENUE)  \n",
              "38                        (NET_INCOME / TOTAL_EQUITY)  \n",
              "39                              (DEBT / TOTAL_EQUITY)  \n",
              "40                           (EPS / DELAY(EPS,1) - 1)  \n",
              "41                  (REVENUE / DELAY(REVENUE, 1) - 1)  \n",
              "42                    (EBITDA / DELAY(EBITDA, 1) - 1)  \n",
              "43                          (FCF / DELAY(FCF, 1) - 1)  \n",
              "44                            (OI / DELAY(OI, 1) - 1)  \n",
              "45                            (NI / DELAY(NI, 1) - 1)  \n",
              "46                        ((P/E) / DELAY(P/E, 1)) - 1  \n",
              "47                (DIVIDEND / DELAY(DIVIDEND, 1) - 1)  \n",
              "48                                     SMA(CLOSE, 20)  \n",
              "49                                     EMA(CLOSE, 20)  \n",
              "50                                            RSI(14)  \n",
              "51                                BBAND(CLOSE, 20, 2)  \n",
              "52                               OBV(CLOSE, 'Volume')  \n",
              "53                                    MOIN(CLOSE, 20)  \n",
              "54                              STOS(CLOSE, 14, 3, 3)  \n",
              "55                               GDP - DELAY (GDP, n)  \n",
              "56                               CPI - DELAY (GDP, n)  \n",
              "57    UNEMPLOYMENT_RATE - DELAY(UNEMPLOYMENT_RATE, n)  \n",
              "58                               CPI - DELAY (GDP, n)  \n",
              "59                     INTEREST_RATE - DELAY (GDP, n)  \n",
              "60                     EXCHANGE_RATE - DELAY (GDP, n)  \n",
              "61  UNEMPLOYMENT_EXPECTATIONS_INDEX - DELAY(UNEMPL...  \n",
              "62       INFLATION_EXPECTATIONS_INDEX - DELAY(CPI, n)  \n",
              "63                      GDP_DEFULATOR - DELAY(GDP, n)  \n",
              "64  INTEREST_RATE_VOLATILITY - DELAY(INTEREST_RATE...  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_dfs = []\n",
        "\n",
        "for item in json_new_text:\n",
        "    alphas = item[\"alphas\"]\n",
        "    df_new = pd.DataFrame(alphas)  # Directly create DataFrame from the list of dictionaries\n",
        "    new_dfs.append(df_new)\n",
        "\n",
        "if new_dfs:\n",
        "    new_combined_df = pd.concat(new_dfs, ignore_index=True)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "new_combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_combined_df.to_csv(\"best_results_ever.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
