{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent used to select the best set of alpha factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/7 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████▏  | 5/7 [02:25<01:00, 30.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Could not determine the better index from the response: Based on the provided data, I can make an educated analysis without generating any Python code.\n",
            "\n",
            "From the above analysis of the two Excel files, it appears that both 13.xlsx and 10.xlsx have a more complex structure than the SSE 50 index data. The SSE 50 index data had a clear table header with straightforward column names, whereas the alpha performance data has a more intricate structure with multiple sheets and columns.\n",
            "\n",
            "However, if we look at the data provided for each sheet, it seems that both alphas have varying levels of performance. Alpha 13 (from 13.xlsx) appears to have higher values in some columns compared to alpha 10 (from 10.xlsx), while alpha 10 has more consistent growth in other columns.\n",
            "\n",
            "Without further analysis or additional data, it's difficult to definitively say which alpha performs better. However, based on the insights gained from the SSE 50 index data and the structure of the alpha performance data, I would lean towards alpha 13 (from 13.xlsx) being potentially better due to its higher values in some columns.\n",
            "\n",
            "Please note that this is a high-level analysis and may not be comprehensive or conclusive. Further investigation and data analysis would be necessary to make a definitive determination.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [03:49<00:00, 32.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best alpha is: 20.xlsx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from ollama import Client  # Import the Ollama client\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the Ollama client\n",
        "client = Client(host='http://localhost:11434')  # Adjust host if needed\n",
        "\n",
        "log_path = \"./log/\"\n",
        "log_file = \"\"\n",
        "\n",
        "\n",
        "def retry_until_expected(run, thread_id, expect):\n",
        "    \"\"\"\n",
        "    This function needs to be adapted for Ollama's run status mechanism.\n",
        "    Ollama's Assistants API might have a different way of tracking run status.\n",
        "    For now, we'll just have a simple delay as Ollama typically runs quickly locally.\n",
        "    Consider removing this or implementing a more suitable check if Ollama provides status updates.\n",
        "    \"\"\"\n",
        "    print(f\"Waiting for Ollama to complete the run...\")\n",
        "    time.sleep(5)  # Adjust the delay as needed\n",
        "\n",
        "\n",
        "def get_last_text_message(thread_id):\n",
        "    \"\"\"\n",
        "    Ollama's API response structure for messages might be different.\n",
        "    This function needs to be adapted to extract the last text message from an Ollama thread.\n",
        "    Assuming Ollama's message structure is similar, this might work, but verify.\n",
        "    \"\"\"\n",
        "    messages = client.chat(model=\"llama3.2\", messages=thread_id) # Assuming thread_id holds the conversation history\n",
        "    if messages and messages['message']['content']:\n",
        "        return messages['message']['content']\n",
        "    return None\n",
        "\n",
        "\n",
        "def log_to_file(type, message):\n",
        "    if type == \"input\":\n",
        "        message = (\n",
        "            \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\"\n",
        "            + \"\\n\"\n",
        "        ) + message\n",
        "    elif type == \"output\":\n",
        "        message = (\n",
        "            \"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\"\n",
        "            + \"\\n\"\n",
        "        ) + message\n",
        "    os.makedirs(log_path, exist_ok=True)\n",
        "    with open(log_path + log_file, \"a\", encoding=\"utf-8\") as file:\n",
        "        file.write(message + \"\\n\")\n",
        "\n",
        "\n",
        "def compare(index1_path, index2_path):\n",
        "    with open(\"./data/prompt/0-instruction.md\", \"r\", encoding=\"utf-8\") as file:\n",
        "        instruction = file.read()\n",
        "\n",
        "    # Ollama doesn't have the concept of persistent assistants like OpenAI.\n",
        "    # We'll simulate the assistant behavior within each 'compare' call.\n",
        "    # The 'instruction' will be part of the initial prompt.\n",
        "\n",
        "    # We'll maintain a list of messages to represent the thread\n",
        "    thread_messages = []\n",
        "\n",
        "    # first message\n",
        "    with open(\"./data/prompt/1-preamble.md\", \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "    log_to_file(\"input\", content)\n",
        "    thread_messages.append({\"role\": \"user\", \"content\": content})\n",
        "    response = client.chat(model=\"llama3.2\", messages=thread_messages)\n",
        "    log_to_file(\"output\", response['message']['content'])\n",
        "    thread_messages.append(response['message'])\n",
        "\n",
        "    # --- second message - 上证50 ---\n",
        "    with open(\"./data/prompt/2-上証50.md\", \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "    log_to_file(\"input\", content)\n",
        "    log_to_file(\"input\", \"./data/上証50.xlsx\")\n",
        "    try:\n",
        "        # Read the Excel file (adjust 'sheet_name' and how you extract relevant info)\n",
        "        shangzheng_data = pd.read_excel(\"./data/上証50.xlsx\", sheet_name=\"Sheet1\").head().to_string() # Example: Read first few rows\n",
        "        content_with_file = f\"{content}\\nData from ./data/上証50.xlsx (first few rows):\\n{shangzheng_data}\"\n",
        "    except Exception as e:\n",
        "        content_with_file = f\"{content}\\nCould not read data from ./data/上証50.xlsx: {e}\"\n",
        "    thread_messages.append({\"role\": \"user\", \"content\": content_with_file})\n",
        "    response = client.chat(model=\"llama3.2\", messages=thread_messages)\n",
        "    log_to_file(\"output\", response['message']['content'])\n",
        "    thread_messages.append(response['message'])\n",
        "\n",
        "    # third message - index 1\n",
        "    with open(\"./data/prompt/3-index1.md\", \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "    log_to_file(\"input\", content)\n",
        "    log_to_file(\"input\", index1_path)\n",
        "    # Similar to the 上证50.xlsx, we'll include a placeholder for the file content.\n",
        "    content_with_file = content + f\"\\n(Data from {index1_path} will be provided in the next step)\"\n",
        "    thread_messages.append({\"role\": \"user\", \"content\": content_with_file})\n",
        "    response = client.chat(model=\"llama3.2\", messages=thread_messages)\n",
        "    log_to_file(\"output\", response['message']['content'])\n",
        "    thread_messages.append(response['message'])\n",
        "\n",
        "    # fourth message - index 2\n",
        "    with open(\"./data/prompt/4-index2.md\", \"r\", encoding=\"utf-8\") as file:\n",
        "        content = file.read()\n",
        "    log_to_file(\"input\", content)\n",
        "    log_to_file(\"input\", index2_path)\n",
        "    # Now, we'll provide the content of both index files and ask for comparison.\n",
        "    try:\n",
        "        with open(index1_path, \"r\", encoding=\"utf-8\") as f1:\n",
        "            index1_content = f1.read()\n",
        "    except Exception as e:\n",
        "        index1_content = f\"Could not read file: {index1_path} - {e}\"\n",
        "\n",
        "    try:\n",
        "        with open(index2_path, \"r\", encoding=\"utf-8\") as f2:\n",
        "            index2_content = f2.read()\n",
        "    except Exception as e:\n",
        "        index2_content = f\"Could not read file: {index2_path} - {e}\"\n",
        "\n",
        "    comparison_prompt = f\"\"\"{content}\n",
        "\n",
        "    Data from {index1_path}:\n",
        "\n",
        "    {index1_content}\n",
        "\n",
        "    Data from {index2_path}:\n",
        "\n",
        "    {index2_content}\n",
        "\n",
        "    YOU MUST NOT GENERATE ANY PYTHON CODES.\n",
        "\n",
        "    Based on the above data and any insights from the 上证50 data provided earlier, which index is better? Please respond with only the number \"1\" if {os.path.basename(index1_path)} is better, or the number \"2\" if {os.path.basename(index2_path)} is better.\n",
        "    \"\"\"\n",
        "    thread_messages.append({\"role\": \"user\", \"content\": comparison_prompt})\n",
        "    response = client.chat(model=\"llama3.2\", messages=thread_messages)\n",
        "    log_to_file(\"output\", response['message']['content'])\n",
        "    thread_messages.append(response['message'])\n",
        "\n",
        "    # --- Improved decision extraction using regex ---\n",
        "    index_match = re.search(r\"\\b(1|2)\\b\", response['message']['content'])\n",
        "    if index_match:\n",
        "        index = index_match.group(1)\n",
        "    else:\n",
        "        print(f\"Warning: Could not determine the better index from the response: {response['message']['content']}\")\n",
        "        index = \"1\" # Default to 1 if unsure\n",
        "\n",
        "    log_to_file(\"output\", f\"The selected better alpha's index is: {index}\")\n",
        "    return index\n",
        "\n",
        "\n",
        "# list files in ./data/alpha-result/\n",
        "files = os.listdir(\"./data/alpha-result/\")\n",
        "files = [f for f in files if f.endswith(\".xlsx\") or f.endswith(\".csv\") or f.endswith(\".txt\")] # Add other relevant file extensions\n",
        "\n",
        "best_file = files[0]\n",
        "best_file_index = 1\n",
        "round_num = 1\n",
        "for i, file in enumerate(tqdm(files[1:])):\n",
        "    index = i + 2\n",
        "    log_file = f\"round-{round_num}-{best_file_index}-{index}.log\"\n",
        "    best_index = compare(\n",
        "        f\"./data/alpha-result/{best_file}\", f\"./data/alpha-result/{file}\"\n",
        "    )\n",
        "    if best_index == \"2\":\n",
        "        best_file = file\n",
        "        best_file_index = index\n",
        "    round_num += 1\n",
        "\n",
        "print(f\"The best alpha is: {best_file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
