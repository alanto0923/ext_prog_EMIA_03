{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hk_tickers = [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', \n",
        "                  \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', \n",
        "                  \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                  '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', \n",
        "                  '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK']\n",
        "len(hk_tickers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Human review/ correction (if there is a minor mistake from LLM output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded 'HK_final_comprehensive_result_3.csv'. Original rows: 16\n",
            "Removed 1 duplicate row(s) based on 'domain' and 'code'.\n",
            "Successfully saved the updated data back to 'HK_final_comprehensive_result_3.csv'. Final rows: 15\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# Define the filename\n",
        "filename = 'HK_final_comprehensive_result_3.csv'\n",
        "\n",
        "try:\n",
        "    # --- Load the CSV file ---\n",
        "    # In your actual use, replace the io.StringIO part with just the filename:\n",
        "    df = pd.read_csv(filename)\n",
        "    # df = pd.read_csv(io.StringIO(csv_data))\n",
        "    print(f\"Successfully loaded '{filename}'. Original rows: {len(df)}\")\n",
        "    # print(\"Original DataFrame head:\\n\", df.head()) # Optional: view data\n",
        "\n",
        "    # --- Identify and remove the duplicate based on 'code' ---\n",
        "    # We keep the 'first' occurrence and remove subsequent ones\n",
        "    # based on the 'domain' and 'code' columns being identical.\n",
        "    # This handles the specific case of row 1 being a duplicate of row 0\n",
        "    # based on these columns.\n",
        "    original_rows = len(df)\n",
        "    df_deduplicated = df.drop_duplicates(subset=['domain', 'code'], keep='first')\n",
        "    rows_removed = original_rows - len(df_deduplicated)\n",
        "\n",
        "    if rows_removed > 0:\n",
        "        print(f\"Removed {rows_removed} duplicate row(s) based on 'domain' and 'code'.\")\n",
        "    else:\n",
        "        print(\"No duplicate rows found based on 'domain' and 'code'.\")\n",
        "\n",
        "    # --- Save the modified DataFrame back to the exact same file ---\n",
        "    # index=False prevents pandas from writing the DataFrame index as a column\n",
        "    df_deduplicated.to_csv(filename, index=False)\n",
        "    print(f\"Successfully saved the updated data back to '{filename}'. Final rows: {len(df_deduplicated)}\")\n",
        "    # print(\"\\nFinal DataFrame head:\\n\", df_deduplicated.head()) # Optional: view updated data\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{filename}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Alpha factor calculation and importing relevant library for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ta in /opt/anaconda3/lib/python3.12/site-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from ta) (1.26.4)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from ta) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->ta) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->ta) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->ta) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install ta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching and processing data for ['0001.HK', '0002.HK', '0003.HK', '0005.HK', '0006.HK', '0011.HK', '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', '0322.HK', '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', '0992.HK', '1024.HK', '1038.HK', '1044.HK', '1088.HK', '1093.HK', '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK', '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK'] from 2020-03-24 00:00:00 to 2025-03-24 00:00:00...\n",
            "Calculating alpha factors for 0001.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0001.HK.\n",
            "Calculating alpha factors for 0002.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0002.HK.\n",
            "Calculating alpha factors for 0003.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0003.HK.\n",
            "Calculating alpha factors for 0005.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0005.HK.\n",
            "Calculating alpha factors for 0006.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0006.HK.\n",
            "Calculating alpha factors for 0011.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0011.HK.\n",
            "Calculating alpha factors for 0012.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0012.HK.\n",
            "Calculating alpha factors for 0016.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0016.HK.\n",
            "Calculating alpha factors for 0027.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0027.HK.\n",
            "Calculating alpha factors for 0066.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0066.HK.\n",
            "Calculating alpha factors for 0101.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0101.HK.\n",
            "Calculating alpha factors for 0175.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0175.HK.\n",
            "Calculating alpha factors for 0241.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0241.HK.\n",
            "Calculating alpha factors for 0267.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0267.HK.\n",
            "Calculating alpha factors for 0285.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0285.HK.\n",
            "Calculating alpha factors for 0288.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0288.HK.\n",
            "Calculating alpha factors for 0291.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0291.HK.\n",
            "Calculating alpha factors for 0316.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0316.HK.\n",
            "Calculating alpha factors for 0322.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0322.HK.\n",
            "Calculating alpha factors for 0386.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0386.HK.\n",
            "Calculating alpha factors for 0388.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0388.HK.\n",
            "Calculating alpha factors for 0669.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0669.HK.\n",
            "Calculating alpha factors for 0688.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0688.HK.\n",
            "Calculating alpha factors for 0700.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0700.HK.\n",
            "Calculating alpha factors for 0762.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0762.HK.\n",
            "Calculating alpha factors for 0823.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0823.HK.\n",
            "Calculating alpha factors for 0836.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0836.HK.\n",
            "Calculating alpha factors for 0857.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0857.HK.\n",
            "Calculating alpha factors for 0868.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0868.HK.\n",
            "Calculating alpha factors for 0881.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0881.HK.\n",
            "Calculating alpha factors for 0883.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0883.HK.\n",
            "Calculating alpha factors for 0939.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0939.HK.\n",
            "Calculating alpha factors for 0941.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0941.HK.\n",
            "Calculating alpha factors for 0960.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0960.HK.\n",
            "Calculating alpha factors for 0968.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0968.HK.\n",
            "Calculating alpha factors for 0981.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0981.HK.\n",
            "Calculating alpha factors for 0992.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 0992.HK.\n",
            "Calculating alpha factors for 1024.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1024.HK.\n",
            "Calculating alpha factors for 1038.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1038.HK.\n",
            "Calculating alpha factors for 1044.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1044.HK.\n",
            "Calculating alpha factors for 1088.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1088.HK.\n",
            "Calculating alpha factors for 1093.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1093.HK.\n",
            "Calculating alpha factors for 1099.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1099.HK.\n",
            "Calculating alpha factors for 1109.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1109.HK.\n",
            "Calculating alpha factors for 1113.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1113.HK.\n",
            "Calculating alpha factors for 1177.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1177.HK.\n",
            "Calculating alpha factors for 1209.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1209.HK.\n",
            "Calculating alpha factors for 1211.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1211.HK.\n",
            "Calculating alpha factors for 1299.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1299.HK.\n",
            "Calculating alpha factors for 1378.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1378.HK.\n",
            "Calculating alpha factors for 1398.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1398.HK.\n",
            "Calculating alpha factors for 1810.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1810.HK.\n",
            "Calculating alpha factors for 1876.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1876.HK.\n",
            "Calculating alpha factors for 1928.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1928.HK.\n",
            "Calculating alpha factors for 1929.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1929.HK.\n",
            "Calculating alpha factors for 1997.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 1997.HK.\n",
            "Calculating alpha factors for 2015.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2015.HK.\n",
            "Calculating alpha factors for 2020.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2020.HK.\n",
            "Calculating alpha factors for 2269.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2269.HK.\n",
            "Calculating alpha factors for 2313.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2313.HK.\n",
            "Calculating alpha factors for 2318.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2318.HK.\n",
            "Calculating alpha factors for 2319.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2319.HK.\n",
            "Calculating alpha factors for 2331.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2331.HK.\n",
            "Calculating alpha factors for 2359.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2359.HK.\n",
            "Calculating alpha factors for 2382.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2382.HK.\n",
            "Calculating alpha factors for 2388.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2388.HK.\n",
            "Calculating alpha factors for 2628.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2628.HK.\n",
            "Calculating alpha factors for 2688.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2688.HK.\n",
            "Calculating alpha factors for 2899.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 2899.HK.\n",
            "Calculating alpha factors for 3690.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 3690.HK.\n",
            "Calculating alpha factors for 3692.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 3692.HK.\n",
            "Calculating alpha factors for 3968.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 3968.HK.\n",
            "Calculating alpha factors for 3988.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 3988.HK.\n",
            "Calculating alpha factors for 6618.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 6618.HK.\n",
            "Calculating alpha factors for 6690.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 6690.HK.\n",
            "Calculating alpha factors for 6862.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 6862.HK.\n",
            "Calculating alpha factors for 9618.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 9618.HK.\n",
            "Calculating alpha factors for 9633.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 9633.HK.\n",
            "Calculating alpha factors for 9888.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 9888.HK.\n",
            "Calculating alpha factors for 9901.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 9901.HK.\n",
            "Calculating alpha factors for 9961.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 9961.HK.\n",
            "Calculating alpha factors for 9988.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 9988.HK.\n",
            "Calculating alpha factors for 9999.HK...\n",
            "Calculating ATR using 'ta' library...\n",
            "Calculating RSI using 'ta' library...\n",
            "Finished calculating for 9999.HK.\n",
            "\n",
            "Combining dataframes...\n",
            "\n",
            "--- Final Combined DataFrame ---\n",
            "Shape: (100025, 17)\n",
            "\n",
            "--- Sample (First 10 rows) ---\n",
            "Price       Date stock_id  ATR_14d  Daily_High_Low_Range  EMA_20d  \\\n",
            "0     2020-03-24  0001.HK      0.0              1.379356      NaN   \n",
            "1     2020-03-25  0001.HK      0.0              1.034517      NaN   \n",
            "2     2020-03-26  0001.HK      0.0              1.685881      NaN   \n",
            "3     2020-03-27  0001.HK      0.0              1.417674      NaN   \n",
            "4     2020-03-30  0001.HK      0.0              1.455989      NaN   \n",
            "5     2020-03-31  0001.HK      0.0              0.651362      NaN   \n",
            "6     2020-04-01  0001.HK      0.0              1.379357      NaN   \n",
            "7     2020-04-02  0001.HK      0.0              1.111149      NaN   \n",
            "8     2020-04-03  0001.HK      0.0              0.804624      NaN   \n",
            "9     2020-04-06  0001.HK      0.0              2.298928      NaN   \n",
            "\n",
            "Price  MA_Crossover_10_50  Mean_Reversion_20d  Moving_Average_Reversion  \\\n",
            "0                     NaN                 NaN                       NaN   \n",
            "1                     NaN                 NaN                       NaN   \n",
            "2                     NaN                 NaN                       NaN   \n",
            "3                     NaN                 NaN                       NaN   \n",
            "4                     NaN                 NaN                       NaN   \n",
            "5                     NaN                 NaN                       NaN   \n",
            "6                     NaN                 NaN                       NaN   \n",
            "7                     NaN                 NaN                       NaN   \n",
            "8                     NaN                 NaN                       NaN   \n",
            "9                     NaN                 NaN                       NaN   \n",
            "\n",
            "Price  Normalized_BBW_20d_2std  Price_Momentum_10d  ROC_50d  RSI_14d  SMA_20d  \\\n",
            "0                          NaN                 NaN      NaN      NaN      NaN   \n",
            "1                          NaN                 NaN      NaN      NaN      NaN   \n",
            "2                          NaN                 NaN      NaN      NaN      NaN   \n",
            "3                          NaN                 NaN      NaN      NaN      NaN   \n",
            "4                          NaN                 NaN      NaN      NaN      NaN   \n",
            "5                          NaN                 NaN      NaN      NaN      NaN   \n",
            "6                          NaN                 NaN      NaN      NaN      NaN   \n",
            "7                          NaN                 NaN      NaN      NaN      NaN   \n",
            "8                          NaN                 NaN      NaN      NaN      NaN   \n",
            "9                          NaN                 NaN      NaN      NaN      NaN   \n",
            "\n",
            "Price  Stochastic_Oscillator_14d  Trading_Volume  VROC_10d  \\\n",
            "0                            NaN        15296458       NaN   \n",
            "1                            NaN        16640222       NaN   \n",
            "2                            NaN        12067696       NaN   \n",
            "3                            NaN        18221966       NaN   \n",
            "4                            NaN        15936885       NaN   \n",
            "5                            NaN        10923688       NaN   \n",
            "6                            NaN        10568769       NaN   \n",
            "7                            NaN         7648777       NaN   \n",
            "8                            NaN         7014328       NaN   \n",
            "9                            NaN        11769045       NaN   \n",
            "\n",
            "Price  Volume_Momentum_50d  \n",
            "0                      NaN  \n",
            "1                      NaN  \n",
            "2                      NaN  \n",
            "3                      NaN  \n",
            "4                      NaN  \n",
            "5                      NaN  \n",
            "6                      NaN  \n",
            "7                      NaN  \n",
            "8                      NaN  \n",
            "9                      NaN  \n",
            "\n",
            "--- Sample (Last 10 rows) ---\n",
            "Price        Date stock_id   ATR_14d  Daily_High_Low_Range     EMA_20d  \\\n",
            "100015 2025-03-11  9999.HK  5.634607              7.800003  159.367708   \n",
            "100016 2025-03-12  9999.HK  5.553564              4.500000  159.713641   \n",
            "100017 2025-03-13  9999.HK  5.428309              3.800003  159.836151   \n",
            "100018 2025-03-14  9999.HK  5.333429              4.099991  159.985088   \n",
            "100019 2025-03-17  9999.HK  5.445327              6.899994  159.700794   \n",
            "100020 2025-03-18  9999.HK  5.256375              2.800003  159.681671   \n",
            "100021 2025-03-19  9999.HK  5.238063              5.000000  159.731035   \n",
            "100022 2025-03-20  9999.HK  5.178202              4.400009  159.442365   \n",
            "100023 2025-03-21  9999.HK  5.201187              4.900009  158.724044   \n",
            "100024 2025-03-24  9999.HK  5.243960              5.699997  158.521755   \n",
            "\n",
            "Price   MA_Crossover_10_50  Mean_Reversion_20d  Moving_Average_Reversion  \\\n",
            "100015            4.200647           -3.791820                 -3.791820   \n",
            "100016            4.137127           -3.143842                 -3.143842   \n",
            "100017            4.225121           -1.261077                 -1.261077   \n",
            "100018            4.480900           -1.843174                 -1.843174   \n",
            "100019            4.544431            2.379366                  2.379366   \n",
            "100020            4.432549           -0.407722                 -0.407722   \n",
            "100021            4.174289           -1.210040                 -1.210040   \n",
            "100022            3.022003            2.232294                  2.232294   \n",
            "100023            1.759711            6.639780                  6.639780   \n",
            "100024            0.935292            1.912050                  1.912050   \n",
            "\n",
            "Price   Normalized_BBW_20d_2std  Price_Momentum_10d    ROC_50d    RSI_14d  \\\n",
            "100015                 0.100239            6.185116  13.769773  57.525436   \n",
            "100016                 0.095643            2.225897  14.725749  56.236718   \n",
            "100017                 0.093773            3.036186  13.637508  53.037777   \n",
            "100018                 0.089177            4.426515  15.301180  53.606177   \n",
            "100019                 0.090186            2.774758  12.963628  46.883946   \n",
            "100020                 0.083457            2.141484  16.265853  50.669154   \n",
            "100021                 0.081710            1.392403  17.546056  51.706898   \n",
            "100022                 0.082528           -4.625688  14.308009  46.445822   \n",
            "100023                 0.091520           -6.002483  10.645385  40.378045   \n",
            "100024                 0.091870           -3.213842  10.766067  47.597101   \n",
            "\n",
            "Price      SMA_20d  Stochastic_Oscillator_14d  Trading_Volume   VROC_10d  \\\n",
            "100015  160.008183                  87.975809         6720457 -26.285892   \n",
            "100016  159.856158                  81.692667         6306770 -24.082907   \n",
            "100017  159.738923                  69.881507         4759937 -37.951647   \n",
            "100018  159.556820                  72.243703         4490257 -61.900225   \n",
            "100019  159.379366                  46.259188         7101968   2.140414   \n",
            "100020  159.092278                  61.023137         6361971  -9.442467   \n",
            "100021  158.989957                  65.157025         6040521  20.028363   \n",
            "100022  158.932291                  44.487496         5602934 -26.169430   \n",
            "100023  158.539774                  16.140695         7816314 -34.458375   \n",
            "100024  158.512056                  36.241650         5898773 -45.526394   \n",
            "\n",
            "Price   Volume_Momentum_50d  \n",
            "100015            4706493.0  \n",
            "100016            6306770.0  \n",
            "100017            2970647.0  \n",
            "100018            1553649.0  \n",
            "100019            7101968.0  \n",
            "100020            2009660.0  \n",
            "100021             -24156.0  \n",
            "100022            3315930.0  \n",
            "100023            1515005.0  \n",
            "100024           -1903272.0  \n",
            "\n",
            "--- DataFrame Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100025 entries, 0 to 100024\n",
            "Data columns (total 17 columns):\n",
            " #   Column                     Non-Null Count   Dtype         \n",
            "---  ------                     --------------   -----         \n",
            " 0   Date                       100025 non-null  datetime64[ns]\n",
            " 1   stock_id                   100025 non-null  object        \n",
            " 2   ATR_14d                    100025 non-null  float64       \n",
            " 3   Daily_High_Low_Range       100025 non-null  float64       \n",
            " 4   EMA_20d                    98448 non-null   float64       \n",
            " 5   MA_Crossover_10_50         95958 non-null   float64       \n",
            " 6   Mean_Reversion_20d         98448 non-null   float64       \n",
            " 7   Moving_Average_Reversion   98448 non-null   float64       \n",
            " 8   Normalized_BBW_20d_2std    98448 non-null   float64       \n",
            " 9   Price_Momentum_10d         99195 non-null   float64       \n",
            " 10  ROC_50d                    95875 non-null   float64       \n",
            " 11  RSI_14d                    98946 non-null   float64       \n",
            " 12  SMA_20d                    98448 non-null   float64       \n",
            " 13  Stochastic_Oscillator_14d  98946 non-null   float64       \n",
            " 14  Trading_Volume             100025 non-null  int64         \n",
            " 15  VROC_10d                   98473 non-null   float64       \n",
            " 16  Volume_Momentum_50d        95875 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(14), int64(1), object(1)\n",
            "memory usage: 13.0+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ta # Still used for ATR, RSI initially\n",
        "from datetime import datetime, timedelta\n",
        "import yfinance as yf\n",
        "\n",
        "# Helper function to ensure required columns exist\n",
        "def _check_columns(df):\n",
        "    required_cols = {'Open', 'High', 'Low', 'Close', 'Volume'}\n",
        "    if not required_cols.issubset(df.columns):\n",
        "        missing_cols = required_cols - set(df.columns)\n",
        "        raise ValueError(f\"Input DataFrame missing required columns: {missing_cols}. Available columns: {list(df.columns)}\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Alpha Factor Calculation Functions (Manual Pandas Implementation)\n",
        "# Using 'Close' column name\n",
        "# --------------------------------------------------\n",
        "\n",
        "# == Momentum Domain ==\n",
        "def calculate_price_momentum_10d(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    close_delayed = df[price_col].shift(10)\n",
        "    momentum = (df[price_col] - close_delayed) / close_delayed\n",
        "    return momentum.replace([np.inf, -np.inf], np.nan) * 100\n",
        "\n",
        "def calculate_ma_crossover_10_50(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    sma10 = df[price_col].rolling(window=10, min_periods=10).mean()\n",
        "    sma50 = df[price_col].rolling(window=50, min_periods=50).mean()\n",
        "    return sma10 - sma50\n",
        "\n",
        "def calculate_volume_momentum_50d(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df)\n",
        "    return df['Volume'] - df['Volume'].shift(50)\n",
        "\n",
        "def calculate_roc_50d(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    close_delayed = df[price_col].shift(50)\n",
        "    roc = ((df[price_col] - close_delayed) / close_delayed) * 100\n",
        "    return roc.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# == Mean Reversion Domain ==\n",
        "def calculate_mean_reversion_20d(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    sma20 = df[price_col].rolling(window=20, min_periods=20).mean()\n",
        "    return sma20 - df[price_col]\n",
        "\n",
        "def calculate_moving_average_reversion(df: pd.DataFrame) -> pd.Series:\n",
        "    # Identical to mean_reversion_20d\n",
        "    return calculate_mean_reversion_20d(df)\n",
        "\n",
        "def calculate_stochastic_oscillator_14d(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    low_14 = df['Low'].rolling(window=14, min_periods=14).min()\n",
        "    high_14 = df['High'].rolling(window=14, min_periods=14).max()\n",
        "    denominator = high_14 - low_14\n",
        "    stoch_k = 100 * (df[price_col] - low_14) / denominator.replace(0, np.nan)\n",
        "    return stoch_k.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# == Volatility Domain ==\n",
        "def calculate_atr_14d(df: pd.DataFrame) -> pd.Series:\n",
        "    # Keeping ta for now for ATR\n",
        "    _check_columns(df);\n",
        "    print(\"Calculating ATR using 'ta' library...\") # Add trace\n",
        "    try:\n",
        "        atr = ta.volatility.AverageTrueRange(\n",
        "            high=df['High'], low=df['Low'], close=df['Close'], window=14\n",
        "        ).average_true_range()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating ATR with ta: {e}\")\n",
        "        atr = pd.Series(np.nan, index=df.index)\n",
        "    return atr\n",
        "\n",
        "def calculate_daily_high_low_range(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df)\n",
        "    return df['High'] - df['Low']\n",
        "\n",
        "def calculate_normalized_bollinger_band_width_20d_2std(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    sma20 = df[price_col].rolling(window=20, min_periods=20).mean()\n",
        "    std20 = df[price_col].rolling(window=20, min_periods=20).std(ddof=1)\n",
        "    bbw_normalized = (4 * std20) / sma20\n",
        "    return bbw_normalized.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# == Liquidity Domain ==\n",
        "def calculate_volume_roc_10d(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df)\n",
        "    vol_delayed = df['Volume'].shift(10)\n",
        "    vroc = ((df['Volume'] / vol_delayed) - 1) * 100\n",
        "    return vroc.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# == Technical Domain ==\n",
        "def calculate_sma_20d(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    sma20 = df[price_col].rolling(window=20, min_periods=20).mean()\n",
        "    return sma20\n",
        "\n",
        "def calculate_ema_20d(df: pd.DataFrame) -> pd.Series:\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    ema20 = df[price_col].ewm(span=20, adjust=False, min_periods=20).mean()\n",
        "    return ema20\n",
        "\n",
        "def calculate_rsi_14d(df: pd.DataFrame) -> pd.Series:\n",
        "    # Keeping ta for now for RSI\n",
        "    _check_columns(df); price_col = 'Close'\n",
        "    print(\"Calculating RSI using 'ta' library...\") # Add trace\n",
        "    try:\n",
        "        rsi = ta.momentum.RSIIndicator(close=df[price_col], window=14).rsi()\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating RSI with ta: {e}\")\n",
        "        rsi = pd.Series(np.nan, index=df.index)\n",
        "    return rsi\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Main Calculation Function\n",
        "# --------------------------------------------------\n",
        "\n",
        "def calculate_all_alpha_factors(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df_out = df.copy()\n",
        "    # Momentum\n",
        "    df_out['Price_Momentum_10d'] = calculate_price_momentum_10d(df_out)\n",
        "    # df_out['ROC_10d'] = calculate_roc_10d(df_out) # REMOVED\n",
        "    df_out['MA_Crossover_10_50'] = calculate_ma_crossover_10_50(df_out)\n",
        "    df_out['Volume_Momentum_50d'] = calculate_volume_momentum_50d(df_out)\n",
        "    df_out['ROC_50d'] = calculate_roc_50d(df_out)\n",
        "    # Mean Reversion\n",
        "    df_out['Mean_Reversion_20d'] = calculate_mean_reversion_20d(df_out)\n",
        "    df_out['Moving_Average_Reversion'] = calculate_moving_average_reversion(df_out)\n",
        "    df_out['Stochastic_Oscillator_14d'] = calculate_stochastic_oscillator_14d(df_out)\n",
        "    # Volatility\n",
        "    df_out['ATR_14d'] = calculate_atr_14d(df_out) # Still uses ta\n",
        "    df_out['Daily_High_Low_Range'] = calculate_daily_high_low_range(df_out)\n",
        "    df_out['Normalized_BBW_20d_2std'] = calculate_normalized_bollinger_band_width_20d_2std(df_out)\n",
        "    # Liquidity\n",
        "    df_out['VROC_10d'] = calculate_volume_roc_10d(df_out)\n",
        "    # Adding Trading Volume as a factor is handled *after* this function call\n",
        "    # by renaming the existing 'Volume' column before final selection.\n",
        "    # Technical\n",
        "    df_out['SMA_20d'] = calculate_sma_20d(df_out)\n",
        "    df_out['EMA_20d'] = calculate_ema_20d(df_out)\n",
        "    df_out['RSI_14d'] = calculate_rsi_14d(df_out) # Still uses ta\n",
        "    return df_out\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Example Usage\n",
        "# --------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tickers = [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', \n",
        "                  \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', \n",
        "                  \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                  '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', \n",
        "                  '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK']\n",
        "    start_date = datetime.strptime(\"2020-03-24\",'%Y-%m-%d')\n",
        "    end_date = datetime.strptime(\"2025-03-24\",'%Y-%m-%d')\n",
        "\n",
        "    processed_data_list = []\n",
        "    alpha_factor_columns = None\n",
        "\n",
        "    # Calculate the end_date for yfinance download (exclusive)\n",
        "    yf_end_date = end_date + timedelta(days=1)\n",
        "\n",
        "    print(f\"Fetching and processing data for {tickers} from {start_date} to {end_date}...\")\n",
        "    for ticker in tickers:\n",
        "        data = None\n",
        "        try:\n",
        "            data = yf.download(ticker, start=start_date, end=yf_end_date,\n",
        "                               auto_adjust=True, progress=False)\n",
        "\n",
        "            if data.empty:\n",
        "                print(f\"No data found for {ticker} in the specified date range.\")\n",
        "                continue\n",
        "\n",
        "            if isinstance(data.columns, pd.MultiIndex):\n",
        "                data.columns = data.columns.get_level_values(0)\n",
        "\n",
        "            print(f\"Calculating alpha factors for {ticker}...\")\n",
        "            alpha_data = calculate_all_alpha_factors(data.copy())\n",
        "            print(f\"Finished calculating for {ticker}.\")\n",
        "\n",
        "            # --- Post-processing for final DataFrame format ---\n",
        "            alpha_data['stock_id'] = ticker\n",
        "            alpha_data.reset_index(inplace=True)\n",
        "\n",
        "            # Handle Date column naming\n",
        "            date_col_name = None\n",
        "            if 'Date' in alpha_data.columns: date_col_name = 'Date'\n",
        "            elif 'Datetime' in alpha_data.columns: date_col_name = 'Datetime'; alpha_data.rename(columns={'Datetime': 'Date'}, inplace=True)\n",
        "            elif 'index' in alpha_data.columns: date_col_name = 'index'; alpha_data.rename(columns={'index': 'Date'}, inplace=True)\n",
        "            else: print(f\"Warning: Could not find Date column for {ticker}. Skipping.\"); continue\n",
        "\n",
        "            # --- ADD TRADING VOLUME ---\n",
        "            # Explicitly rename 'Volume' to 'Trading_Volume' so it's treated as a factor\n",
        "            if 'Volume' in alpha_data.columns:\n",
        "                alpha_data.rename(columns={'Volume': 'Trading_Volume'}, inplace=True)\n",
        "            # --- End Add Trading Volume ---\n",
        "\n",
        "\n",
        "            # Dynamically identify alpha factor columns (only once)\n",
        "            if alpha_factor_columns is None:\n",
        "                # Define original cols expected from yfinance *before* rename + added ones\n",
        "                # 'Volume' is no longer original, it's renamed to 'Trading_Volume'\n",
        "                original_cols = {'Open', 'High', 'Low', 'Close', 'Date', 'stock_id'}\n",
        "                alpha_factor_columns = sorted([col for col in alpha_data.columns if col not in original_cols])\n",
        "                # Ensure 'Trading_Volume' is captured if the rename happened\n",
        "                if 'Trading_Volume' not in alpha_factor_columns and 'Trading_Volume' in alpha_data.columns:\n",
        "                     alpha_factor_columns.append('Trading_Volume')\n",
        "                     alpha_factor_columns.sort()\n",
        "\n",
        "\n",
        "            # Select only the necessary columns for the final DF\n",
        "            columns_to_keep_final = ['Date', 'stock_id'] + alpha_factor_columns\n",
        "            alpha_data_filtered = alpha_data[[col for col in columns_to_keep_final if col in alpha_data.columns]].copy()\n",
        "\n",
        "            processed_data_list.append(alpha_data_filtered)\n",
        "            # --- End of Post-processing ---\n",
        "\n",
        "        except ValueError as ve:\n",
        "             print(f\"Could not process data for {ticker}: {ve}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred while processing {ticker}: {e}\")\n",
        "\n",
        "\n",
        "    # --- Combine and Finalize ---\n",
        "    if processed_data_list:\n",
        "        print(\"\\nCombining dataframes...\")\n",
        "        final_df = pd.concat(processed_data_list, ignore_index=True)\n",
        "\n",
        "        final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
        "        final_df.sort_values(by=['stock_id', 'Date'], inplace=True)\n",
        "        final_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        print(\"\\n--- Final Combined DataFrame ---\")\n",
        "        print(f\"Shape: {final_df.shape}\")\n",
        "        print(\"\\n--- Sample (First 10 rows) ---\")\n",
        "        print(final_df.head(10))\n",
        "        print(\"\\n--- Sample (Last 10 rows) ---\")\n",
        "        print(final_df.tail(10))\n",
        "        print(\"\\n--- DataFrame Info ---\")\n",
        "        final_df.info()\n",
        "    else:\n",
        "        print(\"\\nNo data processed, cannot create final DataFrame.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Price</th>\n",
              "      <th>Date</th>\n",
              "      <th>stock_id</th>\n",
              "      <th>ATR_14d</th>\n",
              "      <th>Daily_High_Low_Range</th>\n",
              "      <th>EMA_20d</th>\n",
              "      <th>MA_Crossover_10_50</th>\n",
              "      <th>Mean_Reversion_20d</th>\n",
              "      <th>Moving_Average_Reversion</th>\n",
              "      <th>Normalized_BBW_20d_2std</th>\n",
              "      <th>Price_Momentum_10d</th>\n",
              "      <th>ROC_50d</th>\n",
              "      <th>RSI_14d</th>\n",
              "      <th>SMA_20d</th>\n",
              "      <th>Stochastic_Oscillator_14d</th>\n",
              "      <th>Trading_Volume</th>\n",
              "      <th>VROC_10d</th>\n",
              "      <th>Volume_Momentum_50d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-03-24</td>\n",
              "      <td>0001.HK</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.379356</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15296458</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-03-25</td>\n",
              "      <td>0001.HK</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.034517</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>16640222</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-03-26</td>\n",
              "      <td>0001.HK</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.685881</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12067696</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-03-27</td>\n",
              "      <td>0001.HK</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.417674</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18221966</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-03-30</td>\n",
              "      <td>0001.HK</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.455989</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15936885</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100020</th>\n",
              "      <td>2025-03-18</td>\n",
              "      <td>9999.HK</td>\n",
              "      <td>5.256375</td>\n",
              "      <td>2.800003</td>\n",
              "      <td>159.681671</td>\n",
              "      <td>4.432549</td>\n",
              "      <td>-0.407722</td>\n",
              "      <td>-0.407722</td>\n",
              "      <td>0.083457</td>\n",
              "      <td>2.141484</td>\n",
              "      <td>16.265853</td>\n",
              "      <td>50.669154</td>\n",
              "      <td>159.092278</td>\n",
              "      <td>61.023137</td>\n",
              "      <td>6361971</td>\n",
              "      <td>-9.442467</td>\n",
              "      <td>2009660.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100021</th>\n",
              "      <td>2025-03-19</td>\n",
              "      <td>9999.HK</td>\n",
              "      <td>5.238063</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>159.731035</td>\n",
              "      <td>4.174289</td>\n",
              "      <td>-1.210040</td>\n",
              "      <td>-1.210040</td>\n",
              "      <td>0.081710</td>\n",
              "      <td>1.392403</td>\n",
              "      <td>17.546056</td>\n",
              "      <td>51.706898</td>\n",
              "      <td>158.989957</td>\n",
              "      <td>65.157025</td>\n",
              "      <td>6040521</td>\n",
              "      <td>20.028363</td>\n",
              "      <td>-24156.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100022</th>\n",
              "      <td>2025-03-20</td>\n",
              "      <td>9999.HK</td>\n",
              "      <td>5.178202</td>\n",
              "      <td>4.400009</td>\n",
              "      <td>159.442365</td>\n",
              "      <td>3.022003</td>\n",
              "      <td>2.232294</td>\n",
              "      <td>2.232294</td>\n",
              "      <td>0.082528</td>\n",
              "      <td>-4.625688</td>\n",
              "      <td>14.308009</td>\n",
              "      <td>46.445822</td>\n",
              "      <td>158.932291</td>\n",
              "      <td>44.487496</td>\n",
              "      <td>5602934</td>\n",
              "      <td>-26.169430</td>\n",
              "      <td>3315930.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100023</th>\n",
              "      <td>2025-03-21</td>\n",
              "      <td>9999.HK</td>\n",
              "      <td>5.201187</td>\n",
              "      <td>4.900009</td>\n",
              "      <td>158.724044</td>\n",
              "      <td>1.759711</td>\n",
              "      <td>6.639780</td>\n",
              "      <td>6.639780</td>\n",
              "      <td>0.091520</td>\n",
              "      <td>-6.002483</td>\n",
              "      <td>10.645385</td>\n",
              "      <td>40.378045</td>\n",
              "      <td>158.539774</td>\n",
              "      <td>16.140695</td>\n",
              "      <td>7816314</td>\n",
              "      <td>-34.458375</td>\n",
              "      <td>1515005.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100024</th>\n",
              "      <td>2025-03-24</td>\n",
              "      <td>9999.HK</td>\n",
              "      <td>5.243960</td>\n",
              "      <td>5.699997</td>\n",
              "      <td>158.521755</td>\n",
              "      <td>0.935292</td>\n",
              "      <td>1.912050</td>\n",
              "      <td>1.912050</td>\n",
              "      <td>0.091870</td>\n",
              "      <td>-3.213842</td>\n",
              "      <td>10.766067</td>\n",
              "      <td>47.597101</td>\n",
              "      <td>158.512056</td>\n",
              "      <td>36.241650</td>\n",
              "      <td>5898773</td>\n",
              "      <td>-45.526394</td>\n",
              "      <td>-1903272.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100025 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Price        Date stock_id   ATR_14d  Daily_High_Low_Range     EMA_20d  \\\n",
              "0      2020-03-24  0001.HK  0.000000              1.379356         NaN   \n",
              "1      2020-03-25  0001.HK  0.000000              1.034517         NaN   \n",
              "2      2020-03-26  0001.HK  0.000000              1.685881         NaN   \n",
              "3      2020-03-27  0001.HK  0.000000              1.417674         NaN   \n",
              "4      2020-03-30  0001.HK  0.000000              1.455989         NaN   \n",
              "...           ...      ...       ...                   ...         ...   \n",
              "100020 2025-03-18  9999.HK  5.256375              2.800003  159.681671   \n",
              "100021 2025-03-19  9999.HK  5.238063              5.000000  159.731035   \n",
              "100022 2025-03-20  9999.HK  5.178202              4.400009  159.442365   \n",
              "100023 2025-03-21  9999.HK  5.201187              4.900009  158.724044   \n",
              "100024 2025-03-24  9999.HK  5.243960              5.699997  158.521755   \n",
              "\n",
              "Price   MA_Crossover_10_50  Mean_Reversion_20d  Moving_Average_Reversion  \\\n",
              "0                      NaN                 NaN                       NaN   \n",
              "1                      NaN                 NaN                       NaN   \n",
              "2                      NaN                 NaN                       NaN   \n",
              "3                      NaN                 NaN                       NaN   \n",
              "4                      NaN                 NaN                       NaN   \n",
              "...                    ...                 ...                       ...   \n",
              "100020            4.432549           -0.407722                 -0.407722   \n",
              "100021            4.174289           -1.210040                 -1.210040   \n",
              "100022            3.022003            2.232294                  2.232294   \n",
              "100023            1.759711            6.639780                  6.639780   \n",
              "100024            0.935292            1.912050                  1.912050   \n",
              "\n",
              "Price   Normalized_BBW_20d_2std  Price_Momentum_10d    ROC_50d    RSI_14d  \\\n",
              "0                           NaN                 NaN        NaN        NaN   \n",
              "1                           NaN                 NaN        NaN        NaN   \n",
              "2                           NaN                 NaN        NaN        NaN   \n",
              "3                           NaN                 NaN        NaN        NaN   \n",
              "4                           NaN                 NaN        NaN        NaN   \n",
              "...                         ...                 ...        ...        ...   \n",
              "100020                 0.083457            2.141484  16.265853  50.669154   \n",
              "100021                 0.081710            1.392403  17.546056  51.706898   \n",
              "100022                 0.082528           -4.625688  14.308009  46.445822   \n",
              "100023                 0.091520           -6.002483  10.645385  40.378045   \n",
              "100024                 0.091870           -3.213842  10.766067  47.597101   \n",
              "\n",
              "Price      SMA_20d  Stochastic_Oscillator_14d  Trading_Volume   VROC_10d  \\\n",
              "0              NaN                        NaN        15296458        NaN   \n",
              "1              NaN                        NaN        16640222        NaN   \n",
              "2              NaN                        NaN        12067696        NaN   \n",
              "3              NaN                        NaN        18221966        NaN   \n",
              "4              NaN                        NaN        15936885        NaN   \n",
              "...            ...                        ...             ...        ...   \n",
              "100020  159.092278                  61.023137         6361971  -9.442467   \n",
              "100021  158.989957                  65.157025         6040521  20.028363   \n",
              "100022  158.932291                  44.487496         5602934 -26.169430   \n",
              "100023  158.539774                  16.140695         7816314 -34.458375   \n",
              "100024  158.512056                  36.241650         5898773 -45.526394   \n",
              "\n",
              "Price   Volume_Momentum_50d  \n",
              "0                       NaN  \n",
              "1                       NaN  \n",
              "2                       NaN  \n",
              "3                       NaN  \n",
              "4                       NaN  \n",
              "...                     ...  \n",
              "100020            2009660.0  \n",
              "100021             -24156.0  \n",
              "100022            3315930.0  \n",
              "100023            1515005.0  \n",
              "100024           -1903272.0  \n",
              "\n",
              "[100025 rows x 17 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df.to_csv(\"alpha_data_values_1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: statsmodels in /opt/anaconda3/lib/python3.12/site-packages (0.14.2)\n",
            "Requirement already satisfied: yfinance in /opt/anaconda3/lib/python3.12/site-packages (0.2.55)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: requests>=2.31 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scipy matplotlib statsmodels yfinance tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.1.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.2)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_market_calendars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.12/site-packages (8.1.6)\n",
            "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (8.27.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (3.0.14)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.12/site-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# REAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ATR_14d',\n",
              " 'Daily_High_Low_Range',\n",
              " 'EMA_20d',\n",
              " 'MA_Crossover_10_50',\n",
              " 'Mean_Reversion_20d',\n",
              " 'Moving_Average_Reversion',\n",
              " 'Normalized_BBW_20d_2std',\n",
              " 'Price_Momentum_10d',\n",
              " 'ROC_50d',\n",
              " 'RSI_14d',\n",
              " 'SMA_20d',\n",
              " 'Stochastic_Oscillator_14d',\n",
              " 'Trading_Volume',\n",
              " 'VROC_10d',\n",
              " 'Volume_Momentum_50d']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alpha_factors_df = pd.read_csv('alpha_data_values_1.csv')\n",
        "alpha_factor_names = alpha_factors_df.drop(columns=['Date', 'stock_id'], errors='ignore').columns[1:].to_list()\n",
        "alpha_factor_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change it to MultiIndex\n",
        "alpha_factors_df = pd.read_csv('alpha_data_values_1.csv')\n",
        "alpha_factor_names = alpha_factors_df.drop(columns=['Date', 'stock_id'], errors='ignore').columns[1:].to_list()\n",
        "alpha_factors_df = alpha_factors_df.rename(columns={'Date': 'date', 'stock_id': 'asset'})\n",
        "alpha_factors = pd.DataFrame()\n",
        "\n",
        "alpha_data = {}\n",
        "for factor_name in alpha_factor_names:\n",
        "    \"\"\"\n",
        "    pivot_df = alpha_factors_df.pivot_table(index='date', columns='asset', values=factor_name)\n",
        "    pivot_df = pivot_df.reindex(dates)  # Reindex to ensure all dates are present\n",
        "    \n",
        "    # Create the MultiIndex and stack the pivoted DataFrame\n",
        "    multi_index = pd.MultiIndex.from_product([pivot_df.index, pivot_df.columns], names=['date', 'asset'])\n",
        "    stacked_series = pivot_df.stack().reindex(multi_index)\n",
        "    \n",
        "    alpha_factors[factor_name] = stacked_series\"\n",
        "    \"\"\"\n",
        "    pivot_df = alpha_factors_df.pivot_table(index='date', columns='asset', values=factor_name)\n",
        "\n",
        "    #pivot_df = pivot_df.reindex(dates)\n",
        "    #print(pivot_df)\n",
        "    #break\n",
        "\n",
        "    multi_index = pd.MultiIndex.from_product([pivot_df.index, pivot_df.columns], names=['date', 'asset'])\n",
        "\n",
        "    stacked_series = pivot_df.stack().reindex(multi_index)\n",
        "\n",
        "    alpha_factors[factor_name] = stacked_series\n",
        "\n",
        "alpha_factors # Print the first few rows to verify the result.\n",
        "\n",
        "alpha_factors.to_csv(\"processed_alpha_data_values_1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>ATR_14d</th>\n",
              "      <th>Daily_High_Low_Range</th>\n",
              "      <th>EMA_20d</th>\n",
              "      <th>MA_Crossover_10_50</th>\n",
              "      <th>Mean_Reversion_20d</th>\n",
              "      <th>Moving_Average_Reversion</th>\n",
              "      <th>Normalized_BBW_20d_2std</th>\n",
              "      <th>Price_Momentum_10d</th>\n",
              "      <th>ROC_50d</th>\n",
              "      <th>RSI_14d</th>\n",
              "      <th>SMA_20d</th>\n",
              "      <th>Stochastic_Oscillator_14d</th>\n",
              "      <th>Trading_Volume</th>\n",
              "      <th>VROC_10d</th>\n",
              "      <th>Volume_Momentum_50d</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th>asset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">2020-03-24</th>\n",
              "      <th>0001.HK</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.379356</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15296458.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0002.HK</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.383914</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9264804.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0003.HK</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.339107</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>48176115.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0005.HK</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.587237</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35692754.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0006.HK</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.422925</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6153012.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">2025-03-24</th>\n",
              "      <th>9888.HK</th>\n",
              "      <td>4.110813</td>\n",
              "      <td>2.800003</td>\n",
              "      <td>91.777740</td>\n",
              "      <td>6.512000</td>\n",
              "      <td>-2.932502</td>\n",
              "      <td>-2.932502</td>\n",
              "      <td>0.213538</td>\n",
              "      <td>1.684786</td>\n",
              "      <td>17.157176</td>\n",
              "      <td>53.872340</td>\n",
              "      <td>90.617501</td>\n",
              "      <td>45.675700</td>\n",
              "      <td>13278795.0</td>\n",
              "      <td>-42.728316</td>\n",
              "      <td>8914676.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9901.HK</th>\n",
              "      <td>1.812617</td>\n",
              "      <td>3.900002</td>\n",
              "      <td>39.011644</td>\n",
              "      <td>-1.117000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.173413</td>\n",
              "      <td>-3.125002</td>\n",
              "      <td>-23.770489</td>\n",
              "      <td>42.638680</td>\n",
              "      <td>38.120000</td>\n",
              "      <td>21.186449</td>\n",
              "      <td>19056392.0</td>\n",
              "      <td>348.918058</td>\n",
              "      <td>15877780.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9961.HK</th>\n",
              "      <td>20.539906</td>\n",
              "      <td>18.799988</td>\n",
              "      <td>497.756730</td>\n",
              "      <td>-14.091614</td>\n",
              "      <td>-27.012465</td>\n",
              "      <td>-27.012465</td>\n",
              "      <td>0.199746</td>\n",
              "      <td>3.574239</td>\n",
              "      <td>0.064281</td>\n",
              "      <td>53.140591</td>\n",
              "      <td>483.487535</td>\n",
              "      <td>81.715599</td>\n",
              "      <td>2491259.0</td>\n",
              "      <td>-53.013801</td>\n",
              "      <td>779344.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9988.HK</th>\n",
              "      <td>6.116542</td>\n",
              "      <td>3.700012</td>\n",
              "      <td>131.956694</td>\n",
              "      <td>22.201001</td>\n",
              "      <td>1.639997</td>\n",
              "      <td>1.639997</td>\n",
              "      <td>0.132679</td>\n",
              "      <td>-1.263938</td>\n",
              "      <td>64.153276</td>\n",
              "      <td>54.166481</td>\n",
              "      <td>134.440000</td>\n",
              "      <td>32.124384</td>\n",
              "      <td>90778453.0</td>\n",
              "      <td>-49.306023</td>\n",
              "      <td>39744597.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999.HK</th>\n",
              "      <td>5.243960</td>\n",
              "      <td>5.699997</td>\n",
              "      <td>158.521755</td>\n",
              "      <td>0.935292</td>\n",
              "      <td>1.912050</td>\n",
              "      <td>1.912050</td>\n",
              "      <td>0.091870</td>\n",
              "      <td>-3.213842</td>\n",
              "      <td>10.766067</td>\n",
              "      <td>47.597101</td>\n",
              "      <td>158.512056</td>\n",
              "      <td>36.241650</td>\n",
              "      <td>5898773.0</td>\n",
              "      <td>-45.526394</td>\n",
              "      <td>-1903272.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>102007 rows × 15 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      ATR_14d  Daily_High_Low_Range     EMA_20d  \\\n",
              "date       asset                                                  \n",
              "2020-03-24 0001.HK   0.000000              1.379356         NaN   \n",
              "           0002.HK   0.000000              2.383914         NaN   \n",
              "           0003.HK   0.000000              0.339107         NaN   \n",
              "           0005.HK   0.000000              0.587237         NaN   \n",
              "           0006.HK   0.000000              1.422925         NaN   \n",
              "...                       ...                   ...         ...   \n",
              "2025-03-24 9888.HK   4.110813              2.800003   91.777740   \n",
              "           9901.HK   1.812617              3.900002   39.011644   \n",
              "           9961.HK  20.539906             18.799988  497.756730   \n",
              "           9988.HK   6.116542              3.700012  131.956694   \n",
              "           9999.HK   5.243960              5.699997  158.521755   \n",
              "\n",
              "                    MA_Crossover_10_50  Mean_Reversion_20d  \\\n",
              "date       asset                                             \n",
              "2020-03-24 0001.HK                 NaN                 NaN   \n",
              "           0002.HK                 NaN                 NaN   \n",
              "           0003.HK                 NaN                 NaN   \n",
              "           0005.HK                 NaN                 NaN   \n",
              "           0006.HK                 NaN                 NaN   \n",
              "...                                ...                 ...   \n",
              "2025-03-24 9888.HK            6.512000           -2.932502   \n",
              "           9901.HK           -1.117000            0.920000   \n",
              "           9961.HK          -14.091614          -27.012465   \n",
              "           9988.HK           22.201001            1.639997   \n",
              "           9999.HK            0.935292            1.912050   \n",
              "\n",
              "                    Moving_Average_Reversion  Normalized_BBW_20d_2std  \\\n",
              "date       asset                                                        \n",
              "2020-03-24 0001.HK                       NaN                      NaN   \n",
              "           0002.HK                       NaN                      NaN   \n",
              "           0003.HK                       NaN                      NaN   \n",
              "           0005.HK                       NaN                      NaN   \n",
              "           0006.HK                       NaN                      NaN   \n",
              "...                                      ...                      ...   \n",
              "2025-03-24 9888.HK                 -2.932502                 0.213538   \n",
              "           9901.HK                  0.920000                 0.173413   \n",
              "           9961.HK                -27.012465                 0.199746   \n",
              "           9988.HK                  1.639997                 0.132679   \n",
              "           9999.HK                  1.912050                 0.091870   \n",
              "\n",
              "                    Price_Momentum_10d    ROC_50d    RSI_14d     SMA_20d  \\\n",
              "date       asset                                                           \n",
              "2020-03-24 0001.HK                 NaN        NaN        NaN         NaN   \n",
              "           0002.HK                 NaN        NaN        NaN         NaN   \n",
              "           0003.HK                 NaN        NaN        NaN         NaN   \n",
              "           0005.HK                 NaN        NaN        NaN         NaN   \n",
              "           0006.HK                 NaN        NaN        NaN         NaN   \n",
              "...                                ...        ...        ...         ...   \n",
              "2025-03-24 9888.HK            1.684786  17.157176  53.872340   90.617501   \n",
              "           9901.HK           -3.125002 -23.770489  42.638680   38.120000   \n",
              "           9961.HK            3.574239   0.064281  53.140591  483.487535   \n",
              "           9988.HK           -1.263938  64.153276  54.166481  134.440000   \n",
              "           9999.HK           -3.213842  10.766067  47.597101  158.512056   \n",
              "\n",
              "                    Stochastic_Oscillator_14d  Trading_Volume    VROC_10d  \\\n",
              "date       asset                                                            \n",
              "2020-03-24 0001.HK                        NaN      15296458.0         NaN   \n",
              "           0002.HK                        NaN       9264804.0         NaN   \n",
              "           0003.HK                        NaN      48176115.0         NaN   \n",
              "           0005.HK                        NaN      35692754.0         NaN   \n",
              "           0006.HK                        NaN       6153012.0         NaN   \n",
              "...                                       ...             ...         ...   \n",
              "2025-03-24 9888.HK                  45.675700      13278795.0  -42.728316   \n",
              "           9901.HK                  21.186449      19056392.0  348.918058   \n",
              "           9961.HK                  81.715599       2491259.0  -53.013801   \n",
              "           9988.HK                  32.124384      90778453.0  -49.306023   \n",
              "           9999.HK                  36.241650       5898773.0  -45.526394   \n",
              "\n",
              "                    Volume_Momentum_50d  \n",
              "date       asset                         \n",
              "2020-03-24 0001.HK                  NaN  \n",
              "           0002.HK                  NaN  \n",
              "           0003.HK                  NaN  \n",
              "           0005.HK                  NaN  \n",
              "           0006.HK                  NaN  \n",
              "...                                 ...  \n",
              "2025-03-24 9888.HK            8914676.0  \n",
              "           9901.HK           15877780.0  \n",
              "           9961.HK             779344.0  \n",
              "           9988.HK           39744597.0  \n",
              "           9999.HK           -1903272.0  \n",
              "\n",
              "[102007 rows x 15 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alpha_factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Start date localized to UTC: 2020-03-24 00:00:00+00:00\n",
            "INFO: End date localized to UTC: 2025-03-24 00:00:00+00:00\n",
            "\n",
            "--- Attempting to define universe based on index: ^HSI ---\n",
            "Attempting to get constituents for ^HSI on 2020-03-24.\n",
            "INFO: Using asset universe (Count: 83): ['0001.HK', '0002.HK', '0003.HK', '0005.HK', '0006.HK', '0011.HK', '0012.HK', '0016.HK', '0027.HK', '0066.HK']...\n",
            "Using pandas_market_calendars for HK business days. Full fetch range index length: 1319\n",
            "Target Analysis Date Range: 2020-03-24 00:00:00+00:00 to 2025-03-24 00:00:00+00:00 (1231 analysis days)\n",
            "\n",
            "--- Downloading Price and Total Volume Data ---\n",
            "Fetching data from 2019-12-16 to 2025-04-29 for 83 assets + benchmark ^HSI...\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  83 of 83 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Asset price/volume data processed. Shape: (1319, 83)\n",
            "Benchmark data processed. Length: 1319\n",
            "\n",
            "--- Fetching Industry Classification Data ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching Industries: 100%|██████████| 83/83 [01:18<00:00,  1.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created Static Industry Dummies shape: (83, 50)\n",
            "\n",
            "--- Loading/Defining Pre-calculated Factors ---\n",
            "Attempting to load factors from: processed_alpha_data_values_1.csv\n",
            "Successfully loaded factors from file. Initial shape: (102007, 15)\n",
            "INFO: Loaded data appears to be in Format 2 (Stacked). Unstacking...\n",
            "Successfully unstacked factors to Format 1 (Wide).\n",
            "INFO: Localizing factor index timezone...\n",
            "INFO: Converting factor index timezone...\n",
            "Reindexing loaded factors to match analysis dates (1231) and assets (83)...\n",
            "Final precalculated factors DataFrame ready. Shape: (1231, 1245)\n",
            "Available factors: ['ATR_14d', 'Daily_High_Low_Range', 'EMA_20d', 'MA_Crossover_10_50', 'Mean_Reversion_20d', 'Moving_Average_Reversion', 'Normalized_BBW_20d_2std', 'Price_Momentum_10d', 'ROC_50d', 'RSI_14d', 'SMA_20d', 'Stochastic_Oscillator_14d', 'Trading_Volume', 'VROC_10d', 'Volume_Momentum_50d']\n",
            "\n",
            "--- Calculating Style Factors (Beta, Size Proxy, Liquidity Proxy, Residual Volatility) ---\n",
            "Calculating style factors using 60-day lookback...\n",
            " - Calculated Size/Liquidity Proxy.\n",
            " - Calculating Beta (this may take a while)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - Calculated Beta.\n",
            " - Calculated Residual Volatility.\n",
            "Style factors calculation finished. Final Shape: (102173, 3)\n",
            "\n",
            "--- Defining/Importing Analysis Functions ---\n",
            "\n",
            "Calculating forward returns for analysis periods: (1, 3, 5) days...\n",
            "\n",
            "Calculating forward returns for IC decay (up to 20 days)...\n",
            "Calculating fwd returns for decay (1 to 20 days)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished calculating 20 forward returns for decay.\n",
            "\n",
            "Preparing single Excel output file: factor_analysis_output_combined/combined_factor_analysis_results_1.xlsx\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing 15 factors found in the input DataFrame...\n",
            "\n",
            "\n",
            "==================== Processing Factor: ATR_14d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for ATR_14d. (14.39s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'ATR_14d' for analysis.\n",
            "Clean aligned data ready for ATR_14d_Raw. Shape: (99610, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing ATR_14d_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing ATR_14d_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing ATR_14d_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.58s)\n",
            "\n",
            "--- Saving results for ATR_14d_Raw to Excel ---\n",
            "--- Results for ATR_14d_Raw saved (9 sheets). (0.27s)---\n",
            "--- Factor ATR_14d processing time: 30.25s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Daily_High_Low_Range ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Daily_High_Low_Range. (13.03s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Daily_High_Low_Range' for analysis.\n",
            "Clean aligned data ready for Daily_High_Low__Raw. Shape: (99610, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Daily_High_Low__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Daily_High_Low__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Daily_High_Low__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (16.04s)\n",
            "\n",
            "--- Saving results for Daily_High_Low__Raw to Excel ---\n",
            "--- Results for Daily_High_Low__Raw saved (9 sheets). (0.23s)---\n",
            "--- Factor Daily_High_Low_Range processing time: 29.31s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: EMA_20d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for EMA_20d. (17.71s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'EMA_20d' for analysis.\n",
            "Clean aligned data ready for EMA_20d_Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing EMA_20d_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing EMA_20d_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing EMA_20d_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.98s)\n",
            "\n",
            "--- Saving results for EMA_20d_Raw to Excel ---\n",
            "--- Results for EMA_20d_Raw saved (9 sheets). (0.28s)---\n",
            "--- Factor EMA_20d processing time: 33.99s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: MA_Crossover_10_50 ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for MA_Crossover_10_50. (14.22s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'MA_Crossover_10_50' for analysis.\n",
            "Clean aligned data ready for MA_Crossover_10_Raw. Shape: (95543, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing MA_Crossover_10_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing MA_Crossover_10_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing MA_Crossover_10_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.63s)\n",
            "\n",
            "--- Saving results for MA_Crossover_10_Raw to Excel ---\n",
            "--- Results for MA_Crossover_10_Raw saved (9 sheets). (0.23s)---\n",
            "--- Factor MA_Crossover_10_50 processing time: 30.09s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Mean_Reversion_20d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Mean_Reversion_20d. (14.47s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Mean_Reversion_20d' for analysis.\n",
            "Clean aligned data ready for Mean_Reversion__Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Mean_Reversion__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Mean_Reversion__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Mean_Reversion__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.64s)\n",
            "\n",
            "--- Saving results for Mean_Reversion__Raw to Excel ---\n",
            "--- Results for Mean_Reversion__Raw saved (9 sheets). (0.23s)---\n",
            "--- Factor Mean_Reversion_20d processing time: 30.34s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Moving_Average_Reversion ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Moving_Average_Reversion. (12.93s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Moving_Average_Reversion' for analysis.\n",
            "Clean aligned data ready for Moving_Average__Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Moving_Average__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Moving_Average__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Moving_Average__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.76s)\n",
            "\n",
            "--- Saving results for Moving_Average__Raw to Excel ---\n",
            "--- Results for Moving_Average__Raw saved (9 sheets). (0.23s)---\n",
            "--- Factor Moving_Average_Reversion processing time: 28.93s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Normalized_BBW_20d_2std ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Normalized_BBW_20d_2std. (11.86s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Normalized_BBW_20d_2std' for analysis.\n",
            "Clean aligned data ready for Normalized_BBW__Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Normalized_BBW__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Normalized_BBW__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Normalized_BBW__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.48s)\n",
            "\n",
            "--- Saving results for Normalized_BBW__Raw to Excel ---\n",
            "--- Results for Normalized_BBW__Raw saved (9 sheets). (0.23s)---\n",
            "--- Factor Normalized_BBW_20d_2std processing time: 27.57s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Price_Momentum_10d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Price_Momentum_10d. (12.24s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Price_Momentum_10d' for analysis.\n",
            "Clean aligned data ready for Price_Momentum__Raw. Shape: (98780, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Price_Momentum__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Price_Momentum__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Price_Momentum__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.82s)\n",
            "\n",
            "--- Saving results for Price_Momentum__Raw to Excel ---\n",
            "--- Results for Price_Momentum__Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor Price_Momentum_10d processing time: 28.30s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: ROC_50d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for ROC_50d. (11.87s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'ROC_50d' for analysis.\n",
            "Clean aligned data ready for ROC_50d_Raw. Shape: (95460, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing ROC_50d_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing ROC_50d_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing ROC_50d_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.51s)\n",
            "\n",
            "--- Saving results for ROC_50d_Raw to Excel ---\n",
            "--- Results for ROC_50d_Raw saved (9 sheets). (0.32s)---\n",
            "--- Factor ROC_50d processing time: 27.70s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: RSI_14d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for RSI_14d. (12.09s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'RSI_14d' for analysis.\n",
            "Clean aligned data ready for RSI_14d_Raw. Shape: (98531, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing RSI_14d_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing RSI_14d_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing RSI_14d_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.73s)\n",
            "\n",
            "--- Saving results for RSI_14d_Raw to Excel ---\n",
            "--- Results for RSI_14d_Raw saved (9 sheets). (0.32s)---\n",
            "--- Factor RSI_14d processing time: 28.15s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: SMA_20d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for SMA_20d. (11.63s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'SMA_20d' for analysis.\n",
            "Clean aligned data ready for SMA_20d_Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing SMA_20d_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing SMA_20d_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing SMA_20d_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.72s)\n",
            "\n",
            "--- Saving results for SMA_20d_Raw to Excel ---\n",
            "--- Results for SMA_20d_Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor SMA_20d processing time: 27.60s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Stochastic_Oscillator_14d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Stochastic_Oscillator_14d. (12.21s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Stochastic_Oscillator_14d' for analysis.\n",
            "Clean aligned data ready for Stochastic_Osci_Raw. Shape: (98531, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.53s)\n",
            "\n",
            "--- Saving results for Stochastic_Osci_Raw to Excel ---\n",
            "--- Results for Stochastic_Osci_Raw saved (9 sheets). (0.33s)---\n",
            "--- Factor Stochastic_Oscillator_14d processing time: 28.08s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Trading_Volume ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Trading_Volume. (11.73s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Trading_Volume' for analysis.\n",
            "Clean aligned data ready for Trading_Volume_Raw. Shape: (99610, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Trading_Volume_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Trading_Volume_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Trading_Volume_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.70s)\n",
            "\n",
            "--- Saving results for Trading_Volume_Raw to Excel ---\n",
            "--- Results for Trading_Volume_Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor Trading_Volume processing time: 27.68s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: VROC_10d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for VROC_10d. (12.23s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'VROC_10d' for analysis.\n",
            "Clean aligned data ready for VROC_10d_Raw. Shape: (98058, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing VROC_10d_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing VROC_10d_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing VROC_10d_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.48s)\n",
            "\n",
            "--- Saving results for VROC_10d_Raw to Excel ---\n",
            "--- Results for VROC_10d_Raw saved (9 sheets). (0.27s)---\n",
            "--- Factor VROC_10d processing time: 27.98s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Volume_Momentum_50d ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Volume_Momentum_50d. (12.04s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Volume_Momentum_50d' for analysis.\n",
            "Clean aligned data ready for Volume_Momentum_Raw. Shape: (95460, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Volume_Momentum_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Volume_Momentum_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Volume_Momentum_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.14s)\n",
            "\n",
            "--- Saving results for Volume_Momentum_Raw to Excel ---\n",
            "--- Results for Volume_Momentum_Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor Volume_Momentum_50d processing time: 27.43s ---\n",
            "\n",
            "All factors processed. Finalizing Excel file: factor_analysis_output_combined/combined_factor_analysis_results_1.xlsx\n",
            "\n",
            "=============================================\n",
            "=== Combined Factor Analysis Script Finished ===\n",
            "=============================================\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "# import matplotlib.pyplot as plt # Keep commented unless plotting is explicitly re-enabled\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.rolling import RollingOLS # Correct import for RollingOLS\n",
        "from statsmodels.tools.sm_exceptions import MissingDataError # Import specific error\n",
        "from numpy.linalg import LinAlgError # Import specific error\n",
        "import yfinance as yf\n",
        "from datetime import timedelta\n",
        "import traceback # For detailed error reporting\n",
        "from tqdm import tqdm # Use standard tqdm\n",
        "import warnings # To suppress specific warnings if needed\n",
        "import os # For path handling\n",
        "import math # For sqrt\n",
        "import openpyxl # Explicitly import for ExcelWriter engine check\n",
        "import time # Can be useful for adding delays\n",
        "\n",
        "# --- Suppress Warnings ---\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "warnings.simplefilter(\"ignore\", category=pd.errors.PerformanceWarning) # Suppress PerformanceWarning if desired\n",
        "pd.options.mode.chained_assignment = None # Suppress SettingWithCopyWarning ('warn' or None)\n",
        "\n",
        "# --- Configuration ---\n",
        "_start_date_str = '2020-03-24'\n",
        "_end_date_str = '2025-03-24'\n",
        "target_timezone = 'UTC'\n",
        "\n",
        "# Analysis Parameters\n",
        "benchmark_ticker = \"^HSI\"\n",
        "analysis_periods_str = ['1D_fwd_ret', '3D_fwd_ret', '5D_fwd_ret']\n",
        "fwd_ret_periods_int = tuple(int(p.split('D')[0]) for p in analysis_periods_str)\n",
        "num_quantiles = 5\n",
        "ic_method = 'spearman' # 'spearman' or 'pearson'\n",
        "neutralization_lookback = 60 # For style factors & neutralization lookback\n",
        "MAX_DECAY_LAG = 20\n",
        "# Lookbacks for style factors (keep generic longest lookback calculation)\n",
        "longest_lookback_generic = neutralization_lookback # Adjust if other lookbacks needed for style/data fetch\n",
        "\n",
        "# Output Configuration\n",
        "output_dir = \"factor_analysis_output_combined\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Define the SINGLE output Excel file name\n",
        "combined_output_filename = os.path.join(output_dir, \"combined_factor_analysis_results_1.xlsx\")\n",
        "\n",
        "# --- Date Handling ---\n",
        "try:\n",
        "    start_date_naive = pd.to_datetime(_start_date_str)\n",
        "    end_date_naive = pd.to_datetime(_end_date_str)\n",
        "    start_date = start_date_naive.tz_localize(target_timezone)\n",
        "    end_date = end_date_naive.tz_localize(target_timezone)\n",
        "    print(f\"INFO: Start date localized to {target_timezone}: {start_date}\")\n",
        "    print(f\"INFO: End date localized to {target_timezone}: {end_date}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not localize start/end dates to timezone '{target_timezone}'. Error: {e}\")\n",
        "    raise ValueError(\"Failed to set timezone for start/end dates\") from e\n",
        "\n",
        "# --- Placeholder for Dynamic Universe ---\n",
        "def get_index_constituents_historical(index_ticker, date_str):\n",
        "    print(f\"Attempting to get constituents for {index_ticker} on {date_str}.\")\n",
        "    #print(\"         This is a placeholder - yfinance lacks this feature.\")\n",
        "    #print(\"         Returning a default hardcoded list for now.\")\n",
        "    default_assets = [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', \n",
        "                  \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', \n",
        "                  \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                  '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', \n",
        "                  '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK']\n",
        "    # Add more realistic tickers if possible for better testing\n",
        "    # default_assets.extend([\"1299.HK\", \"2318.HK\", \"0011.HK\", \"0003.HK\", \"0016.HK\", \"0012.HK\"])\n",
        "    if index_ticker == \"^HSI\": return default_assets\n",
        "    else: return []\n",
        "\n",
        "target_index = \"^HSI\"\n",
        "print(f\"\\n--- Attempting to define universe based on index: {target_index} ---\")\n",
        "assets = get_index_constituents_historical(target_index, _start_date_str)\n",
        "if not assets:\n",
        "    print(f\"ERROR: Could not determine assets for index {target_index}. Falling back to hardcoded list.\")\n",
        "    assets =  [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', \n",
        "                \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', \n",
        "                \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', \n",
        "                '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK']\n",
        "\n",
        "assets = sorted(list(set(assets))) # Ensure unique and sorted\n",
        "print(f\"INFO: Using asset universe (Count: {len(assets)}): {assets[:10]}...\")\n",
        "\n",
        "# --- Create Target Business Day Index ---\n",
        "try:\n",
        "    import pandas_market_calendars as mcal\n",
        "    hk_calendar = mcal.get_calendar('XHKG')\n",
        "    max_fwd_buffer_days = max(fwd_ret_periods_int) + 10 if fwd_ret_periods_int else 10\n",
        "    # Extend fetch range slightly more for lookbacks and forward returns\n",
        "    calendar_start_naive = start_date_naive - timedelta(days=longest_lookback_generic + 40) # Slightly longer buffer\n",
        "    calendar_end_naive = end_date_naive + timedelta(days=max(max_fwd_buffer_days, MAX_DECAY_LAG + 15)) # Slightly longer buffer\n",
        "    schedule = hk_calendar.schedule(start_date=calendar_start_naive, end_date=calendar_end_naive)\n",
        "    fetch_dates_index_raw = pd.to_datetime(schedule.index).tz_localize(schedule.index.tz)\n",
        "    if fetch_dates_index_raw.tz is None: fetch_dates_index_raw = pd.to_datetime(schedule.index).tz_localize('UTC', ambiguous='infer', nonexistent='shift_forward')\n",
        "    fetch_dates_index = fetch_dates_index_raw.tz_convert(target_timezone).drop_duplicates().sort_values() # Ensure unique & sorted early\n",
        "    dates_index = fetch_dates_index[(fetch_dates_index >= start_date) & (fetch_dates_index <= end_date)].drop_duplicates().sort_values()\n",
        "    print(f\"Using pandas_market_calendars for HK business days. Full fetch range index length: {len(fetch_dates_index)}\")\n",
        "except ImportError:\n",
        "    print(\"WARNING: pandas_market_calendars not found. Using pd.date_range(freq='B'). This might include holidays.\")\n",
        "    max_fwd_period = max(fwd_ret_periods_int) if fwd_ret_periods_int else 0\n",
        "    fetch_start_dt_b = start_date - pd.Timedelta(days=longest_lookback_generic + 40) # Adjusted buffer\n",
        "    fetch_end_dt_b = end_date + pd.Timedelta(days=max(max_fwd_period, MAX_DECAY_LAG) + 15) # Adjusted buffer\n",
        "    fetch_dates_index = pd.date_range(start=fetch_start_dt_b, end=fetch_end_dt_b, freq='B', tz=target_timezone).drop_duplicates().sort_values()\n",
        "    dates_index = fetch_dates_index[(fetch_dates_index >= start_date) & (fetch_dates_index <= end_date)].drop_duplicates().sort_values()\n",
        "\n",
        "if dates_index.empty:\n",
        "    raise ValueError(f\"ERROR: Target dates_index is empty after filtering between {start_date} and {end_date}.\")\n",
        "print(f\"Target Analysis Date Range: {dates_index.min()} to {dates_index.max()} ({len(dates_index)} analysis days)\")\n",
        "\n",
        "# --- Data Fetching (Prices and Total Volume) ---\n",
        "print(\"\\n--- Downloading Price and Total Volume Data ---\")\n",
        "prices_lookback = pd.DataFrame()\n",
        "volumes_lookback = pd.DataFrame()\n",
        "benchmark_prices_lookback = pd.Series(dtype=float)\n",
        "try:\n",
        "    fetch_start_str = fetch_dates_index.min().strftime('%Y-%m-%d')\n",
        "    fetch_end_str = (fetch_dates_index.max() + pd.Timedelta(days=1)).strftime('%Y-%m-%d') # Add 1 day for yf end date convention\n",
        "    print(f\"Fetching data from {fetch_start_str} to {fetch_end_str} for {len(assets)} assets + benchmark {benchmark_ticker}...\")\n",
        "\n",
        "    # Fetch asset data\n",
        "    data_assets = yf.download(assets, start=fetch_start_str, end=fetch_end_str, progress=True, timeout=180, group_by='ticker')\n",
        "\n",
        "    # Fetch benchmark data\n",
        "    data_benchmark = yf.download(benchmark_ticker, start=fetch_start_str, end=fetch_end_str, progress=False)\n",
        "\n",
        "    # Process Asset Data\n",
        "    prices_list = []\n",
        "    volumes_list = []\n",
        "    valid_assets = [] # Keep track of assets with successfully downloaded data\n",
        "    if not data_assets.empty:\n",
        "        # Check if data_assets index needs converting (can happen with yfinance sometimes)\n",
        "        if not isinstance(data_assets.index, pd.DatetimeIndex):\n",
        "             try: data_assets.index = pd.to_datetime(data_assets.index)\n",
        "             except: print(\"WARN: Could not convert downloaded asset data index to DatetimeIndex.\")\n",
        "\n",
        "        for asset in assets:\n",
        "            try:\n",
        "                # Access asset data robustly\n",
        "                if isinstance(data_assets.columns, pd.MultiIndex):\n",
        "                    if asset in data_assets.columns.get_level_values(0):\n",
        "                       asset_data = data_assets[asset]\n",
        "                    else:\n",
        "                       print(f\"WARN: No data returned for {asset} in multi-index result.\")\n",
        "                       continue\n",
        "                elif len(assets) == 1 and asset == assets[0]: # Handle case where only one asset was requested (no multi-index)\n",
        "                     asset_data = data_assets\n",
        "                else: # Should not happen if group_by='ticker' worked for multiple assets\n",
        "                     print(f\"WARN: Unexpected data structure for {asset}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                # Select price and volume\n",
        "                adj_close_key = 'Adj Close' if 'Adj Close' in asset_data.columns else 'Close'\n",
        "                if adj_close_key not in asset_data.columns or 'Volume' not in asset_data.columns:\n",
        "                     print(f\"WARN: Missing '{adj_close_key}' or 'Volume' for {asset}. Skipping.\")\n",
        "                     continue\n",
        "                price_col = asset_data[adj_close_key]\n",
        "                volume_col = asset_data['Volume']\n",
        "\n",
        "                # Check for sufficient non-NaN data\n",
        "                if not price_col.dropna().empty: # Check if not ALL NaN\n",
        "                    prices_list.append(price_col.rename(asset))\n",
        "                    volumes_list.append(volume_col.rename(asset))\n",
        "                    valid_assets.append(asset)\n",
        "                else:\n",
        "                    print(f\"WARN: Price data for {asset} is all NaN.\")\n",
        "\n",
        "            except KeyError:\n",
        "                print(f\"WARN: KeyError accessing data for {asset}. Ticker might be invalid or delisted for the period.\")\n",
        "            except Exception as e_asset:\n",
        "                print(f\"WARN: Could not process data for {asset}. Error: {e_asset}\")\n",
        "\n",
        "    if prices_list:\n",
        "        prices_raw = pd.concat(prices_list, axis=1)\n",
        "        volumes_raw = pd.concat(volumes_list, axis=1)\n",
        "\n",
        "        # Convert index to datetime and localize if needed\n",
        "        if not isinstance(prices_raw.index, pd.DatetimeIndex): prices_raw.index = pd.to_datetime(prices_raw.index)\n",
        "        if prices_raw.index.tz is None: prices_raw.index = prices_raw.index.tz_localize('UTC', ambiguous='infer', nonexistent='shift_forward')\n",
        "        prices_raw = prices_raw.tz_convert(target_timezone)\n",
        "\n",
        "        if not isinstance(volumes_raw.index, pd.DatetimeIndex): volumes_raw.index = pd.to_datetime(volumes_raw.index)\n",
        "        if volumes_raw.index.tz is None: volumes_raw.index = volumes_raw.index.tz_localize('UTC', ambiguous='infer', nonexistent='shift_forward')\n",
        "        volumes_raw = volumes_raw.tz_convert(target_timezone)\n",
        "\n",
        "        # Reindex to our full business day index and forward fill prices, fillna(0) volumes\n",
        "        prices_lookback = prices_raw.reindex(fetch_dates_index).ffill()\n",
        "        volumes_lookback = volumes_raw.reindex(fetch_dates_index).fillna(0)\n",
        "        print(f\"Asset price/volume data processed. Shape: {prices_lookback.shape}\")\n",
        "\n",
        "        # --- Crucial: Update asset list to only include those successfully downloaded ---\n",
        "        original_asset_count = len(assets)\n",
        "        assets = sorted(valid_assets) # Update the global assets list\n",
        "        if len(assets) < original_asset_count:\n",
        "            print(f\"INFO: Asset list updated to {len(assets)} tickers with valid data.\")\n",
        "        if not assets: # Check if asset list became empty\n",
        "             print(\"CRITICAL ERROR: No assets remaining after data download/validation. Exiting.\")\n",
        "             exit()\n",
        "        # -------------------------------------------------------------------------------\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR: No valid asset price data could be fetched or processed. Exiting.\")\n",
        "        exit() # Exit if no asset data\n",
        "\n",
        "    # Process Benchmark Data\n",
        "    if not data_benchmark.empty:\n",
        "        if not isinstance(data_benchmark.index, pd.DatetimeIndex): data_benchmark.index = pd.to_datetime(data_benchmark.index)\n",
        "        adj_close_key_bm = 'Adj Close' if 'Adj Close' in data_benchmark.columns else 'Close'\n",
        "        if adj_close_key_bm not in data_benchmark.columns:\n",
        "             print(f\"ERROR: Benchmark price column ('{adj_close_key_bm}') not found.\")\n",
        "             # Create empty series as fallback\n",
        "             benchmark_prices_lookback = pd.Series(dtype=float, index=fetch_dates_index, name=benchmark_ticker)\n",
        "        else:\n",
        "            benchmark_prices_raw = data_benchmark[adj_close_key_bm]\n",
        "            if benchmark_prices_raw.index.tz is None: benchmark_prices_raw.index = benchmark_prices_raw.index.tz_localize('UTC', ambiguous='infer', nonexistent='shift_forward')\n",
        "            benchmark_prices_raw = benchmark_prices_raw.tz_convert(target_timezone)\n",
        "            benchmark_prices_lookback = benchmark_prices_raw.reindex(fetch_dates_index).ffill()\n",
        "            print(f\"Benchmark data processed. Length: {len(benchmark_prices_lookback)}\")\n",
        "    else:\n",
        "        print(\"ERROR: Benchmark data could not be fetched.\")\n",
        "        # Create empty series if benchmark fetch failed\n",
        "        benchmark_prices_lookback = pd.Series(dtype=float, index=fetch_dates_index, name=benchmark_ticker)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR during data download: {e}\"); traceback.print_exc()\n",
        "    print(\"CRITICAL ERROR: Data download failed. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Create dataframes for the analysis period by slicing lookback data\n",
        "# Ensure slicing uses the potentially updated 'assets' list\n",
        "prices = prices_lookback.loc[dates_index, assets].copy()\n",
        "volumes = volumes_lookback.loc[dates_index, assets].copy()\n",
        "benchmark_prices = benchmark_prices_lookback.loc[dates_index].copy()\n",
        "\n",
        "# --- Robust check for empty or all-NaN core data ---\n",
        "prices_all_nan = False\n",
        "if not prices.empty:\n",
        "    prices_all_nan = prices.isna().all().all() # Check if ALL values are NaN\n",
        "\n",
        "benchmark_all_nan = False\n",
        "if not benchmark_prices.empty and isinstance(benchmark_prices, pd.Series): # Ensure it's a Series\n",
        "    benchmark_all_nan = benchmark_prices.isna().all() # Check if ALL values are NaN\n",
        "\n",
        "if prices.empty or benchmark_prices.empty or prices_all_nan or benchmark_all_nan:\n",
        "     print(\"CRITICAL ERROR: Prices or Benchmark data is invalid (empty or all NaN) for the analysis period. Exiting.\")\n",
        "     exit()\n",
        "# --- End of robust check ---\n",
        "\n",
        "\n",
        "# --- Fetch Industry Data ---\n",
        "print(\"\\n--- Fetching Industry Classification Data ---\")\n",
        "def fetch_industry_data(tickers):\n",
        "    industry_dict = {}\n",
        "    missing_industries = []\n",
        "    for ticker_str in tqdm(tickers, desc=\"Fetching Industries\"):\n",
        "        try:\n",
        "            ticker_obj = yf.Ticker(ticker_str)\n",
        "            # info_data = ticker_obj.fast_info # Potentially faster, fewer fields\n",
        "            info_data = ticker_obj.info # Slower but more comprehensive\n",
        "            industry = info_data.get('industry', 'Unknown')\n",
        "            sector = info_data.get('sector', 'Unknown') # Get sector too\n",
        "            # Prefer industry, fall back to sector, then Unknown\n",
        "            if industry in [None, '', 'N/A', 'Unknown']:\n",
        "                 industry = sector if sector not in [None, '', 'N/A', 'Unknown'] else 'Unknown'\n",
        "            final_industry = industry if industry is not None else 'Unknown' # Ensure value is not None\n",
        "\n",
        "            if final_industry == 'Unknown': missing_industries.append(ticker_str)\n",
        "            industry_dict[ticker_str] = final_industry\n",
        "            time.sleep(0.05) # Small delay to avoid potential rate limiting\n",
        "        except Exception as e_ind:\n",
        "            print(f\"WARN: Error fetching industry for {ticker_str}: {e_ind}\") # Show specific error\n",
        "            industry_dict[ticker_str] = 'Unknown'\n",
        "            missing_industries.append(ticker_str)\n",
        "\n",
        "    if missing_industries: print(f\"WARNING: Could not reliably fetch industry/sector for: {list(set(missing_industries))}\")\n",
        "    return pd.Series(industry_dict, name='industry')\n",
        "\n",
        "if assets: # Only fetch if we have assets\n",
        "    asset_industries = fetch_industry_data(assets)\n",
        "    industry_dummies_static = pd.DataFrame()\n",
        "    if not asset_industries.empty:\n",
        "        # Create dummies, ensuring they align with the final 'assets' list\n",
        "        industry_dummies_static = pd.get_dummies(asset_industries.reindex(assets).fillna('Unknown'), dummy_na=False, prefix='Ind').astype(int)\n",
        "        # Drop 'Ind_Unknown' if other industries exist and it's all zero, or if only Unknown exists keep it.\n",
        "        if 'Ind_Unknown' in industry_dummies_static.columns and len(industry_dummies_static.columns) > 1:\n",
        "             if not industry_dummies_static['Ind_Unknown'].any():\n",
        "                  industry_dummies_static = industry_dummies_static.drop('Ind_Unknown', axis=1)\n",
        "        industry_dummies_static.index.name = 'asset'\n",
        "        print(f\"Created Static Industry Dummies shape: {industry_dummies_static.shape}\")\n",
        "        if industry_dummies_static.empty:\n",
        "             print(\"WARN: Industry dummies became empty after processing (e.g., only 'Unknown' dropped). Creating default.\")\n",
        "             industry_dummies_static = pd.DataFrame({'Ind_NoIndustry': 1}, index=assets).astype(int)\n",
        "    else:\n",
        "        print(\"WARNING: Could not create industry dummies (fetch returned empty). Creating default.\")\n",
        "        industry_dummies_static = pd.DataFrame({'Ind_NoIndustry': 1}, index=assets).astype(int)\n",
        "        industry_dummies_static.index.name = 'asset'\n",
        "else:\n",
        "    print(\"WARNING: No assets defined, skipping industry fetch.\")\n",
        "    asset_industries = pd.Series(dtype=str, name='industry')\n",
        "    industry_dummies_static = pd.DataFrame(index=pd.Index([], name='asset')) # Ensure empty df has index\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# === LOAD OR DEFINE YOUR PRE-CALCULATED FACTORS HERE ===\n",
        "# ================================================================\n",
        "print(\"\\n--- Loading/Defining Pre-calculated Factors ---\")\n",
        "\n",
        "# --- INPUT REQUIRED ---\n",
        "# Option 1: Load from file (RECOMMENDED)\n",
        "LOAD_FROM_FILE = True # SET TO TRUE TO LOAD FROM FILE\n",
        "factors_file_path = \"processed_alpha_data_values_1.csv\" # OR .csv, .pkl etc.\n",
        "# Expected format: See FORMAT 1 or FORMAT 2 descriptions below.\n",
        "\n",
        "# Option 2: Define programmatically (like the dummy example)\n",
        "CREATE_DUMMY_FACTORS = True # Set to False if loading from file\n",
        "\n",
        "factors_input_df = pd.DataFrame() # Initialize\n",
        "\n",
        "if LOAD_FROM_FILE:\n",
        "    print(f\"Attempting to load factors from: {factors_file_path}\")\n",
        "    if not os.path.exists(factors_file_path):\n",
        "         print(f\"ERROR: Factors file not found at {factors_file_path}\")\n",
        "    else:\n",
        "        try:\n",
        "            # Example loading parquet (adjust based on your file type)\n",
        "            if factors_file_path.endswith(\".parquet\"):\n",
        "                factors_input_df = pd.read_parquet(factors_file_path)\n",
        "            elif factors_file_path.endswith(\".csv\"):\n",
        "                # Adjust read_csv parameters as needed (e.g., index_col, parse_dates)\n",
        "                # Assuming format 2 (stacked) for CSV example:\n",
        "                factors_input_df = pd.read_csv(factors_file_path, index_col=[0, 1], parse_dates=[0])\n",
        "                # Set index names if not read automatically\n",
        "                if factors_input_df.index.names != ['date', 'asset']:\n",
        "                     print(\"WARN: Setting loaded CSV index names to ['date', 'asset'].\")\n",
        "                     factors_input_df.index.names = ['date', 'asset']\n",
        "            elif factors_file_path.endswith(\".pkl\"):\n",
        "                factors_input_df = pd.read_pickle(factors_file_path)\n",
        "            else:\n",
        "                print(f\"ERROR: Unsupported file format: {factors_file_path}\")\n",
        "\n",
        "            if not factors_input_df.empty:\n",
        "                print(f\"Successfully loaded factors from file. Initial shape: {factors_input_df.shape}\")\n",
        "\n",
        "                # --- Post-load processing based on format ---\n",
        "                # Check if loaded data is Format 2 (stacked: Index=(date, asset), Columns=FactorNames)\n",
        "                if isinstance(factors_input_df.index, pd.MultiIndex) and list(factors_input_df.index.names) == ['date', 'asset']:\n",
        "                    print(\"INFO: Loaded data appears to be in Format 2 (Stacked). Unstacking...\")\n",
        "                    try:\n",
        "                        factors_input_df_wide = factors_input_df.unstack(level='asset')\n",
        "                        factors_input_df_wide.columns = pd.MultiIndex.from_tuples(\n",
        "                            [(col_name, asset_name) for col_name, asset_name in factors_input_df_wide.columns],\n",
        "                            names=['factor_name', 'asset']\n",
        "                        )\n",
        "                        factors_input_df = factors_input_df_wide # Overwrite with Format 1\n",
        "                        print(\"Successfully unstacked factors to Format 1 (Wide).\")\n",
        "                    except Exception as e_unstack_load:\n",
        "                        print(f\"ERROR: Could not unstack the loaded factor DataFrame: {e_unstack_load}\")\n",
        "                        factors_input_df = pd.DataFrame() # Invalidate on error\n",
        "                # Assume loaded data is already Format 1 (wide: Index=date, Columns=(factor_name, asset))\n",
        "                elif isinstance(factors_input_df.index, pd.DatetimeIndex) and isinstance(factors_input_df.columns, pd.MultiIndex):\n",
        "                    print(\"INFO: Loaded data appears to be in Format 1 (Wide).\")\n",
        "                    # Ensure column level names are correct\n",
        "                    if list(factors_input_df.columns.names) != ['factor_name', 'asset']:\n",
        "                        print(\"WARN: Renaming columns to ['factor_name', 'asset']. Please verify.\")\n",
        "                        factors_input_df.columns.names = ['factor_name', 'asset']\n",
        "                else:\n",
        "                    print(\"ERROR: Loaded DataFrame format is not recognized as Format 1 or Format 2.\")\n",
        "                    factors_input_df = pd.DataFrame() # Invalidate\n",
        "\n",
        "        except Exception as e_load:\n",
        "            print(f\"ERROR: Failed to load or process factors file: {e_load}\")\n",
        "            traceback.print_exc()\n",
        "            factors_input_df = pd.DataFrame()\n",
        "\n",
        "elif CREATE_DUMMY_FACTORS:\n",
        "    # Create dummy stacked data for demonstration:\n",
        "    print(\"INFO: Creating dummy factor data for demonstration...\")\n",
        "    if not dates_index.empty and assets:\n",
        "        multi_idx = pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset'])\n",
        "        dummy_data = {\n",
        "            'Factor_Dummy_1': np.random.randn(len(multi_idx)),\n",
        "            'Factor_Dummy_2': np.random.rand(len(multi_idx)) - 0.5\n",
        "        }\n",
        "        factors_input_df_stacked = pd.DataFrame(dummy_data, index=multi_idx)\n",
        "        print(f\"Dummy stacked factors created. Shape: {factors_input_df_stacked.shape}\")\n",
        "\n",
        "        # --- Convert FORMAT 2 (Stacked) to FORMAT 1 (Wide - Preferred by the script) ---\n",
        "        try:\n",
        "            factors_input_df = factors_input_df_stacked.unstack(level='asset')\n",
        "            factors_input_df.columns = pd.MultiIndex.from_tuples(\n",
        "                [(col_name, asset_name) for col_name, asset_name in factors_input_df.columns],\n",
        "                names=['factor_name', 'asset']\n",
        "            )\n",
        "            print(\"Successfully unstacked dummy factors to Format 1 (Wide).\")\n",
        "        except Exception as e_unstack_dummy:\n",
        "            print(f\"ERROR: Could not unstack the dummy factor DataFrame: {e_unstack_dummy}\")\n",
        "            factors_input_df = pd.DataFrame() # Assign empty df on error\n",
        "    else:\n",
        "        print(\"ERROR: Cannot create dummy factors - dates_index or assets are empty.\")\n",
        "        factors_input_df = pd.DataFrame()\n",
        "else:\n",
        "    print(\"INFO: No factor loading or creation specified.\")\n",
        "\n",
        "\n",
        "# --- Validation and Final Preparation ---\n",
        "precalculated_factors_df = pd.DataFrame() # Initialize final df\n",
        "\n",
        "if not factors_input_df.empty:\n",
        "    # Ensure index is DatetimeIndex and has correct timezone\n",
        "    if not isinstance(factors_input_df.index, pd.DatetimeIndex):\n",
        "        try:\n",
        "            factors_input_df.index = pd.to_datetime(factors_input_df.index)\n",
        "        except Exception as e_conv:\n",
        "            print(f\"ERROR: Could not convert factor index to DatetimeIndex: {e_conv}. Invalidating factors.\")\n",
        "            factors_input_df = pd.DataFrame()\n",
        "\n",
        "    if not factors_input_df.empty: # Check again after potential invalidation\n",
        "        if factors_input_df.index.tz is None:\n",
        "            try:\n",
        "                print(\"INFO: Localizing factor index timezone...\")\n",
        "                factors_input_df.index = factors_input_df.index.tz_localize(target_timezone, ambiguous='infer', nonexistent='shift_forward')\n",
        "            except TypeError: # Already localized\n",
        "                 pass\n",
        "            except Exception as e_tz:\n",
        "                print(f\"ERROR: Could not localize factor index timezone: {e_tz}. Invalidating factors.\")\n",
        "                factors_input_df = pd.DataFrame()\n",
        "\n",
        "        if not factors_input_df.empty and factors_input_df.index.tz != target_timezone:\n",
        "            try:\n",
        "                print(\"INFO: Converting factor index timezone...\")\n",
        "                factors_input_df.index = factors_input_df.index.tz_convert(target_timezone)\n",
        "            except Exception as e_tz_conv:\n",
        "                print(f\"ERROR: Could not convert factor index timezone: {e_tz_conv}. Invalidating factors.\")\n",
        "                factors_input_df = pd.DataFrame()\n",
        "\n",
        "    # Ensure columns are MultiIndex ['factor_name', 'asset']\n",
        "    if not factors_input_df.empty:\n",
        "        if isinstance(factors_input_df.columns, pd.MultiIndex) and list(factors_input_df.columns.names) == ['factor_name', 'asset']:\n",
        "             # Reindex to ensure all factors/assets/dates are present\n",
        "             factor_names_present = factors_input_df.columns.get_level_values('factor_name').unique()\n",
        "             # Ensure assets used for reindexing are the ones we have price data for\n",
        "             target_multi_columns = pd.MultiIndex.from_product([factor_names_present, assets], names=['factor_name', 'asset'])\n",
        "\n",
        "             print(f\"Reindexing loaded factors to match analysis dates ({len(dates_index)}) and assets ({len(assets)})...\")\n",
        "             # Reindex BOTH index and columns to match the analysis scope\n",
        "             precalculated_factors_df = factors_input_df.reindex(index=dates_index, columns=target_multi_columns)\n",
        "             # Check for excessive NaNs after reindexing\n",
        "             nan_frac = precalculated_factors_df.isna().mean().mean() if not precalculated_factors_df.empty else 1.0\n",
        "             if precalculated_factors_df.isna().all().all():\n",
        "                  print(\"CRITICAL WARN: Factor DataFrame is ALL NaNs after reindexing. Check date/asset alignment. Analysis will likely fail.\")\n",
        "             elif nan_frac > 0.9: # Example threshold\n",
        "                  print(f\"WARN: Factor DataFrame has >90% NaNs ({nan_frac:.1%}) after reindexing.\")\n",
        "\n",
        "             print(f\"Final precalculated factors DataFrame ready. Shape: {precalculated_factors_df.shape}\")\n",
        "             available_factors = precalculated_factors_df.columns.get_level_values('factor_name').unique().tolist()\n",
        "             print(f\"Available factors: {available_factors}\")\n",
        "             if not available_factors:\n",
        "                  print(\"ERROR: No factor names found after processing. Invalidating.\")\n",
        "                  precalculated_factors_df = pd.DataFrame()\n",
        "\n",
        "        else:\n",
        "             print(\"ERROR: Processed factor DataFrame columns are not MultiIndex named ['factor_name', 'asset']. Invalidating factors.\")\n",
        "             precalculated_factors_df = pd.DataFrame()\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: No factor input data loaded or created.\")\n",
        "    precalculated_factors_df = pd.DataFrame()\n",
        "\n",
        "# Final check before analysis loop\n",
        "if precalculated_factors_df.empty:\n",
        "     print(\"\\nCRITICAL ERROR: The precalculated_factors_df is empty or invalid after loading/processing. Cannot proceed with analysis.\")\n",
        "     exit()\n",
        "# ================================================================\n",
        "# === END OF FACTOR LOADING SECTION ===\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "# --- Calculate Style Factors (Do ONCE before loop) ---\n",
        "print(\"\\n--- Calculating Style Factors (Beta, Size Proxy, Liquidity Proxy, Residual Volatility) ---\")\n",
        "# Initialize with correct index names BUT NO DATA YET\n",
        "style_factors = pd.DataFrame(index=pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset'])) # Base structure\n",
        "style_factors_calculated = {} # Store components temporarily\n",
        "\n",
        "min_periods_neut = max(10, neutralization_lookback // 2)\n",
        "try:\n",
        "    # Use lookback dataframes here\n",
        "    print(f\"Calculating style factors using {neutralization_lookback}-day lookback...\")\n",
        "\n",
        "    # 1. Size/Liquidity Proxy: Log of rolling average dollar volume\n",
        "    if not prices_lookback.empty and not volumes_lookback.empty:\n",
        "        dollar_volume_lb = prices_lookback.loc[:, assets] * volumes_lookback.loc[:, assets] # Ensure asset alignment\n",
        "        rolling_dollar_vol_lb = dollar_volume_lb.rolling(neutralization_lookback, min_periods=min_periods_neut).mean()\n",
        "        epsilon = 1e-9 # Smaller epsilon\n",
        "        size_liquidity_proxy_df = np.log1p(rolling_dollar_vol_lb + epsilon)\n",
        "        size_liquidity_proxy_df = size_liquidity_proxy_df.replace([np.inf, -np.inf], np.nan)\n",
        "        # Slice to analysis dates *before* stacking\n",
        "        size_liq_proxy_stacked = size_liquidity_proxy_df.loc[dates_index, assets].stack(future_stack=True).rename('size_liquidity_proxy')\n",
        "        size_liq_proxy_stacked.index.names = ['date', 'asset'] # Set Index Names\n",
        "        if not size_liq_proxy_stacked.dropna().empty: # Check if not all NaN\n",
        "            style_factors_calculated['size_liquidity_proxy'] = size_liq_proxy_stacked\n",
        "            print(\" - Calculated Size/Liquidity Proxy.\")\n",
        "        else: print(\"WARN: Size/Liquidity Proxy resulted in empty or all-NaN series.\")\n",
        "    else: print(\"WARN: Skipping Size/Liquidity Proxy calc due to missing price/volume lookback data.\")\n",
        "\n",
        "\n",
        "    # 2. Beta: Rolling regression against benchmark\n",
        "    if not prices_lookback.empty and not benchmark_prices_lookback.dropna().empty: # Check benchmark has data\n",
        "        asset_returns_lb = prices_lookback.loc[:, assets].pct_change() # Ensure asset alignment\n",
        "        benchmark_returns_lb = benchmark_prices_lookback.pct_change()\n",
        "\n",
        "        # Ensure benchmark returns are not all NaN before proceeding\n",
        "        if benchmark_returns_lb.dropna().empty:\n",
        "             print(\"WARN: Benchmark returns are all NaN in lookback period. Skipping Beta calculation.\")\n",
        "        else:\n",
        "            aligned_benchmark_ret_lb = benchmark_returns_lb.reindex(asset_returns_lb.index).ffill()\n",
        "            X_beta_base = sm.add_constant(aligned_benchmark_ret_lb.dropna()) # Prepare RHS once\n",
        "            betas = {} # Re-initialize dict for beta results specifically\n",
        "\n",
        "            print(\" - Calculating Beta (this may take a while)...\")\n",
        "            with tqdm(total=len(assets), desc=\"Calculating Beta\", leave=False) as pbar:\n",
        "                for asset in assets:\n",
        "                    y_beta = asset_returns_lb[asset].dropna()\n",
        "\n",
        "                    # --- Robust Beta Calculation Start ---\n",
        "                    if y_beta.empty or X_beta_base.empty:\n",
        "                        betas[asset] = pd.Series(np.nan, index=dates_index, name=asset) # Assign NaN series aligned with main index\n",
        "                        pbar.update(1)\n",
        "                        continue # Skip to next asset\n",
        "\n",
        "                    common_idx_beta = X_beta_base.index.intersection(y_beta.index)\n",
        "\n",
        "                    if len(common_idx_beta) >= neutralization_lookback: # Use >= lookback for min_nobs logic\n",
        "                        X_beta_aligned = X_beta_base.loc[common_idx_beta]\n",
        "                        y_beta_aligned = y_beta.loc[common_idx_beta]\n",
        "\n",
        "                        if y_beta_aligned.empty or X_beta_aligned.empty:\n",
        "                             betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                             pbar.update(1)\n",
        "                             continue\n",
        "\n",
        "                        try:\n",
        "                            # Use imported RollingOLS directly\n",
        "                            rols = RollingOLS(endog=y_beta_aligned, exog=X_beta_aligned,\n",
        "                                             window=neutralization_lookback, min_nobs=min_periods_neut)\n",
        "                            results = rols.fit()\n",
        "                            # Check if params DataFrame is not empty and has enough columns\n",
        "                            if not results.params.empty and results.params.shape[1] > 1:\n",
        "                                 beta_series = results.params.iloc[:, 1] # Beta coeff index 1\n",
        "                                 # Reindex to target dates_index AFTER calculation for this asset\n",
        "                                 betas[asset] = beta_series.reindex(dates_index).ffill().bfill()\n",
        "                            else:\n",
        "                                 #print(f\"WARN [{asset}]: RollingOLS params empty or misshaped.\")\n",
        "                                 betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "\n",
        "                        except IndexError: # Catch index error if params structure unexpected\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS IndexError (likely bad fit).\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                        except MissingDataError: # Catch if not enough observations for a window\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS MissingDataError.\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                        except LinAlgError: # Catch linear algebra errors (e.g., singular matrix)\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS LinAlgError.\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                        except ValueError as e_ols_val: # Catch potential value errors during fit\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS ValueError: {e_ols_val}\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                        except Exception as e_beta_sm: # Catch other unexpected errors\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS failed unexpectedly: {e_beta_sm}\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                    else:\n",
        "                        # Not enough common data points for reliable rolling beta\n",
        "                        betas[asset] = pd.Series(np.nan, index=dates_index, name=asset) # Assign NaN series\n",
        "                    pbar.update(1)\n",
        "                    # --- Robust Beta Calculation End ---\n",
        "\n",
        "            # --- Concatenate Beta results ---\n",
        "            if betas: # Check if the betas dictionary is not empty\n",
        "                try:\n",
        "                    # Filter out any potential non-Series items just in case\n",
        "                    valid_betas = {k: v for k, v in betas.items() if isinstance(v, pd.Series)}\n",
        "                    if valid_betas:\n",
        "                        beta_df = pd.concat(valid_betas.values(), axis=1, keys=valid_betas.keys()) # Use values and keys\n",
        "                        beta_df.columns.name = 'asset' # Name the column index\n",
        "                        # Stack the dataframe (already indexed by dates_index)\n",
        "                        beta_stacked = beta_df.stack(future_stack=True).rename('beta')\n",
        "                        beta_stacked.index.names = ['date', 'asset'] # Set Index Names\n",
        "                        if not beta_stacked.dropna().empty: # Check if not all NaN\n",
        "                            style_factors_calculated['beta'] = beta_stacked\n",
        "                            print(\" - Calculated Beta.\")\n",
        "                        else: print(\"WARN: Beta calculation resulted in empty or all-NaN series after stacking.\")\n",
        "                    else: print(\"WARN: No valid beta Series were generated.\")\n",
        "                except ValueError as e_concat_beta:\n",
        "                     print(f\"ERROR concatenating beta results: {e_concat_beta}\")\n",
        "                     print(\"WARN: Skipping Beta factor due to concatenation error.\")\n",
        "            else:\n",
        "                print(\"WARN: No beta values could be calculated for any asset.\")\n",
        "    else: print(\"WARN: Skipping Beta calc due to missing price/benchmark lookback data.\")\n",
        "\n",
        "\n",
        "    # 3. Residual Volatility: Rolling std dev of returns\n",
        "    if not prices_lookback.empty:\n",
        "        if 'asset_returns_lb' not in locals(): # Calculate if not done for beta\n",
        "             asset_returns_lb = prices_lookback.loc[:, assets].pct_change() # Ensure asset alignment\n",
        "        rolling_std_ret = asset_returns_lb.rolling(neutralization_lookback, min_periods=min_periods_neut).std()\n",
        "        # Slice to analysis dates *before* stacking\n",
        "        res_vol_stacked = rolling_std_ret.loc[dates_index, assets].stack(future_stack=True).rename('residual_vol')\n",
        "        res_vol_stacked.index.names = ['date', 'asset'] # Set Index Names\n",
        "        if not res_vol_stacked.dropna().empty: # Check if not all NaN\n",
        "             style_factors_calculated['residual_vol'] = res_vol_stacked\n",
        "             print(\" - Calculated Residual Volatility.\")\n",
        "        else: print(\"WARN: Residual Volatility calculation resulted in empty or all-NaN series.\")\n",
        "    else: print(\"WARN: Skipping Residual Volatility calc due to missing price lookback data.\")\n",
        "\n",
        "\n",
        "    # --- Combine all calculated factors at the end ---\n",
        "    if style_factors_calculated:\n",
        "         # Ensure all components are Series before concat\n",
        "         valid_components = {k: v for k, v in style_factors_calculated.items() if isinstance(v, pd.Series)}\n",
        "         if valid_components:\n",
        "             style_factors = pd.concat(valid_components.values(), axis=1) # Combine valid Series into DF\n",
        "             # Reindex just in case some date/asset combos were missing in all factors\n",
        "             style_factors = style_factors.reindex(pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset']))\n",
        "             print(f\"Style factors calculation finished. Final Shape: {style_factors.shape}\")\n",
        "         else:\n",
        "              print(\"WARN: No valid style factor components were calculated.\")\n",
        "              style_factors = pd.DataFrame(index=pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset']))\n",
        "    else:\n",
        "         print(\"WARN: No style factors were successfully calculated.\")\n",
        "         style_factors = pd.DataFrame(index=pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset']))\n",
        "\n",
        "\n",
        "except Exception as e_style:\n",
        "    print(f\"ERROR calculating style factors: {e_style}\")\n",
        "    traceback.print_exc()\n",
        "    style_factors = pd.DataFrame(index=pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset']))\n",
        "    print(\"WARN: Style factors calculation failed. Proceeding without them for neutralization.\")\n",
        "\n",
        "\n",
        "# --- Analysis Function Definitions ---\n",
        "print(\"\\n--- Defining/Importing Analysis Functions ---\")\n",
        "\n",
        "def calculate_forward_returns(prices_df, periods):\n",
        "    \"\"\"Calculates forward returns for multiple periods. Corrected version 3.\"\"\"\n",
        "    # prices_df: Index=date, Columns=assets\n",
        "    all_fwd_returns = {} # Store DataFrames for each period\n",
        "\n",
        "    if prices_df.empty:\n",
        "         print(\"ERROR [Fwd Ret]: Input prices_df is empty.\")\n",
        "         return pd.DataFrame(index=pd.MultiIndex([[],[]], [[],[]], names=['date','asset']), columns=analysis_periods_str)\n",
        "\n",
        "    for p in periods:\n",
        "        fwd_ret_col_name = f'{p}D_fwd_ret'\n",
        "        # Calculate returns for all assets for this period 'p'\n",
        "        shifted_price = prices_df.shift(-p)\n",
        "        # Ensure alignment before division, handle potential NaNs gracefully\n",
        "        returns_p = (shifted_price / prices_df - 1).replace([np.inf, -np.inf], np.nan)\n",
        "        all_fwd_returns[fwd_ret_col_name] = returns_p\n",
        "\n",
        "    if not all_fwd_returns:\n",
        "        print(\"WARN [Fwd Ret]: No forward returns calculated.\")\n",
        "        return pd.DataFrame(index=pd.MultiIndex([[],[]], [[],[]], names=['date','asset']), columns=analysis_periods_str)\n",
        "\n",
        "    # Concat creates MultiIndex columns: ('1D_fwd_ret', 'asset1'), ('3D_fwd_ret', 'asset1'), ...\n",
        "    combined_fwd_returns_wide = pd.concat(all_fwd_returns, axis=1)\n",
        "    combined_fwd_returns_wide.columns.names = ['period', 'asset'] # Name the column levels\n",
        "\n",
        "    # Stack the 'asset' level from columns to index to get format:\n",
        "    # Index = MultiIndex('date', 'asset'), Columns = Index(['1D_fwd_ret', '3D_fwd_ret', ...])\n",
        "    fwd_returns_stacked = combined_fwd_returns_wide.stack(level='asset', future_stack=True)\n",
        "    fwd_returns_stacked.index.names = ['date', 'asset'] # Ensure final index names are correct\n",
        "    # Ensure columns are named correctly (should be the periods after stacking 'asset')\n",
        "    fwd_returns_stacked.columns.name = 'period' # Name the columns index\n",
        "\n",
        "    return fwd_returns_stacked\n",
        "\n",
        "def get_quantile_assignments(factor_df, num_quantiles=5):\n",
        "    \"\"\"Assigns assets to quantiles based on factor values for each date.\"\"\"\n",
        "    # Input factor_df: Index=(date, asset), Column='factor'\n",
        "    if factor_df.empty: return pd.DataFrame(columns=['quantile'], index=factor_df.index) # Handle empty input\n",
        "\n",
        "    factor_col_name = 'factor'\n",
        "    if factor_col_name not in factor_df.columns:\n",
        "        if isinstance(factor_df, pd.Series) and factor_df.name == factor_col_name:\n",
        "             factor_df = factor_df.to_frame()\n",
        "        elif not factor_df.empty: # Try using the first column if name isn't 'factor'\n",
        "             original_col = factor_df.columns[0]\n",
        "             factor_df = factor_df[[original_col]].rename(columns={original_col: factor_col_name})\n",
        "        else: # Cannot proceed if empty and no factor column\n",
        "            return pd.DataFrame(columns=['quantile'], index=factor_df.index)\n",
        "\n",
        "    # Use transform to handle broadcasting within groups safely\n",
        "    quantiles = factor_df.groupby(level='date')[factor_col_name].transform(\n",
        "        lambda x: pd.qcut(x, num_quantiles, labels=False, duplicates='drop')\n",
        "    ) + 1 # Labels 1 to N\n",
        "    quantiles = quantiles.rename('quantile')\n",
        "\n",
        "    return quantiles.to_frame() # Return as DataFrame\n",
        "\n",
        "\n",
        "def quantile_analysis(analysis_data, factor_display_name, num_quantiles=5, ret_col='1D_fwd_ret'):\n",
        "    \"\"\"Performs quantile return analysis.\"\"\"\n",
        "    # analysis_data: Index=(date, asset), Columns=['factor', ret_col]\n",
        "    if ret_col not in analysis_data.columns:\n",
        "        print(f\"WARN [Quantile Analysis]: Return column '{ret_col}' not found.\")\n",
        "        return None, None\n",
        "    if 'factor' not in analysis_data.columns:\n",
        "        print(f\"WARN [Quantile Analysis]: Factor column 'factor' not found.\")\n",
        "        return None, None\n",
        "    if analysis_data.empty or analysis_data[['factor', ret_col]].isna().all().all():\n",
        "         print(f\"WARN [Quantile Analysis]: Input data empty or all NaN for {factor_display_name}/{ret_col}.\")\n",
        "         return None, None\n",
        "\n",
        "    quantile_assignments = get_quantile_assignments(analysis_data[['factor']], num_quantiles)\n",
        "    if quantile_assignments.empty or quantile_assignments['quantile'].isna().all():\n",
        "         print(f\"WARN [Quantile Analysis]: Could not assign quantiles for {factor_display_name}.\")\n",
        "         return None, None\n",
        "\n",
        "    data_with_quantiles = analysis_data.join(quantile_assignments, how='inner').dropna(subset=['quantile'])\n",
        "    if data_with_quantiles.empty: # Check after join/dropna\n",
        "         print(f\"WARN [Quantile Analysis]: Data empty after joining quantiles for {factor_display_name}.\")\n",
        "         return None, None\n",
        "\n",
        "    # Mean return per quantile (averaged over time)\n",
        "    mean_ret_by_quantile = data_with_quantiles.groupby('quantile')[ret_col].mean()\n",
        "\n",
        "    # Cumulative return per quantile\n",
        "    daily_mean_ret_by_q = data_with_quantiles.groupby(['date', 'quantile'])[ret_col].mean().unstack(level='quantile')\n",
        "    # Fill missing daily quantile returns (e.g., if a quantile had no members) with 0 for cumulative calc\n",
        "    daily_mean_ret_by_q = daily_mean_ret_by_q.fillna(0)\n",
        "    # Calculate geometric cumulative returns\n",
        "    cumulative_ret_by_q = (1 + daily_mean_ret_by_q).cumprod() - 1\n",
        "\n",
        "    return mean_ret_by_quantile, cumulative_ret_by_q\n",
        "\n",
        "\n",
        "def calculate_quantile_turnover(quantile_assignments, num_quantiles=5):\n",
        "    \"\"\"Calculates quantile turnover.\"\"\"\n",
        "    # quantile_assignments: Index=(date, asset), Column='quantile'\n",
        "    if quantile_assignments.empty or quantile_assignments['quantile'].isna().all():\n",
        "        print(\"WARN [Turnover]: Input quantile assignments are empty or all NaN.\")\n",
        "        return pd.DataFrame() # Return empty df\n",
        "\n",
        "    turnover_results = {}\n",
        "    quantiles_unstacked = quantile_assignments['quantile'].unstack(level='asset')\n",
        "\n",
        "    # Ensure index is sorted for shift to work correctly\n",
        "    quantiles_unstacked = quantiles_unstacked.sort_index()\n",
        "\n",
        "    for q in range(1, num_quantiles + 1):\n",
        "        quantile_members = (quantiles_unstacked == q)\n",
        "        prev_members = quantile_members.shift(1)\n",
        "\n",
        "        # Align and stack, keeping only days where both current and previous exist\n",
        "        combined = pd.concat(\n",
        "            [quantile_members.stack(future_stack=True).rename('current'),\n",
        "             prev_members.stack(future_stack=True).rename('previous')],\n",
        "            axis=1\n",
        "        ).dropna() # Drop rows where either is NaN (i.e., first day, or if assets change)\n",
        "\n",
        "        if combined.empty:\n",
        "             # Handle case with only one day of data or no overlap\n",
        "             daily_turnover_series = pd.Series(np.nan, index=quantiles_unstacked.index)\n",
        "        else:\n",
        "            def daily_turnover(group):\n",
        "                # Check if group is empty or has wrong structure\n",
        "                if group.empty or not all(c in group.columns for c in ['current', 'previous']):\n",
        "                    return np.nan\n",
        "\n",
        "                stayed = (group['current'] & group['previous']).sum()\n",
        "                entered = (group['current'] & ~group['previous']).sum()\n",
        "                exited = (~group['current'] & group['previous']).sum()\n",
        "                total_current = group['current'].sum()\n",
        "                total_previous = group['previous'].sum()\n",
        "\n",
        "                avg_size = (total_current + total_previous) / 2.0\n",
        "                if avg_size < 1e-6: return 0.0 # Handle near-zero avg size\n",
        "\n",
        "                # Using: max(entered, exited) / avg_size\n",
        "                traded = max(entered, exited)\n",
        "                return traded / avg_size if avg_size > 0 else 0.0\n",
        "\n",
        "            # Apply daily turnover calculation\n",
        "            daily_turnover_series = combined.groupby(level='date').apply(daily_turnover)\n",
        "            # Reindex to original dates index to include days with NaN turnover\n",
        "            daily_turnover_series = daily_turnover_series.reindex(quantiles_unstacked.index)\n",
        "\n",
        "\n",
        "        turnover_results[f'Q{q}_Turnover'] = daily_turnover_series\n",
        "\n",
        "    turnover_df = pd.DataFrame(turnover_results)\n",
        "    if not turnover_df.empty:\n",
        "        turnover_df['Mean_Turnover'] = turnover_df.mean(axis=1)\n",
        "    return turnover_df\n",
        "\n",
        "\n",
        "def calculate_ic(analysis_data, factor_display_name, ret_col='1D_fwd_ret', method='spearman'):\n",
        "    \"\"\"Calculates Information Coefficient (IC).\"\"\"\n",
        "    # analysis_data: Index=(date, asset), Columns=['factor', ret_col]\n",
        "    if ret_col not in analysis_data.columns or 'factor' not in analysis_data.columns:\n",
        "         print(f\"WARN [IC]: Missing 'factor' or '{ret_col}' for {factor_display_name}\")\n",
        "         return None, None\n",
        "    if analysis_data.empty or analysis_data[['factor', ret_col]].isna().all().all():\n",
        "         print(f\"WARN [IC]: Input data empty or all NaN for {factor_display_name}/{ret_col}.\")\n",
        "         return None, None\n",
        "\n",
        "    def ic_calc(group):\n",
        "        group_clean = group[['factor', ret_col]].dropna()\n",
        "        if len(group_clean) < 3: return np.nan # Need >= 3 points for reliable correlation? Usually 2 is min.\n",
        "        try:\n",
        "            # Check for zero variance before calculating correlation\n",
        "            factor_std_dev = group_clean['factor'].std()\n",
        "            ret_std_dev = group_clean[ret_col].std()\n",
        "            if pd.isna(factor_std_dev) or factor_std_dev < 1e-9 or pd.isna(ret_std_dev) or ret_std_dev < 1e-9:\n",
        "                 return 0.0 # Treat constant series as zero correlation\n",
        "\n",
        "            if method == 'spearman':\n",
        "                coeff, p_val = spearmanr(group_clean['factor'], group_clean[ret_col])\n",
        "                return coeff if pd.notna(coeff) else 0.0 # Return 0 if spearman returns NaN\n",
        "            elif method == 'pearson':\n",
        "                coeff = group_clean['factor'].corr(group_clean[ret_col], method='pearson')\n",
        "                return coeff if pd.notna(coeff) else 0.0 # Return 0 if pearson returns NaN\n",
        "            else: return np.nan\n",
        "        except ValueError: # Handle other potential errors (e.g., from spearmanr)\n",
        "             return np.nan\n",
        "\n",
        "    daily_ic = analysis_data.groupby(level='date').apply(ic_calc)\n",
        "    daily_ic.name = f'IC_{method}' # Rename the resulting Series\n",
        "\n",
        "    # Summarize IC\n",
        "    ic_mean = daily_ic.mean()\n",
        "    ic_std = daily_ic.std()\n",
        "    icir = ic_mean / ic_std if pd.notna(ic_std) and ic_std > 1e-9 else np.nan # Avoid div by zero/tiny std\n",
        "    hit_rate = (daily_ic > 1e-9).mean() if not daily_ic.dropna().empty else np.nan # Use > small epsilon for hit rate\n",
        "    obs_days = daily_ic.count() # Count non-NaN IC days\n",
        "\n",
        "    ic_summary = pd.Series({\n",
        "        'Mean IC': ic_mean,\n",
        "        'Std Dev IC': ic_std,\n",
        "        'ICIR': icir,\n",
        "        'Hit Rate (>0)': hit_rate,\n",
        "        'Observations (Days)': obs_days\n",
        "    }, name=ret_col) # Use ret_col as the Series name\n",
        "\n",
        "    return ic_summary, daily_ic.to_frame() # Return daily IC as DataFrame\n",
        "\n",
        "\n",
        "def calculate_factor_returns(analysis_data, factor_display_name, ret_col='1D_fwd_ret'):\n",
        "    \"\"\"Calculates factor returns (e.g., long/short portfolio based on factor).\"\"\"\n",
        "    # analysis_data: Index=(date, asset), Columns=['factor', ret_col]\n",
        "    if ret_col not in analysis_data.columns or 'factor' not in analysis_data.columns:\n",
        "        print(f\"WARN [Factor Returns]: Missing 'factor' or '{ret_col}' for {factor_display_name}\")\n",
        "        return None, None, None, None\n",
        "    if analysis_data.empty or analysis_data[['factor', ret_col]].isna().all().all():\n",
        "         print(f\"WARN [Factor Returns]: Input data empty or all NaN for {factor_display_name}/{ret_col}.\")\n",
        "         return None, None, None, None\n",
        "\n",
        "    # 1. Standardize Factor (cross-sectionally)\n",
        "    factor_std = analysis_data.groupby(level='date')['factor'].transform(\n",
        "        lambda x: (x - x.mean()) / x.std() if pd.notna(x.std()) and x.std() > 1e-9 else (x - x.mean()) # Handle zero/tiny std dev\n",
        "    ).fillna(0) # Fill NaNs after standardization (e.g., single asset days) with 0 weight\n",
        "\n",
        "    # 2. Calculate Weighted Return for each day\n",
        "    analysis_data_temp = analysis_data[[ret_col]].copy() # Only need return col\n",
        "    analysis_data_temp['factor_std'] = factor_std\n",
        "    analysis_data_temp['weighted_ret'] = analysis_data_temp['factor_std'] * analysis_data_temp[ret_col]\n",
        "\n",
        "    # --- Daily Factor Return: Weighted average return ---\n",
        "    # Sum of (weight * return) / Sum of abs(weights) <-- For dollar neutral L/S\n",
        "    sum_weighted_ret = analysis_data_temp.groupby(level='date')['weighted_ret'].sum()\n",
        "    sum_abs_weights = analysis_data_temp.groupby(level='date')['factor_std'].apply(lambda x: x.abs().sum())\n",
        "    # Avoid division by zero/NaN if sum of abs weights is zero/NaN for a day\n",
        "    daily_factor_return = (sum_weighted_ret / sum_abs_weights.replace(0, np.nan)).dropna()\n",
        "    daily_factor_return.name = 'factor_daily_ret'\n",
        "\n",
        "\n",
        "    # 3. Calculate Cumulative Return\n",
        "    cumulative_factor_return = pd.Series(index=daily_factor_return.index, dtype=float)\n",
        "    if not daily_factor_return.empty:\n",
        "        cumulative_factor_return = (1 + daily_factor_return).cumprod() - 1\n",
        "    cumulative_factor_return.name = 'factor_cum_ret'\n",
        "\n",
        "    # 4. Calculate Annualized Statistics\n",
        "    ann_factor = 252 # Assuming 252 trading days per year\n",
        "    num_days = len(daily_factor_return)\n",
        "    ann_ret, ann_vol, sharpe = np.nan, np.nan, np.nan # Defaults\n",
        "    if num_days > 5: # Require min days for meaningful stats\n",
        "         mean_daily_ret = daily_factor_return.mean()\n",
        "         std_daily_ret = daily_factor_return.std()\n",
        "         if pd.notna(mean_daily_ret): ann_ret = mean_daily_ret * ann_factor\n",
        "         if pd.notna(std_daily_ret) and std_daily_ret > 1e-9: # Avoid div by tiny std\n",
        "              ann_vol = std_daily_ret * np.sqrt(ann_factor)\n",
        "              if pd.notna(ann_ret) and ann_vol > 1e-9 : sharpe = ann_ret / ann_vol # Ensure vol > 0\n",
        "\n",
        "    ann_stats = pd.Series({\n",
        "        'Annualized Return': ann_ret,\n",
        "        'Annualized Volatility': ann_vol,\n",
        "        'Sharpe Ratio': sharpe,\n",
        "        'Observations (Days)': num_days\n",
        "    }, name=ret_col) # Use ret_col as the Series name\n",
        "\n",
        "    return daily_factor_return.to_frame(), cumulative_factor_return.to_frame(), None, ann_stats # Placeholder for drawdown\n",
        "\n",
        "# --- FIX IS HERE ---\n",
        "def calculate_forward_returns_for_decay(prices_df, max_lag):\n",
        "    \"\"\"Calculates forward returns for multiple lags up to max_lag.\"\"\"\n",
        "    fwd_rets_dict = {}\n",
        "    print(f\"Calculating fwd returns for decay (1 to {max_lag} days)...\")\n",
        "    if prices_df.empty:\n",
        "         print(\"WARN [Decay FwdRets]: Input prices_df is empty.\")\n",
        "         return fwd_rets_dict\n",
        "\n",
        "    shifted_prices = {lag: prices_df.shift(-lag) for lag in range(1, max_lag + 1)}\n",
        "    with tqdm(total=max_lag, desc=\"Fwd Returns Decay\", leave=False) as pbar: # Set leave=False\n",
        "        for lag in range(1, max_lag + 1):\n",
        "            ret_col_name = f'{lag}D_fwd_ret'\n",
        "            fwd_ret_lag = (shifted_prices[lag] / prices_df - 1).replace([np.inf, -np.inf], np.nan)\n",
        "            # Stack to get (date, asset) index\n",
        "            fwd_ret_stacked = fwd_ret_lag.stack(future_stack=True).rename(ret_col_name)\n",
        "            # <<< FIX: Set index names >>>\n",
        "            fwd_ret_stacked.index.names = ['date', 'asset']\n",
        "            # <<< END FIX >>>\n",
        "            if not fwd_ret_stacked.dropna().empty: # Check not all NaN\n",
        "                fwd_rets_dict[lag] = fwd_ret_stacked.dropna() # Store cleaned series\n",
        "            pbar.update(1)\n",
        "    print(f\"Finished calculating {len(fwd_rets_dict)} forward returns for decay.\")\n",
        "    return fwd_rets_dict # Dict: {lag: Series(Index=(date,asset), Value=ret)}\n",
        "# --- END FIX ---\n",
        "\n",
        "def calculate_ic_decay(factor_series_clean, fwd_returns_for_decay_dict, max_lag, method='spearman'):\n",
        "    \"\"\"Calculates IC decay over multiple forward return periods.\"\"\"\n",
        "    # factor_series_clean: Series, Index=(date, asset), Name='factor'\n",
        "    ic_decay_values = {}\n",
        "    print(\"Calculating IC Decay...\")\n",
        "    if factor_series_clean.empty or not fwd_returns_for_decay_dict:\n",
        "         print(\"WARN [IC Decay]: Factor series empty or no fwd returns provided.\")\n",
        "         return pd.Series(dtype=float, name=f'Mean_IC_{method}_Decay').rename_axis('Lag (Days)')\n",
        "\n",
        "    with tqdm(total=max_lag, desc=\"IC Decay\", leave=False) as pbar: # Set leave=False\n",
        "        for lag in range(1, max_lag + 1):\n",
        "            result_ic = np.nan # Default\n",
        "            if lag in fwd_returns_for_decay_dict:\n",
        "                fwd_ret_lag = fwd_returns_for_decay_dict[lag]\n",
        "                if not fwd_ret_lag.empty:\n",
        "                    # Ensure both series are frames for merge (safer)\n",
        "                    factor_frame = factor_series_clean.to_frame()\n",
        "                    ret_frame = fwd_ret_lag.to_frame()\n",
        "                    # <<< Check index names before merge for debugging >>>\n",
        "                    # print(f\"DEBUG IC Decay Lag {lag}: Factor index names: {factor_frame.index.names}, Ret index names: {ret_frame.index.names}\")\n",
        "                    # <<< End Debug >>>\n",
        "                    try:\n",
        "                        aligned_decay = pd.merge(factor_frame, ret_frame,\n",
        "                                                 left_index=True, right_index=True, how='inner')\n",
        "                        aligned_decay = aligned_decay.dropna() # Drop rows with NaNs in either column\n",
        "\n",
        "                        if len(aligned_decay) > 2: # Need enough points\n",
        "                            # Calculate mean daily IC for this lag\n",
        "                            def ic_calc_decay(group):\n",
        "                                if len(group) < 3: return np.nan\n",
        "                                try:\n",
        "                                     # Check variance again\n",
        "                                    factor_std_dev = group['factor'].std()\n",
        "                                    ret_std_dev = group[fwd_ret_lag.name].std()\n",
        "                                    if pd.isna(factor_std_dev) or factor_std_dev < 1e-9 or pd.isna(ret_std_dev) or ret_std_dev < 1e-9:\n",
        "                                         return 0.0 # Treat constant series as zero correlation\n",
        "\n",
        "                                    if method == 'spearman':\n",
        "                                        coeff, p_val = spearmanr(group['factor'], group[fwd_ret_lag.name])\n",
        "                                        return coeff if pd.notna(coeff) else 0.0\n",
        "                                    elif method == 'pearson':\n",
        "                                        coeff = group['factor'].corr(group[fwd_ret_lag.name], method='pearson')\n",
        "                                        return coeff if pd.notna(coeff) else 0.0\n",
        "                                    else: return np.nan\n",
        "                                except ValueError: return np.nan # Handle other errors (e.g. spearmanr issue)\n",
        "\n",
        "                            daily_ic_lag = aligned_decay.groupby(level='date').apply(ic_calc_decay)\n",
        "                            result_ic = daily_ic_lag.mean() # Store the mean IC for this lag\n",
        "                    except ValueError as e_merge_decay: # Catch specific merge errors\n",
        "                         print(f\"ERROR [IC Decay Lag {lag}]: Merge failed - {e_merge_decay}. Skipping lag.\")\n",
        "                         result_ic = np.nan # Ensure NaN if merge fails\n",
        "                    except Exception as e_decay_calc: # Catch other errors during calculation\n",
        "                         print(f\"ERROR [IC Decay Lag {lag}]: Calculation failed - {e_decay_calc}. Skipping lag.\")\n",
        "                         result_ic = np.nan\n",
        "\n",
        "            ic_decay_values[lag] = result_ic\n",
        "            pbar.update(1)\n",
        "\n",
        "    ic_decay_series = pd.Series(ic_decay_values, name=f'Mean_IC_{method}_Decay')\n",
        "    ic_decay_series.index.name = 'Lag (Days)'\n",
        "    return ic_decay_series\n",
        "\n",
        "# --- END OF ANALYSIS FUNCTION DEFINITIONS ---\n",
        "\n",
        "\n",
        "# --- Helper function to save results to Excel ---\n",
        "# Defined once before the loop starts\n",
        "def save_to_excel_combined(df_to_save, base_sheet_name, factor_disp_name, writer_obj):\n",
        "     \"\"\"Saves a dataframe to a sheet in the combined Excel file, handling naming and timezones.\"\"\"\n",
        "     sheet_name_raw = f\"{factor_disp_name}_{base_sheet_name}\"\n",
        "     if len(sheet_name_raw) > 31:\n",
        "          max_len, len_base, len_underscore = 31, len(base_sheet_name), 1\n",
        "          available_for_factor = max_len - len_base - len_underscore\n",
        "          if available_for_factor < 3: sheet_name = sheet_name_raw[:max_len] # Min 3 chars for factor part\n",
        "          else: sheet_name = f\"{factor_disp_name[:available_for_factor]}_{base_sheet_name}\"\n",
        "          print(f\"WARN: Sheet name '{sheet_name_raw}' > 31 chars. Truncated to '{sheet_name}'.\")\n",
        "     else: sheet_name = sheet_name_raw\n",
        "\n",
        "     if df_to_save is not None and not df_to_save.empty:\n",
        "          try:\n",
        "               df_copy = df_to_save.copy()\n",
        "               # Remove timezone info for Excel compatibility\n",
        "               if isinstance(df_copy.index, pd.DatetimeIndex): df_copy.index = df_copy.index.tz_localize(None)\n",
        "               if isinstance(df_copy.columns, pd.DatetimeIndex): df_copy.columns = df_copy.columns.tz_localize(None)\n",
        "               if isinstance(df_copy.index, pd.MultiIndex):\n",
        "                   new_levels = [lvl.tz_localize(None) if isinstance(lvl, pd.DatetimeIndex) else lvl for lvl in df_copy.index.levels]\n",
        "                   df_copy.index = df_copy.index.set_levels(new_levels)\n",
        "               if isinstance(df_copy.columns, pd.MultiIndex):\n",
        "                   new_levels = [lvl.tz_localize(None) if isinstance(lvl, pd.DatetimeIndex) else lvl for lvl in df_copy.columns.levels]\n",
        "                   df_copy.columns = df_copy.columns.set_levels(new_levels)\n",
        "\n",
        "               df_copy.to_excel(writer_obj, sheet_name=sheet_name)\n",
        "               # print(f\"DEBUG: Saved sheet '{sheet_name}'\") # Optional debug\n",
        "               return True # Indicate sheet was saved\n",
        "          except Exception as e_save: print(f\"ERROR saving sheet '{sheet_name}': {e_save}\")\n",
        "     else: print(f\"INFO: No data to save for sheet '{sheet_name}'.\")\n",
        "     return False # Indicate sheet was not saved\n",
        "\n",
        "\n",
        "# --- Calculate Forward Returns for Analysis & IC Decay (Do ONCE before loop) ---\n",
        "if prices.empty or prices.isna().all().all():\n",
        "    print(\"ERROR: Price data is empty or all NaN. Cannot calculate forward returns. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\nCalculating forward returns for analysis periods: {fwd_ret_periods_int} days...\")\n",
        "forward_returns_df_stacked = calculate_forward_returns(prices.copy(), periods=fwd_ret_periods_int)\n",
        "\n",
        "if forward_returns_df_stacked.empty:\n",
        "    print(\"ERROR: Main forward returns calculation failed or resulted in empty data. Exiting.\")\n",
        "    exit()\n",
        "# Check if expected column names exist\n",
        "expected_ret_cols_present = all(col in forward_returns_df_stacked.columns for col in analysis_periods_str)\n",
        "if not expected_ret_cols_present:\n",
        "     print(f\"ERROR: Missing expected forward return columns in calculated df. Expected: {analysis_periods_str}, Found: {forward_returns_df_stacked.columns.tolist()}\")\n",
        "     # exit() # Exit or proceed carefully\n",
        "\n",
        "print(f\"\\nCalculating forward returns for IC decay (up to {MAX_DECAY_LAG} days)...\")\n",
        "fwd_returns_for_decay_dict = calculate_forward_returns_for_decay(prices.copy(), MAX_DECAY_LAG)\n",
        "if not fwd_returns_for_decay_dict:\n",
        "    print(\"WARN: Could not calculate forward returns for IC Decay. Decay analysis will be skipped.\")\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# === Starting Factor Analysis Loop ===\n",
        "# =======================================================\n",
        "print(f\"\\nPreparing single Excel output file: {combined_output_filename}\\n\")\n",
        "\n",
        "if precalculated_factors_df.empty:\n",
        "    print(\"ERROR: No pre-calculated factors found or loaded. Skipping analysis loop.\")\n",
        "else:\n",
        "    # --- Start Excel Writer context ---\n",
        "    overall_success = False # Flag to track if ANY sheet gets written\n",
        "    try:\n",
        "        with pd.ExcelWriter(combined_output_filename, engine='openpyxl') as writer:\n",
        "            unique_factor_names = precalculated_factors_df.columns.get_level_values('factor_name').unique()\n",
        "            print(f\"Analyzing {len(unique_factor_names)} factors found in the input DataFrame...\")\n",
        "\n",
        "            # --- Loop through each factor ---\n",
        "            for factor_name in unique_factor_names:\n",
        "                print(f\"\\n\\n{'='*20} Processing Factor: {factor_name} {'='*20}\")\n",
        "                factor_timer_start = time.time() # Timer for each factor\n",
        "                sheets_saved_this_factor = 0 # Count sheets for this factor\n",
        "\n",
        "                # --- Extract Raw Factor ---\n",
        "                try:\n",
        "                    raw_factor_df = precalculated_factors_df.xs(factor_name, level='factor_name', axis=1).copy()\n",
        "                    raw_factor_df.columns.name = 'asset'\n",
        "                    raw_factor_df.index.name = 'date'\n",
        "                except KeyError:\n",
        "                     print(f\"ERROR: Could not extract factor '{factor_name}' using xs. Skipping.\")\n",
        "                     continue\n",
        "                except Exception as e_extract:\n",
        "                     print(f\"ERROR: Unexpected error extracting factor '{factor_name}': {e_extract}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                if raw_factor_df.empty or raw_factor_df.isna().all().all():\n",
        "                    print(f\"WARN: Raw factor data for {factor_name} is empty or all NaNs after extraction. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # --- Factor Neutralization ---\n",
        "                print(\"\\n--- Performing Factor Neutralization ---\")\n",
        "                neut_timer_start = time.time()\n",
        "                neutralized_factor_df = pd.DataFrame(index=dates_index, columns=assets) # Reinitialize\n",
        "                neutralization_succeeded = False\n",
        "\n",
        "                # Check if any neutralization variables exist and align them\n",
        "                has_industry = False\n",
        "                industry_dummies_aligned = pd.DataFrame()\n",
        "                if 'industry_dummies_static' in locals() and not industry_dummies_static.empty:\n",
        "                     # Align index (assets) with the current final asset list\n",
        "                     industry_dummies_aligned = industry_dummies_static.reindex(assets).fillna(0)\n",
        "                     has_industry = not industry_dummies_aligned.empty\n",
        "\n",
        "                has_style = False\n",
        "                style_factors_aligned = pd.DataFrame()\n",
        "                if 'style_factors' in locals() and not style_factors.empty and not style_factors.isna().all().all():\n",
        "                     # Align style factors (which have MultiIndex date,asset) with raw_factor_df dates\n",
        "                     # And ensure assets match the final 'assets' list\n",
        "                     style_factors_aligned = style_factors.reindex(index=raw_factor_df.index, level='date')\n",
        "                     # Filter style factors to only include current assets\n",
        "                     valid_style_assets = style_factors_aligned.index.get_level_values('asset').unique().intersection(assets)\n",
        "                     if not valid_style_assets.empty:\n",
        "                         style_factors_aligned = style_factors_aligned[style_factors_aligned.index.get_level_values('asset').isin(valid_style_assets)]\n",
        "                         has_style = not style_factors_aligned.dropna(how='all').empty\n",
        "                     else: has_style = False\n",
        "\n",
        "\n",
        "                if not has_industry and not has_style:\n",
        "                    print(\"INFO: No neutralization variables available. Using raw factor.\")\n",
        "                    neutralized_factor_df = raw_factor_df.copy()\n",
        "                    neutralization_succeeded = False\n",
        "                else:\n",
        "                    print(\"Running neutralization regression day by day...\")\n",
        "                    neutralized_residuals_list = []\n",
        "                    # Use index from raw factor that has *some* data for iteration\n",
        "                    valid_dates_for_neut = raw_factor_df.dropna(how='all').index\n",
        "\n",
        "                    with tqdm(total=len(valid_dates_for_neut), desc=f\"Neutralizing {factor_name}\", leave=False) as pbar:\n",
        "                        for date in valid_dates_for_neut:\n",
        "                            factor_today = raw_factor_df.loc[date].dropna()\n",
        "                            if factor_today.empty:\n",
        "                                neutralized_residuals_list.append(pd.Series(np.nan, index=assets, name=date))\n",
        "                                pbar.update(1); continue\n",
        "\n",
        "                            X_list = []\n",
        "                            valid_assets_today = factor_today.index\n",
        "\n",
        "                            # Industry\n",
        "                            if has_industry:\n",
        "                                industry_today = industry_dummies_aligned.reindex(valid_assets_today).dropna(axis=1, how='all').fillna(0)\n",
        "                                # Drop industry dummies that are constant (e.g., all zero after reindex)\n",
        "                                industry_today = industry_today.loc[:, industry_today.nunique() > 1]\n",
        "                                if not industry_today.empty: X_list.append(industry_today)\n",
        "\n",
        "                            # Style Factors\n",
        "                            style_today_aligned_assets = pd.DataFrame() # Init empty\n",
        "                            if has_style and date in style_factors_aligned.index.get_level_values('date'):\n",
        "                                try:\n",
        "                                    style_today = style_factors_aligned.loc[pd.IndexSlice[date, :], :] # Use IndexSlice for robustness\n",
        "                                    if not style_today.empty:\n",
        "                                         # If only one style factor, it might be a Series, convert to frame\n",
        "                                         if isinstance(style_today, pd.Series): style_today = style_today.to_frame()\n",
        "\n",
        "                                         # Reindex style factors for today's valid assets and fill NaNs (e.g., with mean)\n",
        "                                         style_fill_value = style_today.mean() # Calculate mean before reindexing\n",
        "                                         style_today_aligned_assets = style_today.reindex(valid_assets_today, level='asset').fillna(style_fill_value)\n",
        "                                         # Drop style factors that are all NaN after reindexing/filling\n",
        "                                         style_today_aligned_assets = style_today_aligned_assets.dropna(axis=1, how='all') # Drop empty columns\n",
        "                                         if not style_today_aligned_assets.empty:\n",
        "                                             # Remove constant columns (important!) before adding model constant\n",
        "                                             non_const_cols = style_today_aligned_assets.loc[:, style_today_aligned_assets.nunique() > 1]\n",
        "                                             if not non_const_cols.empty: X_list.append(non_const_cols)\n",
        "                                except KeyError: pass # Date might not exist in aligned style factors\n",
        "                                except Exception as e_style_align:\n",
        "                                     print(f\"WARN: Error aligning style factors for {date}: {e_style_align}\")\n",
        "\n",
        "\n",
        "                            if not X_list:\n",
        "                                residuals_today = factor_today\n",
        "                            else:\n",
        "                                try:\n",
        "                                    X_today = pd.concat(X_list, axis=1).astype(float) # Ensure float type\n",
        "                                    # Align Y (factor) and X (exposures) on common assets\n",
        "                                    common_assets = factor_today.index.intersection(X_today.index)\n",
        "                                    if common_assets.empty: # Handle case where no assets overlap after considering exposures\n",
        "                                         residuals_today = factor_today # Fallback to raw\n",
        "                                    else:\n",
        "                                        Y_aligned = factor_today.loc[common_assets].astype(float)\n",
        "                                        X_aligned = X_today.loc[common_assets]\n",
        "\n",
        "                                        # Drop rows/cols with all NaNs AFTER alignment (robustness)\n",
        "                                        X_aligned = X_aligned.dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
        "                                        Y_aligned = Y_aligned.loc[X_aligned.index] # Re-align Y\n",
        "\n",
        "                                        # Check for sufficient data points vs predictors\n",
        "                                        if Y_aligned.empty or X_aligned.empty or len(Y_aligned) <= X_aligned.shape[1]:\n",
        "                                            residuals_today = factor_today # Fallback\n",
        "                                        else:\n",
        "                                            X_w_const = sm.add_constant(X_aligned, has_constant='add')\n",
        "                                            model = sm.OLS(Y_aligned, X_w_const, missing='drop')\n",
        "                                            results = model.fit()\n",
        "                                            residuals_today = results.resid.reindex(Y_aligned.index).fillna(0) # Fill NaNs from regression with 0? Or keep NaN?\n",
        "\n",
        "                                except LinAlgError: # Handle cases like singular matrix\n",
        "                                     residuals_today = factor_today\n",
        "                                except ValueError as e_ols_val: # Handle dimension mismatches etc.\n",
        "                                     residuals_today = factor_today\n",
        "                                except Exception as e_ols:\n",
        "                                     print(f\"WARN: OLS failed unexpectedly for {factor_name} on {date}: {e_ols}\")\n",
        "                                     residuals_today = factor_today # Fallback to raw on error\n",
        "\n",
        "                            # Reindex residuals to full asset list, filling missing ones with NaN\n",
        "                            neutralized_residuals_list.append(residuals_today.reindex(assets).fillna(np.nan))\n",
        "                            pbar.update(1)\n",
        "\n",
        "                    # --- Combine daily neutralized results ---\n",
        "                    if neutralized_residuals_list:\n",
        "                         neutralized_factor_df_temp = pd.concat(neutralized_residuals_list, axis=1).T\n",
        "                         neutralized_factor_df_temp.index.name = 'date'\n",
        "                         # Reindex to ensure all analysis dates are present (fills missing dates with NaN)\n",
        "                         neutralized_factor_df = neutralized_factor_df_temp.reindex(dates_index)\n",
        "                         neutralization_succeeded = True\n",
        "                         print(f\"Neutralization completed for {factor_name}. ({(time.time() - neut_timer_start):.2f}s)\")\n",
        "                    else:\n",
        "                         print(f\"WARN: Neutralization yielded no results for {factor_name}. Using raw factor.\")\n",
        "                         neutralized_factor_df = raw_factor_df.copy()\n",
        "                         neutralization_succeeded = False\n",
        "\n",
        "\n",
        "                # --- Analysis Execution ---\n",
        "                print(\"\\n--- Starting Factor Analysis ---\")\n",
        "                analysis_timer_start = time.time()\n",
        "                factor_to_analyze_df = None\n",
        "                factor_source = \"None\"\n",
        "\n",
        "                # Decide which factor version to use for analysis\n",
        "                if neutralization_succeeded and not neutralized_factor_df.isna().all().all():\n",
        "                    factor_to_analyze_df = neutralized_factor_df.copy()\n",
        "                    factor_source = \"Neut\" # Shortened for sheet names\n",
        "                    print(f\"INFO: Using NEUTRALIZED factor '{factor_name}' for analysis.\")\n",
        "                elif not raw_factor_df.isna().all().all():\n",
        "                    factor_to_analyze_df = raw_factor_df.copy()\n",
        "                    factor_source = \"Raw\"\n",
        "                    print(f\"INFO: Using RAW factor '{factor_name}' for analysis.\")\n",
        "                else:\n",
        "                    print(f\"ERROR: No valid factor data (Raw or Neutralized) found for {factor_name}. Skipping analysis.\")\n",
        "                    continue\n",
        "\n",
        "                # --- Prepare for Analysis ---\n",
        "                analysis_performed = False\n",
        "                # Factor display name for sheet naming - keep it concise\n",
        "                factor_display_name = f\"{factor_name[:15]}_{factor_source}\" # Max 15 chars for factor part\n",
        "\n",
        "                # Stack the chosen factor (Index=date, Columns=assets) -> Series (Index=(date, asset))\n",
        "                factor_to_analyze_df.index.name = 'date'\n",
        "                factor_to_analyze_df.columns.name = 'asset'\n",
        "                factor_series = factor_to_analyze_df.stack(future_stack=True) # Use future_stack, dropna removed\n",
        "                factor_series.index.names = ['date', 'asset']\n",
        "                factor_series.rename('factor', inplace=True) # Ensure Series name is 'factor' for functions\n",
        "                factor_series_clean = factor_series.dropna() # Drop NaNs *after* stacking\n",
        "\n",
        "                # Align factor with forward returns (already stacked)\n",
        "                aligned_data = pd.DataFrame()\n",
        "                if factor_series_clean.empty:\n",
        "                    print(f\"ERROR: Factor series for {factor_display_name} is empty after dropna(). Skipping.\")\n",
        "                    continue\n",
        "                else:\n",
        "                    # Ensure forward returns are uniquely indexed if merging\n",
        "                    fwd_returns_unique = forward_returns_df_stacked[~forward_returns_df_stacked.index.duplicated(keep='first')]\n",
        "                    try:\n",
        "                        # Merge the factor Series (as frame) with the forward returns DataFrame\n",
        "                        aligned_data = pd.merge(factor_series_clean.to_frame(), fwd_returns_unique,\n",
        "                                                left_index=True, right_index=True, how='inner')\n",
        "                    except Exception as merge_err:\n",
        "                        print(f\"ERROR aligning data for {factor_display_name}: {merge_err}\")\n",
        "                        continue\n",
        "\n",
        "                # Final check on aligned data\n",
        "                # Drop rows where factor OR *any* of the analysis return periods are NaN\n",
        "                aligned_data_clean = aligned_data.dropna(subset=['factor'] + analysis_periods_str, how='any')\n",
        "\n",
        "                if aligned_data_clean.empty:\n",
        "                    print(f\"INFO: Skipping analysis for {factor_display_name} - no overlapping data.\")\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"Clean aligned data ready for {factor_display_name}. Shape: {aligned_data_clean.shape}\")\n",
        "                    # Identify return columns actually available after merge/dropna\n",
        "                    available_ret_cols = [col for col in analysis_periods_str if col in aligned_data_clean.columns and not aligned_data_clean[col].isna().all()]\n",
        "                    if not available_ret_cols:\n",
        "                        print(f\"ERROR: No valid forward returns columns remain for {factor_display_name}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # --- Initialize result containers ---\n",
        "                    all_ic_summaries, all_daily_ics = [], {}\n",
        "                    all_quantile_mean_rets, all_quantile_cum_rets = {}, {}\n",
        "                    factor_daily_returns_dict, cumulative_factor_returns_dict = {}, {}\n",
        "                    factor_analysis_summary = []\n",
        "\n",
        "                    # --- Calculate IC Decay ---\n",
        "                    ic_decay_results = pd.Series(dtype=float)\n",
        "                    if fwd_returns_for_decay_dict:\n",
        "                         # Pass the clean factor series (before alignment with specific returns)\n",
        "                         ic_decay_results = calculate_ic_decay(factor_series_clean.copy(), fwd_returns_for_decay_dict, MAX_DECAY_LAG, method=ic_method)\n",
        "\n",
        "                    # --- Calculate Quantile Turnover ---\n",
        "                    all_quantile_turnover = pd.DataFrame()\n",
        "                    print(\"\\n--- Calculating Quantile Turnover ---\")\n",
        "                    # Pass factor from aligned data, only need 'factor' column\n",
        "                    quantile_assignments = get_quantile_assignments(aligned_data_clean[['factor']].copy(), num_quantiles=num_quantiles)\n",
        "                    if not quantile_assignments.empty and not quantile_assignments['quantile'].isna().all():\n",
        "                        all_quantile_turnover = calculate_quantile_turnover(quantile_assignments, num_quantiles=num_quantiles)\n",
        "                    else: print(\"WARN: Could not calculate turnover due to empty/NaN quantile assignments.\")\n",
        "\n",
        "\n",
        "                    # --- Loop through Analysis Periods ---\n",
        "                    for ret_col in available_ret_cols:\n",
        "                        print(f\"\\n===== Analyzing {factor_display_name} vs {ret_col} =====\")\n",
        "                        # Subset data needed for this specific return period\n",
        "                        analysis_data_subset = aligned_data_clean[['factor', ret_col]].dropna()\n",
        "                        if analysis_data_subset.empty:\n",
        "                            print(f\"INFO: No valid data for {factor_display_name} vs {ret_col} after dropna.\")\n",
        "                            continue\n",
        "\n",
        "                        # --- Run Analyses ---\n",
        "                        try: # Add try-except around individual analyses\n",
        "                            mean_ret_q, cum_ret_q = quantile_analysis(analysis_data_subset.copy(), factor_display_name, num_quantiles=num_quantiles, ret_col=ret_col)\n",
        "                            if mean_ret_q is not None: all_quantile_mean_rets[ret_col] = mean_ret_q\n",
        "                            if cum_ret_q is not None: all_quantile_cum_rets[ret_col] = cum_ret_q\n",
        "\n",
        "                            ic_summary, daily_ic = calculate_ic(analysis_data_subset.copy(), factor_display_name, ret_col=ret_col, method=ic_method)\n",
        "                            if ic_summary is not None: all_ic_summaries.append(ic_summary)\n",
        "                            if daily_ic is not None: all_daily_ics[ret_col] = daily_ic\n",
        "\n",
        "                            factor_daily_ret, factor_cum_ret, _, factor_ann_stats = calculate_factor_returns(\n",
        "                                analysis_data_subset.copy(), factor_display_name, ret_col=ret_col\n",
        "                            )\n",
        "                            if factor_daily_ret is not None: factor_daily_returns_dict[ret_col] = factor_daily_ret\n",
        "                            if factor_cum_ret is not None: cumulative_factor_returns_dict[ret_col] = factor_cum_ret\n",
        "                            if factor_ann_stats is not None: factor_analysis_summary.append(factor_ann_stats)\n",
        "\n",
        "                            analysis_performed = True # Mark that at least one analysis ran\n",
        "                        except Exception as e_analyze_period:\n",
        "                             print(f\"ERROR during analysis of {factor_display_name} vs {ret_col}: {e_analyze_period}\")\n",
        "                             traceback.print_exc() # Print detailed error for this period\n",
        "\n",
        "                print(f\"Analysis calculations finished. ({(time.time() - analysis_timer_start):.2f}s)\")\n",
        "\n",
        "                # --- Save Results ---\n",
        "                if analysis_performed:\n",
        "                    print(f\"\\n--- Saving results for {factor_display_name} to Excel ---\")\n",
        "                    save_timer_start = time.time()\n",
        "                    # --- Save each result type using the helper defined outside the loop ---\n",
        "                    if all_ic_summaries: sheets_saved_this_factor += save_to_excel_combined(pd.concat(all_ic_summaries, axis=1), 'IC_Sum', factor_display_name, writer)\n",
        "                    if all_daily_ics: sheets_saved_this_factor += save_to_excel_combined(pd.concat(all_daily_ics, axis=1), 'IC_Daily', factor_display_name, writer)\n",
        "                    if 'ic_decay_results' in locals() and not ic_decay_results.empty: sheets_saved_this_factor += save_to_excel_combined(ic_decay_results.to_frame(), 'IC_Decay', factor_display_name, writer)\n",
        "                    if all_quantile_mean_rets: sheets_saved_this_factor += save_to_excel_combined(pd.concat(all_quantile_mean_rets, axis=1, join='outer').rename_axis('Quantile'), 'Q_MeanRet', factor_display_name, writer)\n",
        "                    if all_quantile_cum_rets:\n",
        "                        all_dfs_cum = []\n",
        "                        for ret_p, cum_df in all_quantile_cum_rets.items():\n",
        "                            if cum_df is not None and not cum_df.empty:\n",
        "                                cum_df.columns.name = 'Quantile'; cum_df.columns = pd.MultiIndex.from_product([[ret_p], cum_df.columns], names=['Return_Period', 'Quantile'])\n",
        "                                all_dfs_cum.append(cum_df)\n",
        "                        sheets_saved_this_factor += save_to_excel_combined(pd.concat(all_dfs_cum, axis=1, join='outer') if all_dfs_cum else pd.DataFrame(), 'Q_CumRet', factor_display_name, writer)\n",
        "                    if 'all_quantile_turnover' in locals() and not all_quantile_turnover.empty: sheets_saved_this_factor += save_to_excel_combined(all_quantile_turnover, 'Q_Turnover', factor_display_name, writer)\n",
        "                    if factor_analysis_summary: sheets_saved_this_factor += save_to_excel_combined(pd.concat(factor_analysis_summary, axis=1).rename_axis('Metric'), 'Fctr_Stats', factor_display_name, writer)\n",
        "                    if factor_daily_returns_dict: sheets_saved_this_factor += save_to_excel_combined(pd.concat(factor_daily_returns_dict, axis=1), 'Fctr_Ret', factor_display_name, writer)\n",
        "                    if cumulative_factor_returns_dict: sheets_saved_this_factor += save_to_excel_combined(pd.concat(cumulative_factor_returns_dict, axis=1), 'Fctr_CumRet', factor_display_name, writer)\n",
        "\n",
        "                    if sheets_saved_this_factor > 0:\n",
        "                        print(f\"--- Results for {factor_display_name} saved ({sheets_saved_this_factor} sheets). ({(time.time() - save_timer_start):.2f}s)---\")\n",
        "                        overall_success = True # Mark that at least one sheet was saved overall\n",
        "                    else:\n",
        "                        print(f\"--- No data frames were valid for saving for {factor_display_name}. ---\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"\\n--- No analysis performed for factor '{factor_display_name}'. No results saved. ---\")\n",
        "\n",
        "                print(f\"--- Factor {factor_name} processing time: {(time.time() - factor_timer_start):.2f}s ---\")\n",
        "            # --- End of loop through factors ---\n",
        "\n",
        "            if overall_success:\n",
        "                 print(f\"\\nAll factors processed. Finalizing Excel file: {combined_output_filename}\")\n",
        "            else:\n",
        "                 print(f\"\\nWARNING: All factors processed, but no analysis results were generated or saved.\")\n",
        "\n",
        "            # ExcelWriter context manager handles saving on exit IF overall_success is True implicitly\n",
        "\n",
        "    # --- End of Excel Writer context ---\n",
        "    except ImportError:\n",
        "        print(\"\\nERROR: Could not prepare Excel file. `openpyxl` library not found.\")\n",
        "        print(\"Please install it: pip install openpyxl\")\n",
        "    except Exception as e_main_loop:\n",
        "        print(f\"\\nERROR occurred during factor analysis loop or Excel writing: {e_main_loop}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# --- End of Script ---\n",
        "print(\"\\n=============================================\")\n",
        "print(\"=== Combined Factor Analysis Script Finished ===\")\n",
        "print(\"=============================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final version of LLM agent to decide which set of factor will be used {'HK_final_comprehensive_result_1.csv':'combined_factor_analysis_results.xlsx','HK_final_comprehensive_result_3.csv':'combined_factor_analysis_results_1.xlsx'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "OLLAMA_OPTIONS = {'temperature': 0.0, \"seed\": 42}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 files to compare in ./factor_analysis_output_combined/. Order: ['combined_factor_analysis_results.xlsx', 'combined_factor_analysis_results_1.xlsx']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Comparing Files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Round 1: Comparing 'combined_factor_analysis_results.xlsx' (Sorted Index 0) vs 'combined_factor_analysis_results_1.xlsx' (Sorted Index 1)\n",
            "Logging to: ./log/round-1-0_vs_1.log\n",
            "\n",
            "--- Comparing ---\n",
            "Index 1: combined_factor_analysis_results.xlsx\n",
            "Index 2: combined_factor_analysis_results_1.xlsx\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Comparing Files: 100%|██████████| 1/1 [00:45<00:00, 45.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM chose index: 2\n",
            "Result: 'combined_factor_analysis_results_1.xlsx' selected as new best.\n",
            "\n",
            "==============================\n",
            "Comparison finished.\n",
            "The best alpha file identified (based on sorted order comparison) is: combined_factor_analysis_results_1.xlsx\n",
            "This file had index 1 in the alphabetically sorted list.\n",
            "==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from ollama import Client\n",
        "from tqdm import tqdm\n",
        "import pandas as pd # Use pd alias consistently\n",
        "\n",
        "# --- Configuration ---\n",
        "OLLAMA_HOST = 'http://localhost:11434' # Adjust host if needed\n",
        "MODEL_NAME = \"llama3.2\" # Define model name once\n",
        "# --- LLM Generation Options for Determinism ---\n",
        "OLLAMA_OPTIONS = {\n",
        "    \"temperature\": 0.0,\n",
        "    \"seed\": 42,  # Use a fixed seed for reproducibility\n",
        "    # Add other options if needed/supported, e.g., top_p: 0.1\n",
        "}\n",
        "\n",
        "BASE_LOG_PATH = \"./log/\"\n",
        "PROMPT_DIR = \"./prompt_new_made/\" # Corrected inconsistent hyphen/underscore\n",
        "DATA_DIR = \"./data/\"\n",
        "FACTOR_RESULTS_DIR = \"./factor_analysis_output_combined/\" # Assuming relative to script location\n",
        "HK_STOCK_FILE = os.path.join(DATA_DIR, \"HK_stock_earning_reports.xlsx\") # Use os.path.join\n",
        "\n",
        "# Initialize the Ollama client\n",
        "try:\n",
        "    client = Client(host=OLLAMA_HOST)\n",
        "    # Optional: Check connection or model availability if needed\n",
        "    # client.list()\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Ollama client at {OLLAMA_HOST}: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "\n",
        "# Global variable for the current log file name (set within the loop)\n",
        "log_file = \"\" # Initialized globally\n",
        "\n",
        "\n",
        "# --- Remove Unused Functions ---\n",
        "# def retry_until_expected(run, thread_id, expect): ...\n",
        "# def get_last_text_message(thread_id): ...\n",
        "\n",
        "\n",
        "# --- Helper Functions --- (Keep your improved versions from previous debug)\n",
        "\n",
        "def log_to_file(type, message):\n",
        "    \"\"\"Logs input/output messages to the current log file.\"\"\"\n",
        "    global log_file\n",
        "    if not log_file:\n",
        "        print(\"Warning: log_file name not set before logging.\")\n",
        "        # Optionally set a default log file name here?\n",
        "        # log_file = \"default_comparison.log\"\n",
        "        return\n",
        "\n",
        "    header = \"\"\n",
        "    if type == \"input\":\n",
        "        header = \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\\n\"\n",
        "    elif type == \"output\":\n",
        "        header = \"<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\n\"\n",
        "    elif type == \"info\" or type == \"error\" or type == \"warning\":\n",
        "         header = f\"--- {type.upper()} ---\\n\"\n",
        "    # else: header remains \"\" for plain messages\n",
        "\n",
        "    full_log_path = os.path.join(BASE_LOG_PATH, log_file)\n",
        "    os.makedirs(BASE_LOG_PATH, exist_ok=True)\n",
        "    try:\n",
        "        with open(full_log_path, \"a\", encoding=\"utf-8\") as file:\n",
        "            file.write(header + str(message) + \"\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to log file {full_log_path}: {e}\")\n",
        "\n",
        "\n",
        "def read_file_content(file_path):\n",
        "    \"\"\"Safely reads content from a file.\"\"\"\n",
        "    try:\n",
        "        # Detect encoding, common ones first\n",
        "        encodings_to_try = ['utf-8', 'gbk', 'latin-1']\n",
        "        content = None\n",
        "        for enc in encodings_to_try:\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding=enc) as f:\n",
        "                    content = f.read()\n",
        "                break\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "            except Exception as e:\n",
        "                return f\"Error reading file {file_path}: {e}\"\n",
        "\n",
        "        if content is None:\n",
        "             return f\"Could not decode file {file_path} with tried encodings.\"\n",
        "        return content\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: File not found at {file_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading file {file_path}: {e}\"\n",
        "\n",
        "\n",
        "def get_excel_data_as_string(file_path, max_rows=20):\n",
        "    \"\"\"Reads an Excel file and returns its content as a string.\"\"\"\n",
        "    try:\n",
        "        # Use the imported pandas as pd\n",
        "        df = pd.read_excel(file_path) # Removed hardcoded sheet_name unless necessary\n",
        "        return f\"Data from {os.path.basename(file_path)} (Headers and first {max_rows} rows):\\n{df.head(max_rows).to_string()}\"\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: Excel file not found at {file_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error reading Excel file {file_path}: {e}\"\n",
        "\n",
        "# --- Core Comparison Logic ---\n",
        "\n",
        "def compare(index1_path, index2_path):\n",
        "    \"\"\"\n",
        "    Compares two index files using Ollama with deterministic settings.\n",
        "    Returns \"1\" or \"2\".\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Comparing ---\")\n",
        "    print(f\"Index 1: {os.path.basename(index1_path)}\")\n",
        "    print(f\"Index 2: {os.path.basename(index2_path)}\")\n",
        "    log_to_file(\"info\", f\"Comparing {os.path.basename(index1_path)} vs {os.path.basename(index2_path)}\")\n",
        "\n",
        "    # --- Load Prompts ---\n",
        "    # Use os.path.join for robustness\n",
        "    instruction = read_file_content(os.path.join(PROMPT_DIR, \"0-instruction.md\"))\n",
        "    preamble = read_file_content(os.path.join(PROMPT_DIR, \"1-preamble.md\"))\n",
        "    file_structure_prompt = read_file_content(os.path.join(PROMPT_DIR, \"2-file_structure.md\"))\n",
        "    index1_prompt = read_file_content(os.path.join(PROMPT_DIR, \"3-index1.md\"))\n",
        "    index2_comparison_prompt_template = read_file_content(os.path.join(PROMPT_DIR, \"4-index2.md\"))\n",
        "\n",
        "    # Basic check if prompts loaded correctly (can be more robust)\n",
        "    if any(p.startswith(\"Error:\") for p in [instruction, preamble, file_structure_prompt, index1_prompt, index2_comparison_prompt_template]):\n",
        "         print(\"Error loading one or more prompt files. Check paths and file existence.\")\n",
        "         log_to_file(\"error\", \"Failed to load one or more prompt files.\")\n",
        "         # Decide how to handle this - maybe return a default or raise exception\n",
        "         return \"1\" # Or None, or raise error\n",
        "\n",
        "    # --- Build Conversation History ---\n",
        "    thread_messages = []\n",
        "    # Optional: Add instruction as system message if appropriate\n",
        "    # if instruction and not instruction.startswith(\"Error:\"):\n",
        "    #    thread_messages.append({\"role\": \"system\", \"content\": instruction})\n",
        "\n",
        "    # Function to handle chat calls and logging consistently\n",
        "    def run_chat_step(user_content, log_input_summary=None):\n",
        "        nonlocal thread_messages # Allow modifying the outer scope variable\n",
        "        if log_input_summary:\n",
        "            log_to_file(\"input\", log_input_summary)\n",
        "        else:\n",
        "             log_to_file(\"input\", user_content[:500] + \"...\") # Log beginning of long content\n",
        "\n",
        "        thread_messages.append({\"role\": \"user\", \"content\": user_content})\n",
        "        try:\n",
        "            # *** Apply deterministic options here ***\n",
        "            response = client.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=thread_messages,\n",
        "                options=OLLAMA_OPTIONS # Pass the options dictionary\n",
        "            )\n",
        "            response_content = response['message']['content'].strip()\n",
        "            log_to_file(\"output\", response_content)\n",
        "            thread_messages.append(response['message']) # Add assistant response to history\n",
        "            return response_content\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Ollama chat step: {e}\")\n",
        "            log_to_file(\"error\", f\"Ollama chat error: {e}\")\n",
        "            return None # Indicate error\n",
        "\n",
        "    # 1. Preamble\n",
        "    if run_chat_step(preamble) is None: return \"1\" # Handle error, return default\n",
        "\n",
        "    # 2. File Structure & HK Data\n",
        "    hk_data_str = get_excel_data_as_string(HK_STOCK_FILE)\n",
        "    content_with_hk_data = f\"{file_structure_prompt}\\n\\n{hk_data_str}\"\n",
        "    if run_chat_step(content_with_hk_data, log_input_summary=f\"{file_structure_prompt[:100]}...\\n+ HK Data from {os.path.basename(HK_STOCK_FILE)}\") is None: return \"1\"\n",
        "\n",
        "    # 3. Index 1 Info (Prompt only)\n",
        "    prompt_for_index1 = f\"{index1_prompt}\\n\\nFile Reference: {os.path.basename(index1_path)}\"\n",
        "    if run_chat_step(prompt_for_index1, log_input_summary=f\"{index1_prompt[:100]}...\\n+ Ref: {os.path.basename(index1_path)}\") is None: return \"1\"\n",
        "\n",
        "    # 4. Index 2 & Comparison Request\n",
        "    index1_content = read_file_content(index1_path)\n",
        "    index2_content = read_file_content(index2_path)\n",
        "\n",
        "    if index1_content.startswith(\"Error:\") or index2_content.startswith(\"Error:\"):\n",
        "        print(f\"Error reading index files: {index1_content} / {index2_content}\")\n",
        "        log_to_file(\"error\", f\"Error reading index files:\\n1: {index1_content}\\n2: {index2_content}\")\n",
        "        return \"1\" # Default on file read error\n",
        "\n",
        "    # Construct the final comparison prompt\n",
        "    comparison_prompt = f\"\"\"{index2_comparison_prompt_template}\n",
        "\n",
        "    --- Data for Index 1 ({os.path.basename(index1_path)}) ---\n",
        "    {index1_content}\n",
        "    --- End of Data for Index 1 ---\n",
        "\n",
        "    --- Data for Index 2 ({os.path.basename(index2_path)}) ---\n",
        "    {index2_content}\n",
        "    --- End of Data for Index 2 ---\n",
        "\n",
        "    YOU MUST NOT GENERATE ANY PYTHON CODES.\n",
        "\n",
        "    Based on all the information provided (including previous context about file structures, HK data, and the data for the two indices above), which index represents the better alpha strategy?\n",
        "\n",
        "    Please respond with only the single digit \"1\" if {os.path.basename(index1_path)} is better, or the single digit \"2\" if {os.path.basename(index2_path)} is better. Your response must contain *only* the number and nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    # Run final comparison chat\n",
        "    final_response_content = run_chat_step(comparison_prompt, log_input_summary=f\"{index2_comparison_prompt_template[:100]}...\\n+ Data for {os.path.basename(index1_path)} & {os.path.basename(index2_path)}\\n+ Instruction: Respond '1' or '2'\")\n",
        "    if final_response_content is None: return \"1\" # Handle error\n",
        "\n",
        "    # --- Extract Decision (Stricter check first) ---\n",
        "    if final_response_content == \"1\":\n",
        "        index = \"1\"\n",
        "    elif final_response_content == \"2\":\n",
        "        index = \"2\"\n",
        "    else:\n",
        "        # Fallback to regex if the strict check fails\n",
        "        index_match = re.search(r\"\\b(1|2)\\b\", final_response_content)\n",
        "        if index_match:\n",
        "            index = index_match.group(1)\n",
        "            warning_msg = f\"LLM response ('{final_response_content}') was not strictly '1' or '2'. Extracted '{index}' using regex.\"\n",
        "            print(f\"Warning: {warning_msg}\")\n",
        "            log_to_file(\"warning\", warning_msg)\n",
        "        else:\n",
        "            error_msg = f\"Could not determine the better index from the response: '{final_response_content}'. Defaulting to 1.\"\n",
        "            print(f\"Error: {error_msg}\")\n",
        "            log_to_file(\"error\", error_msg)\n",
        "            index = \"1\" # Default to 1 if unsure\n",
        "\n",
        "    log_to_file(\"info\", f\"Selected better alpha index: {index} ({os.path.basename(index1_path if index == '1' else index2_path)})\")\n",
        "    print(f\"LLM chose index: {index}\")\n",
        "    return index\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.isdir(FACTOR_RESULTS_DIR):\n",
        "        print(f\"Error: Factor results directory not found: {FACTOR_RESULTS_DIR}\")\n",
        "        exit(1)\n",
        "\n",
        "    files = os.listdir(FACTOR_RESULTS_DIR)\n",
        "    files = [f for f in files if f.lower().endswith((\".xlsx\", \".csv\", \".txt\")) and not f.startswith('~$')]\n",
        "\n",
        "    if not files:\n",
        "        print(f\"Error: No suitable files found in {FACTOR_RESULTS_DIR}\")\n",
        "        exit(1)\n",
        "\n",
        "    # *** Sort the files for consistent comparison order ***\n",
        "    files.sort()\n",
        "    print(f\"Found {len(files)} files to compare in {FACTOR_RESULTS_DIR}. Order: {files}\") # Show sorted order\n",
        "\n",
        "    best_file_name = files[0]\n",
        "    best_file_original_index = 0 # Track original index in the *sorted* list\n",
        "\n",
        "    # Use enumerate starting from 1 for the comparison loop\n",
        "    for i, next_file_name in enumerate(tqdm(files[1:], desc=\"Comparing Files\")):\n",
        "        round_num = i + 1\n",
        "        # Find original index of the 'next' file IN THE SORTED LIST\n",
        "        next_file_original_index = i + 1 # Since files[0] is index 0, files[1] is index 1 etc.\n",
        "\n",
        "        # Update global log file name for this comparison\n",
        "        log_file = f\"round-{round_num}-{best_file_original_index}_vs_{next_file_original_index}.log\"\n",
        "        print(f\"\\nRound {round_num}: Comparing '{best_file_name}' (Sorted Index {best_file_original_index}) vs '{next_file_name}' (Sorted Index {next_file_original_index})\")\n",
        "        print(f\"Logging to: {os.path.join(BASE_LOG_PATH, log_file)}\")\n",
        "\n",
        "        current_best_path = os.path.join(FACTOR_RESULTS_DIR, best_file_name)\n",
        "        next_file_path = os.path.join(FACTOR_RESULTS_DIR, next_file_name)\n",
        "\n",
        "        # Basic check if files exist before comparing (robustness)\n",
        "        if not os.path.exists(current_best_path) or not os.path.exists(next_file_path):\n",
        "             print(f\"Error: One or both files not found for comparison: {current_best_path}, {next_file_path}\")\n",
        "             log_to_file(\"error\", f\"Comparison skipped: File(s) not found - Best: {current_best_path}, Next: {next_file_path}\")\n",
        "             continue # Skip this comparison round\n",
        "\n",
        "        selected_index = compare(current_best_path, next_file_path)\n",
        "\n",
        "        if selected_index == \"2\":\n",
        "            print(f\"Result: '{next_file_name}' selected as new best.\")\n",
        "            log_to_file(\"info\", f\"'{next_file_name}' selected as new best over '{best_file_name}'.\")\n",
        "            best_file_name = next_file_name\n",
        "            # Update the index of the best file to the index of the winner (next file)\n",
        "            best_file_original_index = next_file_original_index\n",
        "        elif selected_index == \"1\":\n",
        "             print(f\"Result: '{best_file_name}' remains the best.\")\n",
        "             log_to_file(\"info\", f\"'{best_file_name}' remains best over '{next_file_name}'.\")\n",
        "             # best_file_original_index remains unchanged\n",
        "        else:\n",
        "            # Handle potential error case from compare function if it returned None or similar\n",
        "            print(f\"Warning: Comparison between {best_file_name} and {next_file_name} resulted in unexpected value '{selected_index}'. Keeping current best.\")\n",
        "            log_to_file(\"warning\", f\"Comparison between {best_file_name} and {next_file_name} resulted in unexpected value '{selected_index}'. Keeping current best.\")\n",
        "            # Keep the current best_file, best_file_original_index remains unchanged\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(f\"Comparison finished.\")\n",
        "    print(f\"The best alpha file identified (based on sorted order comparison) is: {best_file_name}\")\n",
        "    # Optional: map back to original unsorted index if needed, but sorted index is more meaningful for the run\n",
        "    print(f\"This file had index {best_file_original_index} in the alphabetically sorted list.\")\n",
        "    print(\"=\"*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
