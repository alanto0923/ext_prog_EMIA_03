{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Human review/ correction (if there is a minor mistake from LLM output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## If no, just neglect this part of code --> jump to alpha factors' calculation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting asteval\n",
            "  Downloading asteval-1.0.6-py3-none-any.whl.metadata (6.2 kB)\n",
            "Downloading asteval-1.0.6-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: asteval\n",
            "Successfully installed asteval-1.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install asteval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to correct formulas in: HK_final_comprehensive_result_1.csv\n",
            "Successfully loaded 'HK_final_comprehensive_result_1.csv'. Shape: (17, 3)\n",
            "  Index 4: Corrected '(MEAN(CLOSE, 20) - CLOSE)' ---> '(SMA(CLOSE, 20) - CLOSE)'\n",
            "  Index 7: Corrected '(ATR(14))' ---> 'ATR(HIGH, LOW, CLOSE, 14)'\n",
            "  Index 14: Corrected 'RSI(14)' ---> 'RSI(CLOSE, 14)'\n",
            "\n",
            "Successfully applied 3 corrections.\n",
            "Corrected data saved to: HK_final_comprehensive_result_1_corrected.csv\n",
            "You can now use 'HK_final_comprehensive_result_1_corrected.csv' as input for the next step.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "input_csv_filename = \"HK_final_comprehensive_result_1.csv\"\n",
        "# Save to a new file to be safe, you can change this to input_csv_filename to overwrite\n",
        "output_csv_filename = \"HK_final_comprehensive_result_1_corrected.csv\"\n",
        "\n",
        "# Dictionary mapping the row index to the corrected 'code' string\n",
        "corrections = {\n",
        "    4: '(SMA(CLOSE, 20) - CLOSE)',    # Changed MEAN to SMA\n",
        "    7: 'ATR(HIGH, LOW, CLOSE, 14)', # Added required arguments\n",
        "    14: 'RSI(CLOSE, 14)'             # Added required argument\n",
        "}\n",
        "\n",
        "# --- Main Logic ---\n",
        "print(f\"Attempting to correct formulas in: {input_csv_filename}\")\n",
        "\n",
        "if not os.path.exists(input_csv_filename):\n",
        "    print(f\"Error: Input file not found at '{input_csv_filename}'\")\n",
        "else:\n",
        "    try:\n",
        "        # Read the CSV file into a pandas DataFrame\n",
        "        df = pd.read_csv(input_csv_filename)\n",
        "        print(f\"Successfully loaded '{input_csv_filename}'. Shape: {df.shape}\")\n",
        "\n",
        "        if 'code' not in df.columns:\n",
        "             raise KeyError(\"The required 'code' column was not found in the CSV.\")\n",
        "\n",
        "        corrected_count = 0\n",
        "        # Iterate through the corrections dictionary\n",
        "        for index, new_code in corrections.items():\n",
        "            if index in df.index:\n",
        "                original_code = df.loc[index, 'code']\n",
        "                # Update the 'code' column at the specified index\n",
        "                df.loc[index, 'code'] = new_code\n",
        "                print(f\"  Index {index}: Corrected '{original_code}' ---> '{new_code}'\")\n",
        "                corrected_count += 1\n",
        "            else:\n",
        "                print(f\"  Warning: Index {index} specified for correction not found in the DataFrame. Skipping.\")\n",
        "\n",
        "        if corrected_count > 0:\n",
        "            # Save the modified DataFrame to the output CSV file\n",
        "            # index=False prevents pandas from writing the DataFrame index as a column\n",
        "            df.to_csv(output_csv_filename, index=False)\n",
        "            print(f\"\\nSuccessfully applied {corrected_count} corrections.\")\n",
        "            print(f\"Corrected data saved to: {output_csv_filename}\")\n",
        "            if output_csv_filename != input_csv_filename:\n",
        "                print(f\"You can now use '{output_csv_filename}' as input for the next step.\")\n",
        "            else:\n",
        "                 print(f\"Original file '{input_csv_filename}' has been overwritten.\")\n",
        "        else:\n",
        "            print(\"\\nNo corrections were applied (perhaps indices were incorrect or already modified).\")\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: The input file '{input_csv_filename}' is empty.\")\n",
        "    except KeyError as e:\n",
        "         print(f\"Error: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        print(\"Please check the CSV file format and content.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading LLM generated formulas from: HK_final_comprehensive_result_1_corrected.csv\n",
            "Loaded 17 factor formulas.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:   0%|          | 0/83 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Processing ticker: 0001.HK ---\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:   1%|          | 1/83 [00:03<04:53,  3.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0002.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:   2%|▏         | 2/83 [00:04<02:30,  1.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0003.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:   4%|▎         | 3/83 [00:04<01:39,  1.24s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0005.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:   5%|▍         | 4/83 [00:05<01:13,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0006.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:   6%|▌         | 5/83 [00:05<00:58,  1.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0011.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:   7%|▋         | 6/83 [00:06<00:48,  1.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0012.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:   8%|▊         | 7/83 [00:06<00:45,  1.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0016.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  10%|▉         | 8/83 [00:07<00:46,  1.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0027.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  11%|█         | 9/83 [00:07<00:48,  1.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0066.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  12%|█▏        | 10/83 [00:08<00:44,  1.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0101.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  13%|█▎        | 11/83 [00:09<00:43,  1.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0175.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  14%|█▍        | 12/83 [00:09<00:41,  1.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0241.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  16%|█▌        | 13/83 [00:09<00:36,  1.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0267.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  17%|█▋        | 14/83 [00:10<00:36,  1.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0285.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  18%|█▊        | 15/83 [00:10<00:34,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0288.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  19%|█▉        | 16/83 [00:11<00:33,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0291.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  20%|██        | 17/83 [00:12<00:38,  1.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0316.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  22%|██▏       | 18/83 [00:12<00:35,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0322.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  23%|██▎       | 19/83 [00:13<00:33,  1.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0386.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  24%|██▍       | 20/83 [00:13<00:32,  1.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0388.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  25%|██▌       | 21/83 [00:14<00:34,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0669.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  27%|██▋       | 22/83 [00:14<00:33,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0688.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  28%|██▊       | 23/83 [00:15<00:32,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0700.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  29%|██▉       | 24/83 [00:15<00:31,  1.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0762.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  30%|███       | 25/83 [00:16<00:31,  1.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0823.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  31%|███▏      | 26/83 [00:17<00:31,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0836.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  33%|███▎      | 27/83 [00:17<00:30,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0857.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  34%|███▎      | 28/83 [00:18<00:29,  1.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0868.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  35%|███▍      | 29/83 [00:18<00:28,  1.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0881.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  36%|███▌      | 30/83 [00:19<00:26,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0883.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  37%|███▋      | 31/83 [00:19<00:27,  1.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0939.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  39%|███▊      | 32/83 [00:20<00:27,  1.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0941.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  40%|███▉      | 33/83 [00:20<00:27,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0960.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  41%|████      | 34/83 [00:21<00:25,  1.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0968.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  42%|████▏     | 35/83 [00:21<00:25,  1.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0981.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  43%|████▎     | 36/83 [00:22<00:22,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 0992.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  45%|████▍     | 37/83 [00:22<00:22,  2.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1024.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  46%|████▌     | 38/83 [00:27<01:19,  1.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1013, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1038.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  47%|████▋     | 39/83 [00:30<01:38,  2.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1044.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  48%|████▊     | 40/83 [00:32<01:26,  2.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1088.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  49%|████▉     | 41/83 [00:32<01:04,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1093.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  51%|█████     | 42/83 [00:33<00:49,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1099.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  52%|█████▏    | 43/83 [00:33<00:38,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1109.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  53%|█████▎    | 44/83 [00:34<00:33,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1113.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  54%|█████▍    | 45/83 [00:34<00:29,  1.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1177.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  55%|█████▌    | 46/83 [00:35<00:25,  1.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1209.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  57%|█████▋    | 47/83 [00:35<00:22,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1053, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1211.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  58%|█████▊    | 48/83 [00:36<00:21,  1.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1299.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  59%|█████▉    | 49/83 [00:36<00:19,  1.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1378.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  60%|██████    | 50/83 [00:37<00:17,  1.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1398.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  61%|██████▏   | 51/83 [00:37<00:16,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1810.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  63%|██████▎   | 52/83 [00:37<00:14,  2.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1876.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  64%|██████▍   | 53/83 [00:38<00:13,  2.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1348, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1928.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  65%|██████▌   | 54/83 [00:38<00:13,  2.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1929.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  66%|██████▋   | 55/83 [00:39<00:13,  2.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 1997.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  67%|██████▋   | 56/83 [00:39<00:13,  2.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2015.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  69%|██████▊   | 57/83 [00:40<00:12,  2.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (887, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2020.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  70%|██████▉   | 58/83 [00:40<00:12,  2.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2269.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  71%|███████   | 59/83 [00:41<00:11,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2313.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  72%|███████▏  | 60/83 [00:41<00:10,  2.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2318.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  73%|███████▎  | 61/83 [00:42<00:09,  2.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2319.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  75%|███████▍  | 62/83 [00:42<00:08,  2.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2331.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  76%|███████▌  | 63/83 [00:43<00:09,  2.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2359.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  77%|███████▋  | 64/83 [00:43<00:09,  2.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2382.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  78%|███████▊  | 65/83 [00:44<00:09,  1.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2388.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  80%|███████▉  | 66/83 [00:44<00:08,  1.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2628.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  81%|████████  | 67/83 [00:45<00:08,  1.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2688.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  82%|████████▏ | 68/83 [00:45<00:07,  1.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 2899.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  83%|████████▎ | 69/83 [00:46<00:06,  2.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 3690.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  84%|████████▍ | 70/83 [00:46<00:05,  2.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 3692.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  86%|████████▌ | 71/83 [00:46<00:04,  2.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 3968.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  87%|████████▋ | 72/83 [00:47<00:04,  2.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 3988.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  88%|████████▊ | 73/83 [00:47<00:03,  2.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 6618.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  89%|████████▉ | 74/83 [00:47<00:03,  2.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1054, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 6690.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  90%|█████████ | 75/83 [00:48<00:03,  2.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1043, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 6862.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  92%|█████████▏| 76/83 [00:49<00:03,  2.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1364, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 9618.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  93%|█████████▎| 77/83 [00:49<00:02,  2.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1171, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 9633.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  94%|█████████▍| 78/83 [00:49<00:02,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1115, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 9888.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  95%|█████████▌| 79/83 [00:50<00:01,  2.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (983, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 9901.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  96%|█████████▋| 80/83 [00:50<00:01,  2.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1075, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 9961.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  98%|█████████▊| 81/83 [00:51<00:00,  2.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (967, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 9988.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers:  99%|█████████▉| 82/83 [00:51<00:00,  2.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1309, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Processing ticker: 9999.HK ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Tickers: 100%|██████████| 83/83 [00:52<00:00,  1.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Data downloaded. Shape: (1176, 12)\n",
            "    Evaluating 17 formulas...\n",
            "\n",
            "--- Combining results across all tickers ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Combining Factors: 100%|██████████| 17/17 [00:00<00:00, 89.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Successfully calculated and combined 17 factors.\n",
            "Final DataFrame shape (stacked): (100025, 17)\n",
            "\n",
            "Saving calculated factor values to: processed_llm_alpha_data_values.csv\n",
            "Save successful.\n",
            "\n",
            "--- Dynamic Factor Calculation (Block 1 Replacement) Finished ---\n"
          ]
        }
      ],
      "source": [
        "# Block 1 REPLACEMENT: Dynamic LLM Factor Calculation\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from asteval import Interpreter # Import asteval\n",
        "import traceback # For detailed error logging\n",
        "from tqdm import tqdm # For progress bars\n",
        "\n",
        "# --- Configuration ---\n",
        "LLM_FACTORS_CSV = \"HK_final_comprehensive_result_1_corrected.csv\" # Input from Phase I\n",
        "OUTPUT_CSV = \"processed_llm_alpha_data_values.csv\" # Output for Block 2\n",
        "TICKERS_LIST = [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK',\n",
        "                  \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK',\n",
        "                  \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                  '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK',\n",
        "                  '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK'] # Your list of tickers\n",
        "END_DATE_STR = \"2025-03-24\" # Use consistent end date with analysis block\n",
        "START_DATE_STR = \"2020-03-24\" # Use consistent start date with analysis block\n",
        "LOOKBACK_BUFFER_DAYS = 200 # Extra days for rolling calculations (adjust as needed)\n",
        "\n",
        "# --- Define Calculation Functions for asteval ---\n",
        "# These functions MUST operate on pandas Series and return a pandas Series\n",
        "\n",
        "def safe_division(numerator, denominator):\n",
        "    \"\"\"Performs division, replacing division by zero or NaN denominator with NaN.\"\"\"\n",
        "    # Ensure denominator is a Series for .replace\n",
        "    if not isinstance(denominator, pd.Series):\n",
        "        denominator = pd.Series(denominator) # Convert if scalar\n",
        "    denom_safe = denominator.replace(0, np.nan)\n",
        "    return numerator / denom_safe\n",
        "\n",
        "def rolling_mean(series, window):\n",
        "    return series.rolling(window=int(window), min_periods=int(window*0.8)).mean() # Example min_periods\n",
        "\n",
        "def rolling_std(series, window):\n",
        "    return series.rolling(window=int(window), min_periods=int(window*0.8)).std()\n",
        "\n",
        "def rolling_min(series, window):\n",
        "    return series.rolling(window=int(window), min_periods=int(window*0.8)).min()\n",
        "\n",
        "def rolling_max(series, window):\n",
        "    return series.rolling(window=int(window), min_periods=int(window*0.8)).max()\n",
        "\n",
        "def ewma(series, span):\n",
        "    return series.ewm(span=int(span), adjust=False).mean()\n",
        "\n",
        "def series_delay(series, periods):\n",
        "    return series.shift(int(periods))\n",
        "\n",
        "# Use your existing RSI/ATR helpers, ensure they handle Series input/output\n",
        "def calculate_rsi(close_series, window=14):\n",
        "    window = int(window) # Ensure window is int\n",
        "    delta = close_series.diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    # Use ewm directly for rolling average\n",
        "    avg_gain = gain.ewm(com=window - 1, min_periods=window, adjust=False).mean()\n",
        "    avg_loss = loss.ewm(com=window - 1, min_periods=window, adjust=False).mean()\n",
        "    rs = safe_division(avg_gain, avg_loss) # Use safe division\n",
        "    rsi = 100.0 - (100.0 / (1.0 + rs))\n",
        "    return rsi\n",
        "\n",
        "def calculate_atr(high_series, low_series, close_series, window=14):\n",
        "    window = int(window) # Ensure window is int\n",
        "    prev_close = series_delay(close_series, 1) # Use the delay function\n",
        "    tr1 = high_series - low_series\n",
        "    tr2 = abs(high_series - prev_close)\n",
        "    tr3 = abs(low_series - prev_close)\n",
        "    # Need to handle potential different indices if series come from different sources before concat\n",
        "    # Assuming they come from the same df, indices should align\n",
        "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1, skipna=False) # Handle NaNs during max\n",
        "    atr = true_range.ewm(alpha=1.0/window, min_periods=window, adjust=False).mean() # Use ewm\n",
        "    return atr\n",
        "\n",
        "# --- Main Calculation Logic ---\n",
        "\n",
        "# 1. Load LLM Formulas\n",
        "print(f\"Loading LLM generated formulas from: {LLM_FACTORS_CSV}\")\n",
        "try:\n",
        "    llm_formulas_df = pd.read_csv(LLM_FACTORS_CSV)\n",
        "    # Basic validation\n",
        "    if not all(col in llm_formulas_df.columns for col in ['domain', 'name', 'code']):\n",
        "        raise ValueError(\"CSV missing required columns: 'domain', 'name', 'code'\")\n",
        "    # Create a unique factor identifier (e.g., combining name and maybe hash of code)\n",
        "    # Using index as proxy for now, but a more robust name is better\n",
        "    llm_formulas_df['factor_id'] = llm_formulas_df.index.astype(str) + \"_\" + llm_formulas_df['name'].str.replace(r'\\W+', '_', regex=True).str[:30] # Create a usable ID\n",
        "\n",
        "    llm_formulas = llm_formulas_df[['factor_id', 'code']].to_dict('records')\n",
        "    print(f\"Loaded {len(llm_formulas)} factor formulas.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR loading or processing {LLM_FACTORS_CSV}: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 2. Prepare Date Range and Tickers\n",
        "start_date_dt = datetime.strptime(START_DATE_STR, '%Y-%m-%d')\n",
        "end_date_dt = datetime.strptime(END_DATE_STR, '%Y-%m-%d')\n",
        "fetch_start_date = (start_date_dt - timedelta(days=LOOKBACK_BUFFER_DAYS)).strftime('%Y-%m-%d')\n",
        "fetch_end_date = (end_date_dt + timedelta(days=1)).strftime('%Y-%m-%d') # yf includes start, excludes end\n",
        "\n",
        "all_factor_results_stacked = {} # Store final results: {factor_id: Series(Index=(date, asset))}\n",
        "failed_factors = {} # Store errors: {factor_id: [error messages]}\n",
        "\n",
        "# 3. Loop through Tickers\n",
        "for ticker in tqdm(TICKERS_LIST, desc=\"Processing Tickers\"):\n",
        "    print(f\"\\n--- Processing ticker: {ticker} ---\")\n",
        "    # 3.1 Download Data\n",
        "    try:\n",
        "        df_raw = yf.download(ticker, start=fetch_start_date, end=fetch_end_date, progress=False, timeout=30)\n",
        "        if df_raw.empty:\n",
        "            print(f\"    No data downloaded for {ticker}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Rename columns for consistency (lowercase)\n",
        "        new_columns = []\n",
        "        for col in df_raw.columns:\n",
        "            if isinstance(col, tuple):\n",
        "                # If it's a tuple, assume the name is the first element\n",
        "                col_name = str(col[0]) # Convert to string for safety\n",
        "            else:\n",
        "                # Otherwise, treat it as a potential string\n",
        "                col_name = str(col) # Convert to string for safety\n",
        "            # Apply lower() and replace() to the extracted string name\n",
        "            new_columns.append(col_name.lower().replace('adj close', 'adj_close'))\n",
        "        df_raw.columns = new_columns\n",
        "        \n",
        "        if 'adj_close' not in df_raw.columns:\n",
        "            if 'close' in df_raw.columns:\n",
        "                 df_raw['adj_close'] = df_raw['close'] # Use close if adj_close missing\n",
        "            else:\n",
        "                 print(f\"    Missing 'close' or 'adj_close' for {ticker}. Skipping.\")\n",
        "                 continue\n",
        "\n",
        "        # Select and ensure required columns\n",
        "        required_cols = ['open', 'high', 'low', 'close', 'volume', 'adj_close']\n",
        "        df_ticker = df_raw[[col for col in required_cols if col in df_raw.columns]].copy()\n",
        "        if not all(col in df_ticker.columns for col in ['high', 'low', 'close', 'volume']): # Open is optional for many factors\n",
        "             print(f\"    Missing fundamental OHLCV columns for {ticker}. Skipping.\")\n",
        "             continue\n",
        "\n",
        "        # Add aliases commonly used in formulas\n",
        "        df_ticker['CLOSE'] = df_ticker['close']\n",
        "        df_ticker['HIGH'] = df_ticker['high']\n",
        "        df_ticker['LOW'] = df_ticker['low']\n",
        "        df_ticker['VOLUME'] = df_ticker['volume']\n",
        "        if 'open' in df_ticker.columns: df_ticker['OPEN'] = df_ticker['open']\n",
        "        if 'adj_close' in df_ticker.columns: df_ticker['ADJ_CLOSE'] = df_ticker['adj_close']\n",
        "\n",
        "        print(f\"    Data downloaded. Shape: {df_ticker.shape}\")\n",
        "\n",
        "    except Exception as e_yf:\n",
        "        print(f\"    Error downloading data for {ticker}: {e_yf}\")\n",
        "        continue # Skip to next ticker\n",
        "\n",
        "    # 3.2 Setup asteval Interpreter for this ticker\n",
        "    aeval = Interpreter()\n",
        "    # Add safe functions and data to the interpreter's symbol table\n",
        "    aeval.symtable['SMA'] = rolling_mean\n",
        "    aeval.symtable['EMA'] = ewma\n",
        "    aeval.symtable['STD'] = rolling_std\n",
        "    aeval.symtable['MIN'] = rolling_min\n",
        "    aeval.symtable['MAX'] = rolling_max\n",
        "    aeval.symtable['DELAY'] = series_delay\n",
        "    aeval.symtable['RSI'] = calculate_rsi\n",
        "    aeval.symtable['ATR'] = calculate_atr\n",
        "    aeval.symtable['ABS'] = np.abs\n",
        "    aeval.symtable['LOG'] = np.log\n",
        "    aeval.symtable['LOG1P'] = np.log1p\n",
        "    aeval.symtable['SQRT'] = np.sqrt\n",
        "    aeval.symtable['SIGN'] = np.sign\n",
        "    # Add the ticker's data columns\n",
        "    for col in df_ticker.columns:\n",
        "        # Use uppercase aliases if they exist, otherwise use original lowercase\n",
        "        symbol_name = col.upper() if col.upper() in ['OPEN', 'HIGH', 'LOW', 'CLOSE', 'VOLUME', 'ADJ_CLOSE'] else col\n",
        "        aeval.symtable[symbol_name] = df_ticker[col]\n",
        "    # Add numpy for potential use within formulas (use carefully)\n",
        "    aeval.symtable['np'] = np\n",
        "\n",
        "    # 3.3 Loop through LLM formulas and evaluate\n",
        "    print(f\"    Evaluating {len(llm_formulas)} formulas...\")\n",
        "    for factor_info in llm_formulas:\n",
        "        factor_id = factor_info['factor_id']\n",
        "        formula_str = factor_info['code']\n",
        "        # print(f\"      Attempting: {factor_id} = {formula_str}\") # Debug print\n",
        "\n",
        "        try:\n",
        "            # Evaluate the formula string safely\n",
        "            result_series = aeval.eval(formula_str)\n",
        "\n",
        "            # Validate result\n",
        "            if not isinstance(result_series, pd.Series):\n",
        "                raise TypeError(f\"Calculation did not return a pandas Series (returned {type(result_series)})\")\n",
        "\n",
        "            if result_series.empty:\n",
        "                 raise ValueError(\"Calculation returned an empty Series\")\n",
        "\n",
        "            if result_series.isna().all():\n",
        "                 raise ValueError(\"Calculation returned a Series with all NaNs\")\n",
        "\n",
        "            # Ensure index matches the original data's index for this ticker\n",
        "            result_series = result_series.reindex(df_ticker.index)\n",
        "\n",
        "            # --- Store Successful Result ---\n",
        "            # Filter result to the analysis period\n",
        "            result_series_filtered = result_series.loc[start_date_dt:end_date_dt]\n",
        "\n",
        "            # Add asset level to index\n",
        "            result_series_filtered.index = pd.MultiIndex.from_product(\n",
        "                [result_series_filtered.index, [ticker]],\n",
        "                names=['date', 'asset']\n",
        "            )\n",
        "\n",
        "            # Append to the combined dictionary\n",
        "            if factor_id not in all_factor_results_stacked:\n",
        "                all_factor_results_stacked[factor_id] = []\n",
        "            all_factor_results_stacked[factor_id].append(result_series_filtered)\n",
        "            # print(f\"      Success: {factor_id}\") # Debug print\n",
        "\n",
        "        except Exception as e_eval:\n",
        "            error_msg = f\"Ticker {ticker}: Failed evaluating '{formula_str}'. Error: {type(e_eval).__name__}: {e_eval}\"\n",
        "            # print(f\"      ERROR: {error_msg}\") # Debug print\n",
        "            if factor_id not in failed_factors:\n",
        "                failed_factors[factor_id] = []\n",
        "            failed_factors[factor_id].append(error_msg)\n",
        "            # Optional: Print traceback for complex errors\n",
        "            # if not isinstance(e_eval, (SyntaxError, NameError, TypeError, ValueError, KeyError)):\n",
        "            #      traceback.print_exc()\n",
        "\n",
        "\n",
        "# 4. Combine results across all tickers\n",
        "print(\"\\n--- Combining results across all tickers ---\")\n",
        "final_factors_list = []\n",
        "successfully_calculated_factors = set()\n",
        "\n",
        "for factor_id, series_list in tqdm(all_factor_results_stacked.items(), desc=\"Combining Factors\"):\n",
        "    if series_list:\n",
        "        try:\n",
        "            combined_series = pd.concat(series_list).sort_index()\n",
        "            # Optional: Drop duplicates just in case (shouldn't happen with MultiIndex)\n",
        "            combined_series = combined_series[~combined_series.index.duplicated(keep='first')]\n",
        "            combined_series.name = factor_id # Set Series name\n",
        "            final_factors_list.append(combined_series)\n",
        "            successfully_calculated_factors.add(factor_id)\n",
        "        except Exception as e_concat:\n",
        "            print(f\"ERROR concatenating results for factor {factor_id}: {e_concat}\")\n",
        "            if factor_id not in failed_factors: failed_factors[factor_id] = []\n",
        "            failed_factors[factor_id].append(f\"Concatenation Error: {e_concat}\")\n",
        "\n",
        "# 5. Create Final DataFrame (Stacked Format)\n",
        "final_stacked_df = pd.DataFrame()\n",
        "if final_factors_list:\n",
        "    try:\n",
        "        final_stacked_df = pd.concat(final_factors_list, axis=1)\n",
        "        # Ensure index names are correct\n",
        "        final_stacked_df.index.names = ['date', 'asset']\n",
        "        print(f\"\\nSuccessfully calculated and combined {len(final_stacked_df.columns)} factors.\")\n",
        "        print(f\"Final DataFrame shape (stacked): {final_stacked_df.shape}\")\n",
        "    except Exception as e_final_concat:\n",
        "        print(f\"ERROR creating final DataFrame: {e_final_concat}\")\n",
        "else:\n",
        "    print(\"\\nNo factors were successfully calculated for any ticker.\")\n",
        "\n",
        "# 6. Report Failures\n",
        "if failed_factors:\n",
        "    print(\"\\n--- Factor Evaluation Failures ---\")\n",
        "    num_failed = len(failed_factors)\n",
        "    num_successful = len(successfully_calculated_factors)\n",
        "    total_attempted = len(llm_formulas)\n",
        "    print(f\"Attempted: {total_attempted}, Succeeded: {num_successful}, Failed: {num_failed}\")\n",
        "    # Print details for a few failed factors\n",
        "    for i, (factor_id, errors) in enumerate(failed_factors.items()):\n",
        "        if i < 5 or num_failed < 10: # Print details for first 5 or all if < 10 failed\n",
        "             original_formula = next((f['code'] for f in llm_formulas if f['factor_id'] == factor_id), \"N/A\")\n",
        "             print(f\"\\nFactor: {factor_id} (Formula: {original_formula})\")\n",
        "             print(f\"  Errors ({len(errors)} tickers):\")\n",
        "             # Print first few error messages\n",
        "             for j, err in enumerate(errors):\n",
        "                  if j < 3: print(f\"    - {err}\")\n",
        "                  elif j == 3: print(f\"    - ... ({len(errors) - 3} more errors)\")\n",
        "                  else: break\n",
        "        elif i == 5:\n",
        "             print(f\"\\n... and {num_failed - 5} more failed factors.\")\n",
        "             break\n",
        "\n",
        "\n",
        "# 7. Save the results\n",
        "if not final_stacked_df.empty:\n",
        "    print(f\"\\nSaving calculated factor values to: {OUTPUT_CSV}\")\n",
        "    try:\n",
        "        # Convert datetime index level to string before saving to CSV if needed,\n",
        "        # or handle timezone removal if Block 2 expects naive datetimes.\n",
        "        # Assuming Block 2 handles timezone conversion if needed based on its code.\n",
        "        final_stacked_df.to_csv(OUTPUT_CSV)\n",
        "        print(\"Save successful.\")\n",
        "    except Exception as e_save:\n",
        "        print(f\"ERROR saving results to {OUTPUT_CSV}: {e_save}\")\n",
        "else:\n",
        "    print(\"\\nNo data to save.\")\n",
        "\n",
        "print(\"\\n--- Dynamic Factor Calculation (Block 1 Replacement) Finished ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Now you can proceed with Block 2, loading data from OUTPUT_CSV\n",
        "# Make sure Block 2's loading section points to processed_llm_alpha_data_values.csv\n",
        "# and expects the STACKED format (Index = ['date', 'asset'], Columns = factor_ids)\n",
        "# ================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>0_Price_Momentum_10_days_</th>\n",
              "      <th>1_Rate_of_Change_ROC_10_days_</th>\n",
              "      <th>2_Moving_Average_Crossover_10_vs</th>\n",
              "      <th>3_Volume_Momentum</th>\n",
              "      <th>4_Mean_Reversion_20_days_</th>\n",
              "      <th>5_Moving_Average_Reversion</th>\n",
              "      <th>6_Stochastic_Oscillator_K_14_day</th>\n",
              "      <th>7_Average_True_Range_ATR_14_day_</th>\n",
              "      <th>8_Daily_High_Low_Range</th>\n",
              "      <th>9_Normalized_Bollinger_Band_Widt</th>\n",
              "      <th>10_Volume_Rate_of_Change_VROC_10_</th>\n",
              "      <th>11_Trading_Volume</th>\n",
              "      <th>12_Moving_Average_MA_</th>\n",
              "      <th>13_Exponential_Moving_Average_MA_</th>\n",
              "      <th>14_Relative_Strength_Index_RSI_</th>\n",
              "      <th>15_Bollinger_Bands_20_day_</th>\n",
              "      <th>16_Stochastic_Oscillator_K_14_day</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th>asset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">2020-03-24</th>\n",
              "      <th>0001.HK</th>\n",
              "      <td>-0.261719</td>\n",
              "      <td>-0.261719</td>\n",
              "      <td>-11.243292</td>\n",
              "      <td>11133732.0</td>\n",
              "      <td>9.331732</td>\n",
              "      <td>9.331732</td>\n",
              "      <td>0.096280</td>\n",
              "      <td>1.825778</td>\n",
              "      <td>1.379356</td>\n",
              "      <td>0.574961</td>\n",
              "      <td>inf</td>\n",
              "      <td>15296458</td>\n",
              "      <td>45.539855</td>\n",
              "      <td>43.705973</td>\n",
              "      <td>23.699002</td>\n",
              "      <td>0.712520</td>\n",
              "      <td>0.096280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0002.HK</th>\n",
              "      <td>-0.170147</td>\n",
              "      <td>-0.170147</td>\n",
              "      <td>-3.527573</td>\n",
              "      <td>5853606.0</td>\n",
              "      <td>7.791596</td>\n",
              "      <td>7.791596</td>\n",
              "      <td>0.141588</td>\n",
              "      <td>1.835635</td>\n",
              "      <td>2.383914</td>\n",
              "      <td>0.272050</td>\n",
              "      <td>0.752496</td>\n",
              "      <td>9264804</td>\n",
              "      <td>61.469395</td>\n",
              "      <td>59.999745</td>\n",
              "      <td>27.437552</td>\n",
              "      <td>0.863975</td>\n",
              "      <td>0.141588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0003.HK</th>\n",
              "      <td>-0.183619</td>\n",
              "      <td>-0.183619</td>\n",
              "      <td>-0.660818</td>\n",
              "      <td>34027820.0</td>\n",
              "      <td>1.625504</td>\n",
              "      <td>1.625504</td>\n",
              "      <td>0.172222</td>\n",
              "      <td>0.360250</td>\n",
              "      <td>0.339107</td>\n",
              "      <td>0.254600</td>\n",
              "      <td>0.772976</td>\n",
              "      <td>48176115</td>\n",
              "      <td>10.737181</td>\n",
              "      <td>10.524233</td>\n",
              "      <td>24.209762</td>\n",
              "      <td>0.872700</td>\n",
              "      <td>0.172222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0005.HK</th>\n",
              "      <td>-0.077710</td>\n",
              "      <td>-0.077710</td>\n",
              "      <td>-7.111191</td>\n",
              "      <td>16681279.0</td>\n",
              "      <td>2.910289</td>\n",
              "      <td>2.910289</td>\n",
              "      <td>0.306929</td>\n",
              "      <td>1.263020</td>\n",
              "      <td>0.596777</td>\n",
              "      <td>0.311176</td>\n",
              "      <td>-0.483654</td>\n",
              "      <td>35692754</td>\n",
              "      <td>38.796450</td>\n",
              "      <td>38.542959</td>\n",
              "      <td>31.921737</td>\n",
              "      <td>0.844412</td>\n",
              "      <td>0.306929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0006.HK</th>\n",
              "      <td>-0.220159</td>\n",
              "      <td>-0.220159</td>\n",
              "      <td>-4.299421</td>\n",
              "      <td>3932800.0</td>\n",
              "      <td>6.136819</td>\n",
              "      <td>6.136819</td>\n",
              "      <td>0.161290</td>\n",
              "      <td>1.304744</td>\n",
              "      <td>1.422925</td>\n",
              "      <td>0.363965</td>\n",
              "      <td>0.268858</td>\n",
              "      <td>6153012</td>\n",
              "      <td>38.316811</td>\n",
              "      <td>37.216102</td>\n",
              "      <td>25.250537</td>\n",
              "      <td>0.818017</td>\n",
              "      <td>0.161290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">2025-03-24</th>\n",
              "      <th>9888.HK</th>\n",
              "      <td>0.016848</td>\n",
              "      <td>0.016848</td>\n",
              "      <td>6.512000</td>\n",
              "      <td>8914676.0</td>\n",
              "      <td>-2.932502</td>\n",
              "      <td>-2.932502</td>\n",
              "      <td>0.456757</td>\n",
              "      <td>4.110813</td>\n",
              "      <td>2.800003</td>\n",
              "      <td>0.213538</td>\n",
              "      <td>-0.427283</td>\n",
              "      <td>13278795</td>\n",
              "      <td>90.617501</td>\n",
              "      <td>91.777740</td>\n",
              "      <td>53.872340</td>\n",
              "      <td>0.893231</td>\n",
              "      <td>0.456757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9901.HK</th>\n",
              "      <td>-0.031250</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>-1.117000</td>\n",
              "      <td>15877780.0</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.211864</td>\n",
              "      <td>1.812617</td>\n",
              "      <td>3.900002</td>\n",
              "      <td>0.173413</td>\n",
              "      <td>3.489181</td>\n",
              "      <td>19056392</td>\n",
              "      <td>38.120000</td>\n",
              "      <td>39.011644</td>\n",
              "      <td>42.638680</td>\n",
              "      <td>0.913294</td>\n",
              "      <td>0.211864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9961.HK</th>\n",
              "      <td>0.035742</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>-14.091614</td>\n",
              "      <td>779344.0</td>\n",
              "      <td>-27.012465</td>\n",
              "      <td>-27.012465</td>\n",
              "      <td>0.817156</td>\n",
              "      <td>20.539906</td>\n",
              "      <td>18.799988</td>\n",
              "      <td>0.199746</td>\n",
              "      <td>-0.530138</td>\n",
              "      <td>2491259</td>\n",
              "      <td>483.487535</td>\n",
              "      <td>497.756730</td>\n",
              "      <td>53.140591</td>\n",
              "      <td>0.900127</td>\n",
              "      <td>0.817156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9988.HK</th>\n",
              "      <td>-0.012639</td>\n",
              "      <td>-0.012639</td>\n",
              "      <td>22.201001</td>\n",
              "      <td>39744597.0</td>\n",
              "      <td>1.639997</td>\n",
              "      <td>1.639997</td>\n",
              "      <td>0.321244</td>\n",
              "      <td>6.116542</td>\n",
              "      <td>3.700012</td>\n",
              "      <td>0.132679</td>\n",
              "      <td>-0.493060</td>\n",
              "      <td>90778453</td>\n",
              "      <td>134.440000</td>\n",
              "      <td>131.956694</td>\n",
              "      <td>54.166481</td>\n",
              "      <td>0.933661</td>\n",
              "      <td>0.321244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999.HK</th>\n",
              "      <td>-0.032138</td>\n",
              "      <td>-0.032138</td>\n",
              "      <td>0.935292</td>\n",
              "      <td>-1903272.0</td>\n",
              "      <td>1.912050</td>\n",
              "      <td>1.912050</td>\n",
              "      <td>0.362416</td>\n",
              "      <td>5.243960</td>\n",
              "      <td>5.699997</td>\n",
              "      <td>0.091870</td>\n",
              "      <td>-0.455264</td>\n",
              "      <td>5898773</td>\n",
              "      <td>158.512056</td>\n",
              "      <td>158.521755</td>\n",
              "      <td>47.597101</td>\n",
              "      <td>0.954065</td>\n",
              "      <td>0.362416</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100025 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    0_Price_Momentum_10_days_  1_Rate_of_Change_ROC_10_days_  \\\n",
              "date       asset                                                               \n",
              "2020-03-24 0001.HK                  -0.261719                      -0.261719   \n",
              "           0002.HK                  -0.170147                      -0.170147   \n",
              "           0003.HK                  -0.183619                      -0.183619   \n",
              "           0005.HK                  -0.077710                      -0.077710   \n",
              "           0006.HK                  -0.220159                      -0.220159   \n",
              "...                                       ...                            ...   \n",
              "2025-03-24 9888.HK                   0.016848                       0.016848   \n",
              "           9901.HK                  -0.031250                      -0.031250   \n",
              "           9961.HK                   0.035742                       0.035742   \n",
              "           9988.HK                  -0.012639                      -0.012639   \n",
              "           9999.HK                  -0.032138                      -0.032138   \n",
              "\n",
              "                    2_Moving_Average_Crossover_10_vs  3_Volume_Momentum  \\\n",
              "date       asset                                                          \n",
              "2020-03-24 0001.HK                        -11.243292         11133732.0   \n",
              "           0002.HK                         -3.527573          5853606.0   \n",
              "           0003.HK                         -0.660818         34027820.0   \n",
              "           0005.HK                         -7.111191         16681279.0   \n",
              "           0006.HK                         -4.299421          3932800.0   \n",
              "...                                              ...                ...   \n",
              "2025-03-24 9888.HK                          6.512000          8914676.0   \n",
              "           9901.HK                         -1.117000         15877780.0   \n",
              "           9961.HK                        -14.091614           779344.0   \n",
              "           9988.HK                         22.201001         39744597.0   \n",
              "           9999.HK                          0.935292         -1903272.0   \n",
              "\n",
              "                    4_Mean_Reversion_20_days_  5_Moving_Average_Reversion  \\\n",
              "date       asset                                                            \n",
              "2020-03-24 0001.HK                   9.331732                    9.331732   \n",
              "           0002.HK                   7.791596                    7.791596   \n",
              "           0003.HK                   1.625504                    1.625504   \n",
              "           0005.HK                   2.910289                    2.910289   \n",
              "           0006.HK                   6.136819                    6.136819   \n",
              "...                                       ...                         ...   \n",
              "2025-03-24 9888.HK                  -2.932502                   -2.932502   \n",
              "           9901.HK                   0.920000                    0.920000   \n",
              "           9961.HK                 -27.012465                  -27.012465   \n",
              "           9988.HK                   1.639997                    1.639997   \n",
              "           9999.HK                   1.912050                    1.912050   \n",
              "\n",
              "                    6_Stochastic_Oscillator_K_14_day  \\\n",
              "date       asset                                       \n",
              "2020-03-24 0001.HK                          0.096280   \n",
              "           0002.HK                          0.141588   \n",
              "           0003.HK                          0.172222   \n",
              "           0005.HK                          0.306929   \n",
              "           0006.HK                          0.161290   \n",
              "...                                              ...   \n",
              "2025-03-24 9888.HK                          0.456757   \n",
              "           9901.HK                          0.211864   \n",
              "           9961.HK                          0.817156   \n",
              "           9988.HK                          0.321244   \n",
              "           9999.HK                          0.362416   \n",
              "\n",
              "                    7_Average_True_Range_ATR_14_day_  8_Daily_High_Low_Range  \\\n",
              "date       asset                                                               \n",
              "2020-03-24 0001.HK                          1.825778                1.379356   \n",
              "           0002.HK                          1.835635                2.383914   \n",
              "           0003.HK                          0.360250                0.339107   \n",
              "           0005.HK                          1.263020                0.596777   \n",
              "           0006.HK                          1.304744                1.422925   \n",
              "...                                              ...                     ...   \n",
              "2025-03-24 9888.HK                          4.110813                2.800003   \n",
              "           9901.HK                          1.812617                3.900002   \n",
              "           9961.HK                         20.539906               18.799988   \n",
              "           9988.HK                          6.116542                3.700012   \n",
              "           9999.HK                          5.243960                5.699997   \n",
              "\n",
              "                    9_Normalized_Bollinger_Band_Widt  \\\n",
              "date       asset                                       \n",
              "2020-03-24 0001.HK                          0.574961   \n",
              "           0002.HK                          0.272050   \n",
              "           0003.HK                          0.254600   \n",
              "           0005.HK                          0.311176   \n",
              "           0006.HK                          0.363965   \n",
              "...                                              ...   \n",
              "2025-03-24 9888.HK                          0.213538   \n",
              "           9901.HK                          0.173413   \n",
              "           9961.HK                          0.199746   \n",
              "           9988.HK                          0.132679   \n",
              "           9999.HK                          0.091870   \n",
              "\n",
              "                    10_Volume_Rate_of_Change_VROC_10_  11_Trading_Volume  \\\n",
              "date       asset                                                           \n",
              "2020-03-24 0001.HK                                inf           15296458   \n",
              "           0002.HK                           0.752496            9264804   \n",
              "           0003.HK                           0.772976           48176115   \n",
              "           0005.HK                          -0.483654           35692754   \n",
              "           0006.HK                           0.268858            6153012   \n",
              "...                                               ...                ...   \n",
              "2025-03-24 9888.HK                          -0.427283           13278795   \n",
              "           9901.HK                           3.489181           19056392   \n",
              "           9961.HK                          -0.530138            2491259   \n",
              "           9988.HK                          -0.493060           90778453   \n",
              "           9999.HK                          -0.455264            5898773   \n",
              "\n",
              "                    12_Moving_Average_MA_  13_Exponential_Moving_Average_MA_  \\\n",
              "date       asset                                                               \n",
              "2020-03-24 0001.HK              45.539855                          43.705973   \n",
              "           0002.HK              61.469395                          59.999745   \n",
              "           0003.HK              10.737181                          10.524233   \n",
              "           0005.HK              38.796450                          38.542959   \n",
              "           0006.HK              38.316811                          37.216102   \n",
              "...                                   ...                                ...   \n",
              "2025-03-24 9888.HK              90.617501                          91.777740   \n",
              "           9901.HK              38.120000                          39.011644   \n",
              "           9961.HK             483.487535                         497.756730   \n",
              "           9988.HK             134.440000                         131.956694   \n",
              "           9999.HK             158.512056                         158.521755   \n",
              "\n",
              "                    14_Relative_Strength_Index_RSI_  \\\n",
              "date       asset                                      \n",
              "2020-03-24 0001.HK                        23.699002   \n",
              "           0002.HK                        27.437552   \n",
              "           0003.HK                        24.209762   \n",
              "           0005.HK                        31.921737   \n",
              "           0006.HK                        25.250537   \n",
              "...                                             ...   \n",
              "2025-03-24 9888.HK                        53.872340   \n",
              "           9901.HK                        42.638680   \n",
              "           9961.HK                        53.140591   \n",
              "           9988.HK                        54.166481   \n",
              "           9999.HK                        47.597101   \n",
              "\n",
              "                    15_Bollinger_Bands_20_day_  \\\n",
              "date       asset                                 \n",
              "2020-03-24 0001.HK                    0.712520   \n",
              "           0002.HK                    0.863975   \n",
              "           0003.HK                    0.872700   \n",
              "           0005.HK                    0.844412   \n",
              "           0006.HK                    0.818017   \n",
              "...                                        ...   \n",
              "2025-03-24 9888.HK                    0.893231   \n",
              "           9901.HK                    0.913294   \n",
              "           9961.HK                    0.900127   \n",
              "           9988.HK                    0.933661   \n",
              "           9999.HK                    0.954065   \n",
              "\n",
              "                    16_Stochastic_Oscillator_K_14_day  \n",
              "date       asset                                       \n",
              "2020-03-24 0001.HK                           0.096280  \n",
              "           0002.HK                           0.141588  \n",
              "           0003.HK                           0.172222  \n",
              "           0005.HK                           0.306929  \n",
              "           0006.HK                           0.161290  \n",
              "...                                               ...  \n",
              "2025-03-24 9888.HK                           0.456757  \n",
              "           9901.HK                           0.211864  \n",
              "           9961.HK                           0.817156  \n",
              "           9988.HK                           0.321244  \n",
              "           9999.HK                           0.362416  \n",
              "\n",
              "[100025 rows x 17 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_stacked_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hk_tickers = [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', \n",
        "                  \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', \n",
        "                  \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                  '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', \n",
        "                  '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK']\n",
        "len(hk_tickers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Alpha factor calculation and importing relevant library for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Processing ticker: 0001.HK ---\n",
            "YF.download() has changed argument auto_adjust default to True\n",
            "    Raw columns structure for 0001.HK: MultiIndex([( 'Close', '0001.HK'),\n",
            "            (  'High', '0001.HK'),\n",
            "            (   'Low', '0001.HK'),\n",
            "            (  'Open', '0001.HK'),\n",
            "            ('Volume', '0001.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0001.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0002.HK ---\n",
            "    Raw columns structure for 0002.HK: MultiIndex([( 'Close', '0002.HK'),\n",
            "            (  'High', '0002.HK'),\n",
            "            (   'Low', '0002.HK'),\n",
            "            (  'Open', '0002.HK'),\n",
            "            ('Volume', '0002.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0002.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0003.HK ---\n",
            "    Raw columns structure for 0003.HK: MultiIndex([( 'Close', '0003.HK'),\n",
            "            (  'High', '0003.HK'),\n",
            "            (   'Low', '0003.HK'),\n",
            "            (  'Open', '0003.HK'),\n",
            "            ('Volume', '0003.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0003.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0005.HK ---\n",
            "    Raw columns structure for 0005.HK: MultiIndex([( 'Close', '0005.HK'),\n",
            "            (  'High', '0005.HK'),\n",
            "            (   'Low', '0005.HK'),\n",
            "            (  'Open', '0005.HK'),\n",
            "            ('Volume', '0005.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0005.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0006.HK ---\n",
            "    Raw columns structure for 0006.HK: MultiIndex([( 'Close', '0006.HK'),\n",
            "            (  'High', '0006.HK'),\n",
            "            (   'Low', '0006.HK'),\n",
            "            (  'Open', '0006.HK'),\n",
            "            ('Volume', '0006.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0006.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0011.HK ---\n",
            "    Raw columns structure for 0011.HK: MultiIndex([( 'Close', '0011.HK'),\n",
            "            (  'High', '0011.HK'),\n",
            "            (   'Low', '0011.HK'),\n",
            "            (  'Open', '0011.HK'),\n",
            "            ('Volume', '0011.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0011.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0012.HK ---\n",
            "    Raw columns structure for 0012.HK: MultiIndex([( 'Close', '0012.HK'),\n",
            "            (  'High', '0012.HK'),\n",
            "            (   'Low', '0012.HK'),\n",
            "            (  'Open', '0012.HK'),\n",
            "            ('Volume', '0012.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0012.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0016.HK ---\n",
            "    Raw columns structure for 0016.HK: MultiIndex([( 'Close', '0016.HK'),\n",
            "            (  'High', '0016.HK'),\n",
            "            (   'Low', '0016.HK'),\n",
            "            (  'Open', '0016.HK'),\n",
            "            ('Volume', '0016.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0016.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0027.HK ---\n",
            "    Raw columns structure for 0027.HK: MultiIndex([( 'Close', '0027.HK'),\n",
            "            (  'High', '0027.HK'),\n",
            "            (   'Low', '0027.HK'),\n",
            "            (  'Open', '0027.HK'),\n",
            "            ('Volume', '0027.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0027.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0066.HK ---\n",
            "    Raw columns structure for 0066.HK: MultiIndex([( 'Close', '0066.HK'),\n",
            "            (  'High', '0066.HK'),\n",
            "            (   'Low', '0066.HK'),\n",
            "            (  'Open', '0066.HK'),\n",
            "            ('Volume', '0066.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0066.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0101.HK ---\n",
            "    Raw columns structure for 0101.HK: MultiIndex([( 'Close', '0101.HK'),\n",
            "            (  'High', '0101.HK'),\n",
            "            (   'Low', '0101.HK'),\n",
            "            (  'Open', '0101.HK'),\n",
            "            ('Volume', '0101.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0101.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0175.HK ---\n",
            "    Raw columns structure for 0175.HK: MultiIndex([( 'Close', '0175.HK'),\n",
            "            (  'High', '0175.HK'),\n",
            "            (   'Low', '0175.HK'),\n",
            "            (  'Open', '0175.HK'),\n",
            "            ('Volume', '0175.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0175.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0241.HK ---\n",
            "    Raw columns structure for 0241.HK: MultiIndex([( 'Close', '0241.HK'),\n",
            "            (  'High', '0241.HK'),\n",
            "            (   'Low', '0241.HK'),\n",
            "            (  'Open', '0241.HK'),\n",
            "            ('Volume', '0241.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0241.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0267.HK ---\n",
            "    Raw columns structure for 0267.HK: MultiIndex([( 'Close', '0267.HK'),\n",
            "            (  'High', '0267.HK'),\n",
            "            (   'Low', '0267.HK'),\n",
            "            (  'Open', '0267.HK'),\n",
            "            ('Volume', '0267.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0267.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0285.HK ---\n",
            "    Raw columns structure for 0285.HK: MultiIndex([( 'Close', '0285.HK'),\n",
            "            (  'High', '0285.HK'),\n",
            "            (   'Low', '0285.HK'),\n",
            "            (  'Open', '0285.HK'),\n",
            "            ('Volume', '0285.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0285.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0288.HK ---\n",
            "    Raw columns structure for 0288.HK: MultiIndex([( 'Close', '0288.HK'),\n",
            "            (  'High', '0288.HK'),\n",
            "            (   'Low', '0288.HK'),\n",
            "            (  'Open', '0288.HK'),\n",
            "            ('Volume', '0288.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0288.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0291.HK ---\n",
            "    Raw columns structure for 0291.HK: MultiIndex([( 'Close', '0291.HK'),\n",
            "            (  'High', '0291.HK'),\n",
            "            (   'Low', '0291.HK'),\n",
            "            (  'Open', '0291.HK'),\n",
            "            ('Volume', '0291.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0291.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0316.HK ---\n",
            "    Raw columns structure for 0316.HK: MultiIndex([( 'Close', '0316.HK'),\n",
            "            (  'High', '0316.HK'),\n",
            "            (   'Low', '0316.HK'),\n",
            "            (  'Open', '0316.HK'),\n",
            "            ('Volume', '0316.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0316.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0322.HK ---\n",
            "    Raw columns structure for 0322.HK: MultiIndex([( 'Close', '0322.HK'),\n",
            "            (  'High', '0322.HK'),\n",
            "            (   'Low', '0322.HK'),\n",
            "            (  'Open', '0322.HK'),\n",
            "            ('Volume', '0322.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0322.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0386.HK ---\n",
            "    Raw columns structure for 0386.HK: MultiIndex([( 'Close', '0386.HK'),\n",
            "            (  'High', '0386.HK'),\n",
            "            (   'Low', '0386.HK'),\n",
            "            (  'Open', '0386.HK'),\n",
            "            ('Volume', '0386.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0386.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0388.HK ---\n",
            "    Raw columns structure for 0388.HK: MultiIndex([( 'Close', '0388.HK'),\n",
            "            (  'High', '0388.HK'),\n",
            "            (   'Low', '0388.HK'),\n",
            "            (  'Open', '0388.HK'),\n",
            "            ('Volume', '0388.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0388.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0669.HK ---\n",
            "    Raw columns structure for 0669.HK: MultiIndex([( 'Close', '0669.HK'),\n",
            "            (  'High', '0669.HK'),\n",
            "            (   'Low', '0669.HK'),\n",
            "            (  'Open', '0669.HK'),\n",
            "            ('Volume', '0669.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0669.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0688.HK ---\n",
            "    Raw columns structure for 0688.HK: MultiIndex([( 'Close', '0688.HK'),\n",
            "            (  'High', '0688.HK'),\n",
            "            (   'Low', '0688.HK'),\n",
            "            (  'Open', '0688.HK'),\n",
            "            ('Volume', '0688.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0688.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0700.HK ---\n",
            "    Raw columns structure for 0700.HK: MultiIndex([( 'Close', '0700.HK'),\n",
            "            (  'High', '0700.HK'),\n",
            "            (   'Low', '0700.HK'),\n",
            "            (  'Open', '0700.HK'),\n",
            "            ('Volume', '0700.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0700.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0762.HK ---\n",
            "    Raw columns structure for 0762.HK: MultiIndex([( 'Close', '0762.HK'),\n",
            "            (  'High', '0762.HK'),\n",
            "            (   'Low', '0762.HK'),\n",
            "            (  'Open', '0762.HK'),\n",
            "            ('Volume', '0762.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0762.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0823.HK ---\n",
            "    Raw columns structure for 0823.HK: MultiIndex([( 'Close', '0823.HK'),\n",
            "            (  'High', '0823.HK'),\n",
            "            (   'Low', '0823.HK'),\n",
            "            (  'Open', '0823.HK'),\n",
            "            ('Volume', '0823.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0823.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0836.HK ---\n",
            "    Raw columns structure for 0836.HK: MultiIndex([( 'Close', '0836.HK'),\n",
            "            (  'High', '0836.HK'),\n",
            "            (   'Low', '0836.HK'),\n",
            "            (  'Open', '0836.HK'),\n",
            "            ('Volume', '0836.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0836.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0857.HK ---\n",
            "    Raw columns structure for 0857.HK: MultiIndex([( 'Close', '0857.HK'),\n",
            "            (  'High', '0857.HK'),\n",
            "            (   'Low', '0857.HK'),\n",
            "            (  'Open', '0857.HK'),\n",
            "            ('Volume', '0857.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0857.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0868.HK ---\n",
            "    Raw columns structure for 0868.HK: MultiIndex([( 'Close', '0868.HK'),\n",
            "            (  'High', '0868.HK'),\n",
            "            (   'Low', '0868.HK'),\n",
            "            (  'Open', '0868.HK'),\n",
            "            ('Volume', '0868.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0868.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0881.HK ---\n",
            "    Raw columns structure for 0881.HK: MultiIndex([( 'Close', '0881.HK'),\n",
            "            (  'High', '0881.HK'),\n",
            "            (   'Low', '0881.HK'),\n",
            "            (  'Open', '0881.HK'),\n",
            "            ('Volume', '0881.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0881.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0883.HK ---\n",
            "    Raw columns structure for 0883.HK: MultiIndex([( 'Close', '0883.HK'),\n",
            "            (  'High', '0883.HK'),\n",
            "            (   'Low', '0883.HK'),\n",
            "            (  'Open', '0883.HK'),\n",
            "            ('Volume', '0883.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0883.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0939.HK ---\n",
            "    Raw columns structure for 0939.HK: MultiIndex([( 'Close', '0939.HK'),\n",
            "            (  'High', '0939.HK'),\n",
            "            (   'Low', '0939.HK'),\n",
            "            (  'Open', '0939.HK'),\n",
            "            ('Volume', '0939.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0939.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0941.HK ---\n",
            "    Raw columns structure for 0941.HK: MultiIndex([( 'Close', '0941.HK'),\n",
            "            (  'High', '0941.HK'),\n",
            "            (   'Low', '0941.HK'),\n",
            "            (  'Open', '0941.HK'),\n",
            "            ('Volume', '0941.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0941.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0960.HK ---\n",
            "    Raw columns structure for 0960.HK: MultiIndex([( 'Close', '0960.HK'),\n",
            "            (  'High', '0960.HK'),\n",
            "            (   'Low', '0960.HK'),\n",
            "            (  'Open', '0960.HK'),\n",
            "            ('Volume', '0960.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0960.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0968.HK ---\n",
            "    Raw columns structure for 0968.HK: MultiIndex([( 'Close', '0968.HK'),\n",
            "            (  'High', '0968.HK'),\n",
            "            (   'Low', '0968.HK'),\n",
            "            (  'Open', '0968.HK'),\n",
            "            ('Volume', '0968.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0968.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0981.HK ---\n",
            "    Raw columns structure for 0981.HK: MultiIndex([( 'Close', '0981.HK'),\n",
            "            (  'High', '0981.HK'),\n",
            "            (   'Low', '0981.HK'),\n",
            "            (  'Open', '0981.HK'),\n",
            "            ('Volume', '0981.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0981.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 0992.HK ---\n",
            "    Raw columns structure for 0992.HK: MultiIndex([( 'Close', '0992.HK'),\n",
            "            (  'High', '0992.HK'),\n",
            "            (   'Low', '0992.HK'),\n",
            "            (  'Open', '0992.HK'),\n",
            "            ('Volume', '0992.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 0992.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1024.HK ---\n",
            "    Raw columns structure for 1024.HK: MultiIndex([( 'Close', '1024.HK'),\n",
            "            (  'High', '1024.HK'),\n",
            "            (   'Low', '1024.HK'),\n",
            "            (  'Open', '1024.HK'),\n",
            "            ('Volume', '1024.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1024.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1038.HK ---\n",
            "    Raw columns structure for 1038.HK: MultiIndex([( 'Close', '1038.HK'),\n",
            "            (  'High', '1038.HK'),\n",
            "            (   'Low', '1038.HK'),\n",
            "            (  'Open', '1038.HK'),\n",
            "            ('Volume', '1038.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1038.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1044.HK ---\n",
            "    Raw columns structure for 1044.HK: MultiIndex([( 'Close', '1044.HK'),\n",
            "            (  'High', '1044.HK'),\n",
            "            (   'Low', '1044.HK'),\n",
            "            (  'Open', '1044.HK'),\n",
            "            ('Volume', '1044.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1044.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1088.HK ---\n",
            "    Raw columns structure for 1088.HK: MultiIndex([( 'Close', '1088.HK'),\n",
            "            (  'High', '1088.HK'),\n",
            "            (   'Low', '1088.HK'),\n",
            "            (  'Open', '1088.HK'),\n",
            "            ('Volume', '1088.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1088.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1093.HK ---\n",
            "    Raw columns structure for 1093.HK: MultiIndex([( 'Close', '1093.HK'),\n",
            "            (  'High', '1093.HK'),\n",
            "            (   'Low', '1093.HK'),\n",
            "            (  'Open', '1093.HK'),\n",
            "            ('Volume', '1093.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1093.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1099.HK ---\n",
            "    Raw columns structure for 1099.HK: MultiIndex([( 'Close', '1099.HK'),\n",
            "            (  'High', '1099.HK'),\n",
            "            (   'Low', '1099.HK'),\n",
            "            (  'Open', '1099.HK'),\n",
            "            ('Volume', '1099.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1099.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1109.HK ---\n",
            "    Raw columns structure for 1109.HK: MultiIndex([( 'Close', '1109.HK'),\n",
            "            (  'High', '1109.HK'),\n",
            "            (   'Low', '1109.HK'),\n",
            "            (  'Open', '1109.HK'),\n",
            "            ('Volume', '1109.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1109.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1113.HK ---\n",
            "    Raw columns structure for 1113.HK: MultiIndex([( 'Close', '1113.HK'),\n",
            "            (  'High', '1113.HK'),\n",
            "            (   'Low', '1113.HK'),\n",
            "            (  'Open', '1113.HK'),\n",
            "            ('Volume', '1113.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1113.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1177.HK ---\n",
            "    Raw columns structure for 1177.HK: MultiIndex([( 'Close', '1177.HK'),\n",
            "            (  'High', '1177.HK'),\n",
            "            (   'Low', '1177.HK'),\n",
            "            (  'Open', '1177.HK'),\n",
            "            ('Volume', '1177.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1177.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1209.HK ---\n",
            "    Raw columns structure for 1209.HK: MultiIndex([( 'Close', '1209.HK'),\n",
            "            (  'High', '1209.HK'),\n",
            "            (   'Low', '1209.HK'),\n",
            "            (  'Open', '1209.HK'),\n",
            "            ('Volume', '1209.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1209.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1211.HK ---\n",
            "    Raw columns structure for 1211.HK: MultiIndex([( 'Close', '1211.HK'),\n",
            "            (  'High', '1211.HK'),\n",
            "            (   'Low', '1211.HK'),\n",
            "            (  'Open', '1211.HK'),\n",
            "            ('Volume', '1211.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1211.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1299.HK ---\n",
            "    Raw columns structure for 1299.HK: MultiIndex([( 'Close', '1299.HK'),\n",
            "            (  'High', '1299.HK'),\n",
            "            (   'Low', '1299.HK'),\n",
            "            (  'Open', '1299.HK'),\n",
            "            ('Volume', '1299.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1299.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1378.HK ---\n",
            "    Raw columns structure for 1378.HK: MultiIndex([( 'Close', '1378.HK'),\n",
            "            (  'High', '1378.HK'),\n",
            "            (   'Low', '1378.HK'),\n",
            "            (  'Open', '1378.HK'),\n",
            "            ('Volume', '1378.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1378.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1398.HK ---\n",
            "    Raw columns structure for 1398.HK: MultiIndex([( 'Close', '1398.HK'),\n",
            "            (  'High', '1398.HK'),\n",
            "            (   'Low', '1398.HK'),\n",
            "            (  'Open', '1398.HK'),\n",
            "            ('Volume', '1398.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1398.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1810.HK ---\n",
            "    Raw columns structure for 1810.HK: MultiIndex([( 'Close', '1810.HK'),\n",
            "            (  'High', '1810.HK'),\n",
            "            (   'Low', '1810.HK'),\n",
            "            (  'Open', '1810.HK'),\n",
            "            ('Volume', '1810.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1810.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1876.HK ---\n",
            "    Raw columns structure for 1876.HK: MultiIndex([( 'Close', '1876.HK'),\n",
            "            (  'High', '1876.HK'),\n",
            "            (   'Low', '1876.HK'),\n",
            "            (  'Open', '1876.HK'),\n",
            "            ('Volume', '1876.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1876.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1928.HK ---\n",
            "    Raw columns structure for 1928.HK: MultiIndex([( 'Close', '1928.HK'),\n",
            "            (  'High', '1928.HK'),\n",
            "            (   'Low', '1928.HK'),\n",
            "            (  'Open', '1928.HK'),\n",
            "            ('Volume', '1928.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1928.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1929.HK ---\n",
            "    Raw columns structure for 1929.HK: MultiIndex([( 'Close', '1929.HK'),\n",
            "            (  'High', '1929.HK'),\n",
            "            (   'Low', '1929.HK'),\n",
            "            (  'Open', '1929.HK'),\n",
            "            ('Volume', '1929.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1929.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 1997.HK ---\n",
            "    Raw columns structure for 1997.HK: MultiIndex([( 'Close', '1997.HK'),\n",
            "            (  'High', '1997.HK'),\n",
            "            (   'Low', '1997.HK'),\n",
            "            (  'Open', '1997.HK'),\n",
            "            ('Volume', '1997.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 1997.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2015.HK ---\n",
            "    Raw columns structure for 2015.HK: MultiIndex([( 'Close', '2015.HK'),\n",
            "            (  'High', '2015.HK'),\n",
            "            (   'Low', '2015.HK'),\n",
            "            (  'Open', '2015.HK'),\n",
            "            ('Volume', '2015.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2015.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2020.HK ---\n",
            "    Raw columns structure for 2020.HK: MultiIndex([( 'Close', '2020.HK'),\n",
            "            (  'High', '2020.HK'),\n",
            "            (   'Low', '2020.HK'),\n",
            "            (  'Open', '2020.HK'),\n",
            "            ('Volume', '2020.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2020.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2269.HK ---\n",
            "    Raw columns structure for 2269.HK: MultiIndex([( 'Close', '2269.HK'),\n",
            "            (  'High', '2269.HK'),\n",
            "            (   'Low', '2269.HK'),\n",
            "            (  'Open', '2269.HK'),\n",
            "            ('Volume', '2269.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2269.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2313.HK ---\n",
            "    Raw columns structure for 2313.HK: MultiIndex([( 'Close', '2313.HK'),\n",
            "            (  'High', '2313.HK'),\n",
            "            (   'Low', '2313.HK'),\n",
            "            (  'Open', '2313.HK'),\n",
            "            ('Volume', '2313.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2313.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2318.HK ---\n",
            "    Raw columns structure for 2318.HK: MultiIndex([( 'Close', '2318.HK'),\n",
            "            (  'High', '2318.HK'),\n",
            "            (   'Low', '2318.HK'),\n",
            "            (  'Open', '2318.HK'),\n",
            "            ('Volume', '2318.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2318.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2319.HK ---\n",
            "    Raw columns structure for 2319.HK: MultiIndex([( 'Close', '2319.HK'),\n",
            "            (  'High', '2319.HK'),\n",
            "            (   'Low', '2319.HK'),\n",
            "            (  'Open', '2319.HK'),\n",
            "            ('Volume', '2319.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2319.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2331.HK ---\n",
            "    Raw columns structure for 2331.HK: MultiIndex([( 'Close', '2331.HK'),\n",
            "            (  'High', '2331.HK'),\n",
            "            (   'Low', '2331.HK'),\n",
            "            (  'Open', '2331.HK'),\n",
            "            ('Volume', '2331.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2331.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2359.HK ---\n",
            "    Raw columns structure for 2359.HK: MultiIndex([( 'Close', '2359.HK'),\n",
            "            (  'High', '2359.HK'),\n",
            "            (   'Low', '2359.HK'),\n",
            "            (  'Open', '2359.HK'),\n",
            "            ('Volume', '2359.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2359.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2382.HK ---\n",
            "    Raw columns structure for 2382.HK: MultiIndex([( 'Close', '2382.HK'),\n",
            "            (  'High', '2382.HK'),\n",
            "            (   'Low', '2382.HK'),\n",
            "            (  'Open', '2382.HK'),\n",
            "            ('Volume', '2382.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2382.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2388.HK ---\n",
            "    Raw columns structure for 2388.HK: MultiIndex([( 'Close', '2388.HK'),\n",
            "            (  'High', '2388.HK'),\n",
            "            (   'Low', '2388.HK'),\n",
            "            (  'Open', '2388.HK'),\n",
            "            ('Volume', '2388.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2388.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2628.HK ---\n",
            "    Raw columns structure for 2628.HK: MultiIndex([( 'Close', '2628.HK'),\n",
            "            (  'High', '2628.HK'),\n",
            "            (   'Low', '2628.HK'),\n",
            "            (  'Open', '2628.HK'),\n",
            "            ('Volume', '2628.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2628.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2688.HK ---\n",
            "    Raw columns structure for 2688.HK: MultiIndex([( 'Close', '2688.HK'),\n",
            "            (  'High', '2688.HK'),\n",
            "            (   'Low', '2688.HK'),\n",
            "            (  'Open', '2688.HK'),\n",
            "            ('Volume', '2688.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2688.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 2899.HK ---\n",
            "    Raw columns structure for 2899.HK: MultiIndex([( 'Close', '2899.HK'),\n",
            "            (  'High', '2899.HK'),\n",
            "            (   'Low', '2899.HK'),\n",
            "            (  'Open', '2899.HK'),\n",
            "            ('Volume', '2899.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 2899.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 3690.HK ---\n",
            "    Raw columns structure for 3690.HK: MultiIndex([( 'Close', '3690.HK'),\n",
            "            (  'High', '3690.HK'),\n",
            "            (   'Low', '3690.HK'),\n",
            "            (  'Open', '3690.HK'),\n",
            "            ('Volume', '3690.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 3690.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 3692.HK ---\n",
            "    Raw columns structure for 3692.HK: MultiIndex([( 'Close', '3692.HK'),\n",
            "            (  'High', '3692.HK'),\n",
            "            (   'Low', '3692.HK'),\n",
            "            (  'Open', '3692.HK'),\n",
            "            ('Volume', '3692.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 3692.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 3968.HK ---\n",
            "    Raw columns structure for 3968.HK: MultiIndex([( 'Close', '3968.HK'),\n",
            "            (  'High', '3968.HK'),\n",
            "            (   'Low', '3968.HK'),\n",
            "            (  'Open', '3968.HK'),\n",
            "            ('Volume', '3968.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 3968.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 3988.HK ---\n",
            "    Raw columns structure for 3988.HK: MultiIndex([( 'Close', '3988.HK'),\n",
            "            (  'High', '3988.HK'),\n",
            "            (   'Low', '3988.HK'),\n",
            "            (  'Open', '3988.HK'),\n",
            "            ('Volume', '3988.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 3988.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 6618.HK ---\n",
            "    Raw columns structure for 6618.HK: MultiIndex([( 'Close', '6618.HK'),\n",
            "            (  'High', '6618.HK'),\n",
            "            (   'Low', '6618.HK'),\n",
            "            (  'Open', '6618.HK'),\n",
            "            ('Volume', '6618.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 6618.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 6690.HK ---\n",
            "    Raw columns structure for 6690.HK: MultiIndex([( 'Close', '6690.HK'),\n",
            "            (  'High', '6690.HK'),\n",
            "            (   'Low', '6690.HK'),\n",
            "            (  'Open', '6690.HK'),\n",
            "            ('Volume', '6690.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 6690.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 6862.HK ---\n",
            "    Raw columns structure for 6862.HK: MultiIndex([( 'Close', '6862.HK'),\n",
            "            (  'High', '6862.HK'),\n",
            "            (   'Low', '6862.HK'),\n",
            "            (  'Open', '6862.HK'),\n",
            "            ('Volume', '6862.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 6862.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 9618.HK ---\n",
            "    Raw columns structure for 9618.HK: MultiIndex([( 'Close', '9618.HK'),\n",
            "            (  'High', '9618.HK'),\n",
            "            (   'Low', '9618.HK'),\n",
            "            (  'Open', '9618.HK'),\n",
            "            ('Volume', '9618.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 9618.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 9633.HK ---\n",
            "    Raw columns structure for 9633.HK: MultiIndex([( 'Close', '9633.HK'),\n",
            "            (  'High', '9633.HK'),\n",
            "            (   'Low', '9633.HK'),\n",
            "            (  'Open', '9633.HK'),\n",
            "            ('Volume', '9633.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 9633.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 9888.HK ---\n",
            "    Raw columns structure for 9888.HK: MultiIndex([( 'Close', '9888.HK'),\n",
            "            (  'High', '9888.HK'),\n",
            "            (   'Low', '9888.HK'),\n",
            "            (  'Open', '9888.HK'),\n",
            "            ('Volume', '9888.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 9888.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 9901.HK ---\n",
            "    Raw columns structure for 9901.HK: MultiIndex([( 'Close', '9901.HK'),\n",
            "            (  'High', '9901.HK'),\n",
            "            (   'Low', '9901.HK'),\n",
            "            (  'Open', '9901.HK'),\n",
            "            ('Volume', '9901.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 9901.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 9961.HK ---\n",
            "    Raw columns structure for 9961.HK: MultiIndex([( 'Close', '9961.HK'),\n",
            "            (  'High', '9961.HK'),\n",
            "            (   'Low', '9961.HK'),\n",
            "            (  'Open', '9961.HK'),\n",
            "            ('Volume', '9961.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 9961.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 9988.HK ---\n",
            "    Raw columns structure for 9988.HK: MultiIndex([( 'Close', '9988.HK'),\n",
            "            (  'High', '9988.HK'),\n",
            "            (   'Low', '9988.HK'),\n",
            "            (  'Open', '9988.HK'),\n",
            "            ('Volume', '9988.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 9988.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "--- Processing ticker: 9999.HK ---\n",
            "    Raw columns structure for 9999.HK: MultiIndex([( 'Close', '9999.HK'),\n",
            "            (  'High', '9999.HK'),\n",
            "            (   'Low', '9999.HK'),\n",
            "            (  'Open', '9999.HK'),\n",
            "            ('Volume', '9999.HK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "    Detected MultiIndex columns for 9999.HK. Attempting to simplify.\n",
            "    Simplified columns: Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object', name='Price')\n",
            "\n",
            "--- Final Combined Alpha Factors DataFrame ---\n",
            "        Date stock_id  Price_Momentum_10D  Rate_of_Change_10D  MA_Crossover_10_50  Volume_Momentum_50D  \\\n",
            "0 2020-03-24  0001.HK                 NaN                 NaN                 NaN                  NaN   \n",
            "1 2020-03-25  0001.HK                 NaN                 NaN                 NaN                  NaN   \n",
            "2 2020-03-26  0001.HK                 NaN                 NaN                 NaN                  NaN   \n",
            "3 2020-03-27  0001.HK                 NaN                 NaN                 NaN                  NaN   \n",
            "4 2020-03-30  0001.HK                 NaN                 NaN                 NaN                  NaN   \n",
            "\n",
            "   Mean_Reversion_20D  Moving_Avg_Reversion_20D  Stochastic_Oscillator_K_14D  ATR_14D  Daily_High_Low_Range  \\\n",
            "0                 NaN                       NaN                          NaN      NaN              1.379356   \n",
            "1                 NaN                       NaN                          NaN      NaN              1.034517   \n",
            "2                 NaN                       NaN                          NaN      NaN              1.685881   \n",
            "3                 NaN                       NaN                          NaN      NaN              1.417674   \n",
            "4                 NaN                       NaN                          NaN      NaN              1.455989   \n",
            "\n",
            "   Norm_Bollinger_Width_20D  Volume_ROC_10D  Trading_Volume  Moving_Average_20D  Exponential_MA_20D  RSI_14D  \\\n",
            "0                       NaN             NaN        15296458                 NaN           36.208118      NaN   \n",
            "1                       NaN             NaN        16640222                 NaN           36.408819      NaN   \n",
            "2                       NaN             NaN        12067696                 NaN           36.601352      NaN   \n",
            "3                       NaN             NaN        18221966                 NaN           36.976249      NaN   \n",
            "4                       NaN             NaN        15936885                 NaN           37.209618      NaN   \n",
            "\n",
            "   Bollinger_Ratio_LB_20D  Stochastic_Oscillator_D_14D  \n",
            "0                     NaN                          NaN  \n",
            "1                     NaN                          NaN  \n",
            "2                     NaN                          NaN  \n",
            "3                     NaN                          NaN  \n",
            "4                     NaN                          NaN  \n",
            "\n",
            "...\n",
            "             Date stock_id  Price_Momentum_10D  Rate_of_Change_10D  MA_Crossover_10_50  Volume_Momentum_50D  \\\n",
            "100020 2025-03-18  9999.HK            0.021415            0.021415            4.432549            2009660.0   \n",
            "100021 2025-03-19  9999.HK            0.013924            0.013924            4.174289             -24156.0   \n",
            "100022 2025-03-20  9999.HK           -0.046257           -0.046257            3.022003            3315930.0   \n",
            "100023 2025-03-21  9999.HK           -0.060025           -0.060025            1.759711            1515005.0   \n",
            "100024 2025-03-24  9999.HK           -0.032138           -0.032138            0.935292           -1903272.0   \n",
            "\n",
            "        Mean_Reversion_20D  Moving_Avg_Reversion_20D  Stochastic_Oscillator_K_14D   ATR_14D  Daily_High_Low_Range  \\\n",
            "100020           -0.407722                 -0.407722                    61.023137  5.256375              2.800003   \n",
            "100021           -1.210040                 -1.210040                    65.157025  5.238063              5.000000   \n",
            "100022            2.232294                  2.232294                    44.487496  5.178202              4.400009   \n",
            "100023            6.639780                  6.639780                    16.140695  5.201187              4.900009   \n",
            "100024            1.912050                  1.912050                    36.241650  5.243960              5.699997   \n",
            "\n",
            "        Norm_Bollinger_Width_20D  Volume_ROC_10D  Trading_Volume  Moving_Average_20D  Exponential_MA_20D    RSI_14D  \\\n",
            "100020                  0.083457       -0.094425         6361971          159.092278          159.681671  50.669154   \n",
            "100021                  0.081710        0.200284         6040521          158.989957          159.731035  51.706898   \n",
            "100022                  0.082528       -0.261694         5602934          158.932291          159.442365  46.445822   \n",
            "100023                  0.091520       -0.344584         7816314          158.539774          158.724044  40.378045   \n",
            "100024                  0.091870       -0.455264         5898773          158.512056          158.521755  47.597101   \n",
            "\n",
            "        Bollinger_Ratio_LB_20D  Stochastic_Oscillator_D_14D  \n",
            "100020                0.958271                    61.023137  \n",
            "100021                0.959145                    65.157025  \n",
            "100022                0.958736                    44.487496  \n",
            "100023                0.954240                    16.140695  \n",
            "100024                0.954065                    36.241650  \n",
            "\n",
            "DataFrame Shape: (100025, 19)\n",
            "\n",
            "Full results saved to manual_alpha_factors_5years.csv\n"
          ]
        }
      ],
      "source": [
        "# HK_final_comprehensive_result_1.csv\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- Helper Functions (keep as is) ---\n",
        "def calculate_rsi(close_series, window=14):\n",
        "    delta = close_series.diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.ewm(com=window - 1, min_periods=window, adjust=False).mean()\n",
        "    avg_loss = loss.ewm(com=window - 1, min_periods=window, adjust=False).mean()\n",
        "    rs = avg_gain / avg_loss.replace(0, np.nan)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "def calculate_atr(high_series, low_series, close_series, window=14):\n",
        "    prev_close = close_series.shift(1)\n",
        "    tr1 = high_series - low_series\n",
        "    tr2 = abs(high_series - prev_close)\n",
        "    tr3 = abs(low_series - prev_close)\n",
        "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    atr = true_range.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
        "    return atr\n",
        "\n",
        "\n",
        "# --- Main Function to Calculate All Alpha Factors (REVISED FOR COLUMN INDEXING) ---\n",
        "def calculate_all_factors_for_ticker(ticker, start_date, end_date_inclusive):\n",
        "    \"\"\"\n",
        "    Downloads data and calculates all specified alpha factors for a single ticker.\n",
        "    Handles potential MultiIndex columns from yfinance.\n",
        "    \"\"\"\n",
        "    print(f\"--- Processing ticker: {ticker} ---\")\n",
        "    #buffer_days = 75\n",
        "    #start_buffer = (pd.to_datetime(start_date) - timedelta(days=buffer_days)).strftime('%Y-%m-%d')\n",
        "    \n",
        "    # Calculate the end_date for yfinance download (exclusive)\n",
        "    yf_end_date = end_date_inclusive + timedelta(days=1)\n",
        "\n",
        "    try:\n",
        "        # Download data\n",
        "        df_raw = yf.download(ticker, start=start_date, end=yf_end_date, progress=False)\n",
        "        if df_raw.empty:\n",
        "            print(f\"No data downloaded for {ticker}. Skipping.\")\n",
        "            return None\n",
        "\n",
        "        # --- DEBUG: Inspect Column Structure ---\n",
        "        print(f\"    Raw columns structure for {ticker}: {df_raw.columns}\")\n",
        "        # --- END DEBUG ---\n",
        "\n",
        "        # --- Handle Potential MultiIndex Columns ---\n",
        "        df = df_raw.copy() # Work on a copy\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            print(f\"    Detected MultiIndex columns for {ticker}. Attempting to simplify.\")\n",
        "            # Flatten MultiIndex by taking the first level (usually OHLCV)\n",
        "            # Adjust this logic if the structure is different (e.g., ('High', ''))\n",
        "            try:\n",
        "                df.columns = df.columns.get_level_values(0)\n",
        "                 # Check for duplicate columns after flattening (e.g., if multiple levels existed)\n",
        "                if df.columns.has_duplicates:\n",
        "                    print(f\"    Warning: Duplicate columns found after flattening MultiIndex for {ticker}. Keeping first occurrence.\")\n",
        "                    df = df.loc[:, ~df.columns.duplicated(keep='first')] # Keep first instance\n",
        "                print(f\"    Simplified columns: {df.columns}\")\n",
        "            except Exception as mi_err:\n",
        "                print(f\"    Error simplifying MultiIndex columns for {ticker}: {mi_err}. Skipping.\")\n",
        "                return None\n",
        "\n",
        "        # --- Ensure standard columns exist after potential flattening ---\n",
        "        required_cols = ['High', 'Low', 'Close', 'Volume']\n",
        "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"    Missing required columns after processing for {ticker}: {missing_cols}. Available: {df.columns}. Skipping.\")\n",
        "            return None\n",
        "\n",
        "        # --- Now access columns, they should be Series ---\n",
        "        # (The check from the previous version is less critical now, but can be kept as safety)\n",
        "        for col in required_cols:\n",
        "            if not isinstance(df[col], pd.Series):\n",
        "                # This really shouldn't happen after the MultiIndex handling\n",
        "                print(f\"!!! CRITICAL WARNING: Column '{col}' is STILL not a Series for {ticker} after processing. Type: {type(df[col])} !!!\")\n",
        "                return None # Skip if data structure is fundamentally wrong\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data download or column processing for {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    # --- 1. Calculate Base Indicators (using the processed `df`) ---\n",
        "    # (Calculations remain the same, but now using the df with simplified columns)\n",
        "    base_indicators = {}\n",
        "    try:\n",
        "        base_indicators['Close_Shift_10'] = df['Close'].shift(10)\n",
        "        base_indicators['Volume_Shift_10'] = df['Volume'].shift(10)\n",
        "        base_indicators['Volume_Shift_50'] = df['Volume'].shift(50)\n",
        "        base_indicators['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
        "        base_indicators['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
        "        base_indicators['SMA_50'] = df['Close'].rolling(window=50).mean()\n",
        "        base_indicators['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "        base_indicators['STD_20'] = df['Close'].rolling(window=20).std()\n",
        "        base_indicators['MIN_14_LOW'] = df['Low'].rolling(window=14).min()\n",
        "        base_indicators['MAX_14_HIGH'] = df['High'].rolling(window=14).max()\n",
        "        base_indicators['RSI_14'] = calculate_rsi(df['Close'], window=14)\n",
        "        base_indicators['ATR_14'] = calculate_atr(df['High'], df['Low'], df['Close'], window=14)\n",
        "    except Exception as base_calc_err:\n",
        "         print(f\"   Error calculating base indicators for {ticker}: {base_calc_err}. Skipping.\")\n",
        "         return None\n",
        "\n",
        "    # --- Combine original df and base indicators for easier reference ---\n",
        "    # Use only the necessary columns from the processed df\n",
        "    calc_df = pd.concat([df[['High', 'Low', 'Close', 'Volume']], pd.DataFrame(base_indicators)], axis=1)\n",
        "\n",
        "\n",
        "    # --- 2. Calculate Alpha Factors and Validate Type (keep calculate_and_validate helper) ---\n",
        "    factor_cols = {}\n",
        "\n",
        "    def calculate_and_validate(factor_name, calculation_lambda):\n",
        "        \"\"\"Calculates a factor and ensures it's a Series.\"\"\"\n",
        "        try:\n",
        "            result = calculation_lambda()\n",
        "            if isinstance(result, pd.Series):\n",
        "                factor_cols[factor_name] = result\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"  *** Type Issue Calculating {factor_name} for {ticker}! Expected Series, got {type(result)} ***\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"  *** Error Calculating {factor_name} for {ticker}: {type(e).__name__}: {e} ***\")\n",
        "            return False\n",
        "\n",
        "    # (Calculations using calculate_and_validate remain the same as previous version)\n",
        "    calculate_and_validate('Price_Momentum_10D', lambda: (calc_df['Close'] - calc_df['Close_Shift_10']) / calc_df['Close_Shift_10'].replace(0, np.nan))\n",
        "    if 'Price_Momentum_10D' in factor_cols: calculate_and_validate('Rate_of_Change_10D', lambda: factor_cols['Price_Momentum_10D'])\n",
        "    calculate_and_validate('MA_Crossover_10_50', lambda: calc_df['SMA_10'] - calc_df['SMA_50'])\n",
        "    calculate_and_validate('Volume_Momentum_50D', lambda: calc_df['Volume'] - calc_df['Volume_Shift_50'])\n",
        "    calculate_and_validate('Mean_Reversion_20D', lambda: calc_df['SMA_20'] - calc_df['Close'])\n",
        "    if 'Mean_Reversion_20D' in factor_cols: calculate_and_validate('Moving_Avg_Reversion_20D', lambda: factor_cols['Mean_Reversion_20D'])\n",
        "    calculate_and_validate('Stochastic_Oscillator_K_14D', lambda: ((calc_df['Close'] - calc_df['MIN_14_LOW']) / (calc_df['MAX_14_HIGH'] - calc_df['MIN_14_LOW']).replace(0, np.nan)) * 100)\n",
        "    calculate_and_validate('ATR_14D', lambda: calc_df['ATR_14'])\n",
        "    calculate_and_validate('Daily_High_Low_Range', lambda: calc_df['High'] - calc_df['Low'])\n",
        "    calculate_and_validate('Norm_Bollinger_Width_20D', lambda: (4 * calc_df['STD_20']) / calc_df['SMA_20'].replace(0, np.nan))\n",
        "    calculate_and_validate('Volume_ROC_10D', lambda: (calc_df['Volume'] / calc_df['Volume_Shift_10'].replace(0, np.nan)) - 1)\n",
        "    calculate_and_validate('Trading_Volume', lambda: calc_df['Volume'])\n",
        "    calculate_and_validate('Moving_Average_20D', lambda: calc_df['SMA_20'])\n",
        "    calculate_and_validate('Exponential_MA_20D', lambda: calc_df['EMA_20'])\n",
        "    calculate_and_validate('RSI_14D', lambda: calc_df['RSI_14'])\n",
        "    calculate_and_validate('Bollinger_Ratio_LB_20D', lambda: (calc_df['SMA_20'] - 2 * calc_df['STD_20']) / calc_df['SMA_20'].replace(0, np.nan))\n",
        "    if 'Stochastic_Oscillator_K_14D' in factor_cols: calculate_and_validate('Stochastic_Oscillator_D_14D', lambda: factor_cols['Stochastic_Oscillator_K_14D'])\n",
        "\n",
        "\n",
        "    # --- 3. Combine *Validated* Factors into DataFrame ---\n",
        "    # (This section remains the same as previous version)\n",
        "    if not factor_cols:\n",
        "        print(f\"No factors could be successfully calculated as Series for {ticker}. Skipping DataFrame creation.\")\n",
        "        return None\n",
        "    try:\n",
        "        factors_df = pd.DataFrame(factor_cols)\n",
        "        factors_df['stock_id'] = ticker\n",
        "        factors_df = factors_df.loc[start_date:]\n",
        "    except ValueError as ve:\n",
        "         print(f\"!!! ERROR creating final DataFrame for {ticker} even after validation: {ve} !!!\")\n",
        "         return None\n",
        "    except Exception as final_e:\n",
        "         print(f\"!!! UNEXPECTED ERROR creating final DataFrame for {ticker}: {final_e} !!!\")\n",
        "         return None\n",
        "\n",
        "    return factors_df\n",
        "\n",
        "# --- Main Execution (keep as is) ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Define tickers and time period\n",
        "    hk_tickers = [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', \n",
        "                  \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', \n",
        "                  \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                  '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', \n",
        "                  '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK']\n",
        "\n",
        "    end_date = datetime.strptime(\"2025-03-24\",'%Y-%m-%d')\n",
        "    start_date = datetime.strptime(\"2020-03-24\",'%Y-%m-%d')\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for ticker_id in hk_tickers:\n",
        "        ticker_factors = calculate_all_factors_for_ticker(ticker_id, start_date, end_date)\n",
        "        if ticker_factors is not None:\n",
        "            all_results.append(ticker_factors)\n",
        "\n",
        "    if all_results:\n",
        "        final_df = pd.concat(all_results)\n",
        "        final_df = final_df.reset_index()\n",
        "\n",
        "        # Make sure 'Date' column is correct type after reset_index\n",
        "        if 'Date' not in final_df.columns and 'index' in final_df.columns:\n",
        "             final_df = final_df.rename(columns={'index': 'Date'})\n",
        "        final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
        "\n",
        "\n",
        "        cols = ['Date', 'stock_id'] + [col for col in final_df.columns if col not in ['Date', 'stock_id']]\n",
        "        final_df = final_df[cols]\n",
        "\n",
        "        print(\"\\n--- Final Combined Alpha Factors DataFrame ---\")\n",
        "        pd.set_option('display.max_rows', 100)\n",
        "        pd.set_option('display.max_columns', 20)\n",
        "        pd.set_option('display.width', 120)\n",
        "        print(final_df.head())\n",
        "        print(\"\\n...\")\n",
        "        print(final_df.tail())\n",
        "        print(f\"\\nDataFrame Shape: {final_df.shape}\")\n",
        "\n",
        "        try:\n",
        "            output_csv = \"manual_alpha_factors_5years.csv\"\n",
        "            final_df.to_csv(output_csv, index=False)\n",
        "            print(f\"\\nFull results saved to {output_csv}\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError saving results to CSV: {e}\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\nNo results were generated for any ticker.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df.to_csv(\"alpha_data_values.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (3.9.2)\n",
            "Requirement already satisfied: statsmodels in /opt/anaconda3/lib/python3.12/site-packages (0.14.2)\n",
            "Requirement already satisfied: yfinance in /opt/anaconda3/lib/python3.12/site-packages (0.2.55)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from statsmodels) (0.5.6)\n",
            "Requirement already satisfied: requests>=2.31 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (2.4.2)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/anaconda3/lib/python3.12/site-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scipy matplotlib statsmodels yfinance tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas_market_calendars in /opt/anaconda3/lib/python3.12/site-packages (5.1.0)\n",
            "Requirement already satisfied: pandas>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.2.2)\n",
            "Requirement already satisfied: tzdata in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2023.3)\n",
            "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (2.9.0.post0)\n",
            "Requirement already satisfied: exchange-calendars>=3.3 in /opt/anaconda3/lib/python3.12/site-packages (from pandas_market_calendars) (4.10)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (1.26.4)\n",
            "Requirement already satisfied: pyluach in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.2.0)\n",
            "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.0)\n",
            "Requirement already satisfied: korean_lunar_calendar in /opt/anaconda3/lib/python3.12/site-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.1->pandas_market_calendars) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->pandas_market_calendars) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_market_calendars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.12/site-packages (8.1.6)\n",
            "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (8.27.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.14 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (3.0.14)\n",
            "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.12/site-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /opt/anaconda3/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# REAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Price_Momentum_10D',\n",
              " 'Rate_of_Change_10D',\n",
              " 'MA_Crossover_10_50',\n",
              " 'Volume_Momentum_50D',\n",
              " 'Mean_Reversion_20D',\n",
              " 'Moving_Avg_Reversion_20D',\n",
              " 'Stochastic_Oscillator_K_14D',\n",
              " 'ATR_14D',\n",
              " 'Daily_High_Low_Range',\n",
              " 'Norm_Bollinger_Width_20D',\n",
              " 'Volume_ROC_10D',\n",
              " 'Trading_Volume',\n",
              " 'Moving_Average_20D',\n",
              " 'Exponential_MA_20D',\n",
              " 'RSI_14D',\n",
              " 'Bollinger_Ratio_LB_20D',\n",
              " 'Stochastic_Oscillator_D_14D']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alpha_factors_df = pd.read_csv('alpha_data_values.csv')\n",
        "alpha_factor_names = alpha_factors_df.drop(columns=['Date', 'stock_id'], errors='ignore').columns[1:].to_list()\n",
        "alpha_factor_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change it to MultiIndex\n",
        "alpha_factors_df = pd.read_csv('alpha_data_values.csv')\n",
        "alpha_factor_names = alpha_factors_df.drop(columns=['Date', 'stock_id'], errors='ignore').columns[1:].to_list()\n",
        "alpha_factors_df = alpha_factors_df.rename(columns={'Date': 'date', 'stock_id': 'asset'})\n",
        "alpha_factors = pd.DataFrame()\n",
        "\n",
        "alpha_data = {}\n",
        "for factor_name in alpha_factor_names:\n",
        "    \"\"\"\n",
        "    pivot_df = alpha_factors_df.pivot_table(index='date', columns='asset', values=factor_name)\n",
        "    pivot_df = pivot_df.reindex(dates)  # Reindex to ensure all dates are present\n",
        "    \n",
        "    # Create the MultiIndex and stack the pivoted DataFrame\n",
        "    multi_index = pd.MultiIndex.from_product([pivot_df.index, pivot_df.columns], names=['date', 'asset'])\n",
        "    stacked_series = pivot_df.stack().reindex(multi_index)\n",
        "    \n",
        "    alpha_factors[factor_name] = stacked_series\"\n",
        "    \"\"\"\n",
        "    pivot_df = alpha_factors_df.pivot_table(index='date', columns='asset', values=factor_name)\n",
        "\n",
        "    #pivot_df = pivot_df.reindex(dates)\n",
        "    #print(pivot_df)\n",
        "    #break\n",
        "\n",
        "    multi_index = pd.MultiIndex.from_product([pivot_df.index, pivot_df.columns], names=['date', 'asset'])\n",
        "\n",
        "    stacked_series = pivot_df.stack().reindex(multi_index)\n",
        "\n",
        "    alpha_factors[factor_name] = stacked_series\n",
        "\n",
        "# alpha_factors # Print the first few rows to verify the result.\n",
        "\n",
        "alpha_factors.to_csv(\"processed_alpha_data_values.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Price_Momentum_10D</th>\n",
              "      <th>Rate_of_Change_10D</th>\n",
              "      <th>MA_Crossover_10_50</th>\n",
              "      <th>Volume_Momentum_50D</th>\n",
              "      <th>Mean_Reversion_20D</th>\n",
              "      <th>Moving_Avg_Reversion_20D</th>\n",
              "      <th>Stochastic_Oscillator_K_14D</th>\n",
              "      <th>ATR_14D</th>\n",
              "      <th>Daily_High_Low_Range</th>\n",
              "      <th>Norm_Bollinger_Width_20D</th>\n",
              "      <th>Volume_ROC_10D</th>\n",
              "      <th>Trading_Volume</th>\n",
              "      <th>Moving_Average_20D</th>\n",
              "      <th>Exponential_MA_20D</th>\n",
              "      <th>RSI_14D</th>\n",
              "      <th>Bollinger_Ratio_LB_20D</th>\n",
              "      <th>Stochastic_Oscillator_D_14D</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th>asset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">2020-04-07</th>\n",
              "      <th>0001.HK</th>\n",
              "      <td>0.219048</td>\n",
              "      <td>0.219048</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.341042</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.111247</td>\n",
              "      <td>13594770.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>39.130322</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0002.HK</th>\n",
              "      <td>0.181347</td>\n",
              "      <td>0.181347</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.708473</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.289064</td>\n",
              "      <td>6586685.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>56.771402</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0003.HK</th>\n",
              "      <td>0.122977</td>\n",
              "      <td>0.122977</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.235901</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.303770</td>\n",
              "      <td>33541644.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9.416128</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0005.HK</th>\n",
              "      <td>-0.109756</td>\n",
              "      <td>-0.109756</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.861280</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.074696</td>\n",
              "      <td>74051605.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33.489280</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0006.HK</th>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.985104</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.170215</td>\n",
              "      <td>5105675.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33.616435</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"5\" valign=\"top\">2025-03-24</th>\n",
              "      <th>9888.HK</th>\n",
              "      <td>0.016848</td>\n",
              "      <td>0.016848</td>\n",
              "      <td>6.512000</td>\n",
              "      <td>8914676.0</td>\n",
              "      <td>-2.932502</td>\n",
              "      <td>-2.932502</td>\n",
              "      <td>45.675700</td>\n",
              "      <td>4.110813</td>\n",
              "      <td>2.800003</td>\n",
              "      <td>0.213538</td>\n",
              "      <td>-0.427283</td>\n",
              "      <td>13278795.0</td>\n",
              "      <td>90.617501</td>\n",
              "      <td>91.777740</td>\n",
              "      <td>53.872340</td>\n",
              "      <td>0.893231</td>\n",
              "      <td>45.675700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9901.HK</th>\n",
              "      <td>-0.031250</td>\n",
              "      <td>-0.031250</td>\n",
              "      <td>-1.117000</td>\n",
              "      <td>15877780.0</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>21.186449</td>\n",
              "      <td>1.812617</td>\n",
              "      <td>3.900002</td>\n",
              "      <td>0.173413</td>\n",
              "      <td>3.489181</td>\n",
              "      <td>19056392.0</td>\n",
              "      <td>38.120000</td>\n",
              "      <td>39.011644</td>\n",
              "      <td>42.638680</td>\n",
              "      <td>0.913294</td>\n",
              "      <td>21.186449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9961.HK</th>\n",
              "      <td>0.035742</td>\n",
              "      <td>0.035742</td>\n",
              "      <td>-14.091614</td>\n",
              "      <td>779344.0</td>\n",
              "      <td>-27.012465</td>\n",
              "      <td>-27.012465</td>\n",
              "      <td>81.715599</td>\n",
              "      <td>20.539906</td>\n",
              "      <td>18.799988</td>\n",
              "      <td>0.199746</td>\n",
              "      <td>-0.530138</td>\n",
              "      <td>2491259.0</td>\n",
              "      <td>483.487535</td>\n",
              "      <td>497.756730</td>\n",
              "      <td>53.140591</td>\n",
              "      <td>0.900127</td>\n",
              "      <td>81.715599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9988.HK</th>\n",
              "      <td>-0.012639</td>\n",
              "      <td>-0.012639</td>\n",
              "      <td>22.201001</td>\n",
              "      <td>39744597.0</td>\n",
              "      <td>1.639997</td>\n",
              "      <td>1.639997</td>\n",
              "      <td>32.124384</td>\n",
              "      <td>6.116542</td>\n",
              "      <td>3.700012</td>\n",
              "      <td>0.132679</td>\n",
              "      <td>-0.493060</td>\n",
              "      <td>90778453.0</td>\n",
              "      <td>134.440000</td>\n",
              "      <td>131.956694</td>\n",
              "      <td>54.166481</td>\n",
              "      <td>0.933661</td>\n",
              "      <td>32.124384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999.HK</th>\n",
              "      <td>-0.032138</td>\n",
              "      <td>-0.032138</td>\n",
              "      <td>0.935292</td>\n",
              "      <td>-1903272.0</td>\n",
              "      <td>1.912050</td>\n",
              "      <td>1.912050</td>\n",
              "      <td>36.241650</td>\n",
              "      <td>5.243960</td>\n",
              "      <td>5.699997</td>\n",
              "      <td>0.091870</td>\n",
              "      <td>-0.455264</td>\n",
              "      <td>5898773.0</td>\n",
              "      <td>158.512056</td>\n",
              "      <td>158.521755</td>\n",
              "      <td>47.597101</td>\n",
              "      <td>0.954065</td>\n",
              "      <td>36.241650</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101177 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Price_Momentum_10D  Rate_of_Change_10D  MA_Crossover_10_50  Volume_Momentum_50D  \\\n",
              "date       asset                                                                                      \n",
              "2020-04-07 0001.HK            0.219048            0.219048                 NaN                  NaN   \n",
              "           0002.HK            0.181347            0.181347                 NaN                  NaN   \n",
              "           0003.HK            0.122977            0.122977                 NaN                  NaN   \n",
              "           0005.HK           -0.109756           -0.109756                 NaN                  NaN   \n",
              "           0006.HK            0.142857            0.142857                 NaN                  NaN   \n",
              "...                                ...                 ...                 ...                  ...   \n",
              "2025-03-24 9888.HK            0.016848            0.016848            6.512000            8914676.0   \n",
              "           9901.HK           -0.031250           -0.031250           -1.117000           15877780.0   \n",
              "           9961.HK            0.035742            0.035742          -14.091614             779344.0   \n",
              "           9988.HK           -0.012639           -0.012639           22.201001           39744597.0   \n",
              "           9999.HK           -0.032138           -0.032138            0.935292           -1903272.0   \n",
              "\n",
              "                    Mean_Reversion_20D  Moving_Avg_Reversion_20D  Stochastic_Oscillator_K_14D    ATR_14D  \\\n",
              "date       asset                                                                                           \n",
              "2020-04-07 0001.HK                 NaN                       NaN                          NaN        NaN   \n",
              "           0002.HK                 NaN                       NaN                          NaN        NaN   \n",
              "           0003.HK                 NaN                       NaN                          NaN        NaN   \n",
              "           0005.HK                 NaN                       NaN                          NaN        NaN   \n",
              "           0006.HK                 NaN                       NaN                          NaN        NaN   \n",
              "...                                ...                       ...                          ...        ...   \n",
              "2025-03-24 9888.HK           -2.932502                 -2.932502                    45.675700   4.110813   \n",
              "           9901.HK            0.920000                  0.920000                    21.186449   1.812617   \n",
              "           9961.HK          -27.012465                -27.012465                    81.715599  20.539906   \n",
              "           9988.HK            1.639997                  1.639997                    32.124384   6.116542   \n",
              "           9999.HK            1.912050                  1.912050                    36.241650   5.243960   \n",
              "\n",
              "                    Daily_High_Low_Range  Norm_Bollinger_Width_20D  Volume_ROC_10D  Trading_Volume  \\\n",
              "date       asset                                                                                     \n",
              "2020-04-07 0001.HK              1.341042                       NaN       -0.111247      13594770.0   \n",
              "           0002.HK              1.708473                       NaN       -0.289064       6586685.0   \n",
              "           0003.HK              0.235901                       NaN       -0.303770      33541644.0   \n",
              "           0005.HK              0.861280                       NaN        1.074696      74051605.0   \n",
              "           0006.HK              0.985104                       NaN       -0.170215       5105675.0   \n",
              "...                                  ...                       ...             ...             ...   \n",
              "2025-03-24 9888.HK              2.800003                  0.213538       -0.427283      13278795.0   \n",
              "           9901.HK              3.900002                  0.173413        3.489181      19056392.0   \n",
              "           9961.HK             18.799988                  0.199746       -0.530138       2491259.0   \n",
              "           9988.HK              3.700012                  0.132679       -0.493060      90778453.0   \n",
              "           9999.HK              5.699997                  0.091870       -0.455264       5898773.0   \n",
              "\n",
              "                    Moving_Average_20D  Exponential_MA_20D    RSI_14D  Bollinger_Ratio_LB_20D  \\\n",
              "date       asset                                                                                \n",
              "2020-04-07 0001.HK                 NaN           39.130322        NaN                     NaN   \n",
              "           0002.HK                 NaN           56.771402        NaN                     NaN   \n",
              "           0003.HK                 NaN            9.416128        NaN                     NaN   \n",
              "           0005.HK                 NaN           33.489280        NaN                     NaN   \n",
              "           0006.HK                 NaN           33.616435        NaN                     NaN   \n",
              "...                                ...                 ...        ...                     ...   \n",
              "2025-03-24 9888.HK           90.617501           91.777740  53.872340                0.893231   \n",
              "           9901.HK           38.120000           39.011644  42.638680                0.913294   \n",
              "           9961.HK          483.487535          497.756730  53.140591                0.900127   \n",
              "           9988.HK          134.440000          131.956694  54.166481                0.933661   \n",
              "           9999.HK          158.512056          158.521755  47.597101                0.954065   \n",
              "\n",
              "                    Stochastic_Oscillator_D_14D  \n",
              "date       asset                                 \n",
              "2020-04-07 0001.HK                          NaN  \n",
              "           0002.HK                          NaN  \n",
              "           0003.HK                          NaN  \n",
              "           0005.HK                          NaN  \n",
              "           0006.HK                          NaN  \n",
              "...                                         ...  \n",
              "2025-03-24 9888.HK                    45.675700  \n",
              "           9901.HK                    21.186449  \n",
              "           9961.HK                    81.715599  \n",
              "           9988.HK                    32.124384  \n",
              "           9999.HK                    36.241650  \n",
              "\n",
              "[101177 rows x 17 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "alpha_factors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Start date localized to UTC: 2020-03-24 00:00:00+00:00\n",
            "INFO: End date localized to UTC: 2025-03-24 00:00:00+00:00\n",
            "\n",
            "--- Attempting to define universe based on index: ^HSI ---\n",
            "Attempting to get constituents for ^HSI on 2020-03-24.\n",
            "INFO: Using asset universe (Count: 83): ['0001.HK', '0002.HK', '0003.HK', '0005.HK', '0006.HK', '0011.HK', '0012.HK', '0016.HK', '0027.HK', '0066.HK']...\n",
            "Using pandas_market_calendars for HK business days. Full fetch range index length: 1319\n",
            "Target Analysis Date Range: 2020-03-24 00:00:00+00:00 to 2025-03-24 00:00:00+00:00 (1231 analysis days)\n",
            "\n",
            "--- Downloading Price and Total Volume Data ---\n",
            "Fetching data from 2019-12-16 to 2025-04-29 for 83 assets + benchmark ^HSI...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  83 of 83 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Asset price/volume data processed. Shape: (1319, 83)\n",
            "Benchmark data processed. Length: 1319\n",
            "\n",
            "--- Fetching Industry Classification Data ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching Industries: 100%|██████████| 83/83 [01:21<00:00,  1.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created Static Industry Dummies shape: (83, 50)\n",
            "\n",
            "--- Loading/Defining Pre-calculated Factors ---\n",
            "Attempting to load factors from: processed_alpha_data_values.csv\n",
            "Successfully loaded factors from file. Initial shape: (101177, 17)\n",
            "INFO: Loaded data appears to be in Format 2 (Stacked). Unstacking...\n",
            "Successfully unstacked factors to Format 1 (Wide).\n",
            "INFO: Localizing factor index timezone...\n",
            "INFO: Converting factor index timezone...\n",
            "Reindexing loaded factors to match analysis dates (1231) and assets (83)...\n",
            "Final precalculated factors DataFrame ready. Shape: (1231, 1411)\n",
            "Available factors: ['Price_Momentum_10D', 'Rate_of_Change_10D', 'MA_Crossover_10_50', 'Volume_Momentum_50D', 'Mean_Reversion_20D', 'Moving_Avg_Reversion_20D', 'Stochastic_Oscillator_K_14D', 'ATR_14D', 'Daily_High_Low_Range', 'Norm_Bollinger_Width_20D', 'Volume_ROC_10D', 'Trading_Volume', 'Moving_Average_20D', 'Exponential_MA_20D', 'RSI_14D', 'Bollinger_Ratio_LB_20D', 'Stochastic_Oscillator_D_14D']\n",
            "\n",
            "--- Calculating Style Factors (Beta, Size Proxy, Liquidity Proxy, Residual Volatility) ---\n",
            "Calculating style factors using 60-day lookback...\n",
            " - Calculated Size/Liquidity Proxy.\n",
            " - Calculating Beta (this may take a while)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - Calculated Beta.\n",
            " - Calculated Residual Volatility.\n",
            "Style factors calculation finished. Final Shape: (102173, 3)\n",
            "\n",
            "--- Defining/Importing Analysis Functions ---\n",
            "\n",
            "Calculating forward returns for analysis periods: (1, 3, 5) days...\n",
            "\n",
            "Calculating forward returns for IC decay (up to 20 days)...\n",
            "Calculating fwd returns for decay (1 to 20 days)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                   \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished calculating 20 forward returns for decay.\n",
            "\n",
            "Preparing single Excel output file: factor_analysis_output_combined/combined_factor_analysis_results.xlsx\n",
            "\n",
            "Analyzing 17 factors found in the input DataFrame...\n",
            "\n",
            "\n",
            "==================== Processing Factor: Price_Momentum_10D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Price_Momentum_10D. (13.26s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Price_Momentum_10D' for analysis.\n",
            "Clean aligned data ready for Price_Momentum__Raw. Shape: (98780, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Price_Momentum__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Price_Momentum__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Price_Momentum__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.15s)\n",
            "\n",
            "--- Saving results for Price_Momentum__Raw to Excel ---\n",
            "--- Results for Price_Momentum__Raw saved (9 sheets). (0.27s)---\n",
            "--- Factor Price_Momentum_10D processing time: 28.68s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Rate_of_Change_10D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Rate_of_Change_10D. (11.99s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Rate_of_Change_10D' for analysis.\n",
            "Clean aligned data ready for Rate_of_Change__Raw. Shape: (98780, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Rate_of_Change__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Rate_of_Change__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Rate_of_Change__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.30s)\n",
            "\n",
            "--- Saving results for Rate_of_Change__Raw to Excel ---\n",
            "--- Results for Rate_of_Change__Raw saved (9 sheets). (0.27s)---\n",
            "--- Factor Rate_of_Change_10D processing time: 27.56s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: MA_Crossover_10_50 ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for MA_Crossover_10_50. (11.66s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'MA_Crossover_10_50' for analysis.\n",
            "Clean aligned data ready for MA_Crossover_10_Raw. Shape: (95543, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing MA_Crossover_10_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing MA_Crossover_10_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing MA_Crossover_10_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.62s)\n",
            "\n",
            "--- Saving results for MA_Crossover_10_Raw to Excel ---\n",
            "--- Results for MA_Crossover_10_Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor MA_Crossover_10_50 processing time: 27.51s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Volume_Momentum_50D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                      \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Volume_Momentum_50D. (12.53s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Volume_Momentum_50D' for analysis.\n",
            "Clean aligned data ready for Volume_Momentum_Raw. Shape: (95460, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Volume_Momentum_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Volume_Momentum_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Volume_Momentum_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.26s)\n",
            "\n",
            "--- Saving results for Volume_Momentum_Raw to Excel ---\n",
            "--- Results for Volume_Momentum_Raw saved (9 sheets). (0.23s)---\n",
            "--- Factor Volume_Momentum_50D processing time: 28.03s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Mean_Reversion_20D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Mean_Reversion_20D. (13.00s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Mean_Reversion_20D' for analysis.\n",
            "Clean aligned data ready for Mean_Reversion__Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Mean_Reversion__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Mean_Reversion__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Mean_Reversion__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.66s)\n",
            "\n",
            "--- Saving results for Mean_Reversion__Raw to Excel ---\n",
            "--- Results for Mean_Reversion__Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor Mean_Reversion_20D processing time: 28.90s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Moving_Avg_Reversion_20D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Moving_Avg_Reversion_20D. (15.88s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Moving_Avg_Reversion_20D' for analysis.\n",
            "Clean aligned data ready for Moving_Avg_Reve_Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Moving_Avg_Reve_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Moving_Avg_Reve_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Moving_Avg_Reve_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (16.47s)\n",
            "\n",
            "--- Saving results for Moving_Avg_Reve_Raw to Excel ---\n",
            "--- Results for Moving_Avg_Reve_Raw saved (9 sheets). (0.31s)---\n",
            "--- Factor Moving_Avg_Reversion_20D processing time: 32.67s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Stochastic_Oscillator_K_14D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Stochastic_Oscillator_K_14D. (12.90s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Stochastic_Oscillator_K_14D' for analysis.\n",
            "Clean aligned data ready for Stochastic_Osci_Raw. Shape: (98531, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.63s)\n",
            "\n",
            "--- Saving results for Stochastic_Osci_Raw to Excel ---\n",
            "--- Results for Stochastic_Osci_Raw saved (9 sheets). (0.31s)---\n",
            "--- Factor Stochastic_Oscillator_K_14D processing time: 28.84s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: ATR_14D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for ATR_14D. (12.28s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'ATR_14D' for analysis.\n",
            "Clean aligned data ready for ATR_14D_Raw. Shape: (98531, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing ATR_14D_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing ATR_14D_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing ATR_14D_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.55s)\n",
            "\n",
            "--- Saving results for ATR_14D_Raw to Excel ---\n",
            "--- Results for ATR_14D_Raw saved (9 sheets). (0.23s)---\n",
            "--- Factor ATR_14D processing time: 28.08s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Daily_High_Low_Range ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Daily_High_Low_Range. (12.08s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Daily_High_Low_Range' for analysis.\n",
            "Clean aligned data ready for Daily_High_Low__Raw. Shape: (98890, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Daily_High_Low__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Daily_High_Low__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Daily_High_Low__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.82s)\n",
            "\n",
            "--- Saving results for Daily_High_Low__Raw to Excel ---\n",
            "--- Results for Daily_High_Low__Raw saved (9 sheets). (0.37s)---\n",
            "--- Factor Daily_High_Low_Range processing time: 28.27s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Norm_Bollinger_Width_20D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                           \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Norm_Bollinger_Width_20D. (11.77s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Norm_Bollinger_Width_20D' for analysis.\n",
            "Clean aligned data ready for Norm_Bollinger__Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Norm_Bollinger__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Norm_Bollinger__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Norm_Bollinger__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.80s)\n",
            "\n",
            "--- Saving results for Norm_Bollinger__Raw to Excel ---\n",
            "--- Results for Norm_Bollinger__Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor Norm_Bollinger_Width_20D processing time: 27.80s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Volume_ROC_10D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                 \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Volume_ROC_10D. (12.49s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Volume_ROC_10D' for analysis.\n",
            "Clean aligned data ready for Volume_ROC_10D_Raw. Shape: (98058, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Volume_ROC_10D_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Volume_ROC_10D_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Volume_ROC_10D_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.82s)\n",
            "\n",
            "--- Saving results for Volume_ROC_10D_Raw to Excel ---\n",
            "--- Results for Volume_ROC_10D_Raw saved (9 sheets). (0.25s)---\n",
            "--- Factor Volume_ROC_10D processing time: 28.56s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Trading_Volume ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Trading_Volume. (13.91s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Trading_Volume' for analysis.\n",
            "Clean aligned data ready for Trading_Volume_Raw. Shape: (98890, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Trading_Volume_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Trading_Volume_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Trading_Volume_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.83s)\n",
            "\n",
            "--- Saving results for Trading_Volume_Raw to Excel ---\n",
            "--- Results for Trading_Volume_Raw saved (9 sheets). (0.34s)---\n",
            "--- Factor Trading_Volume processing time: 30.08s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Moving_Average_20D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Moving_Average_20D. (13.73s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Moving_Average_20D' for analysis.\n",
            "Clean aligned data ready for Moving_Average__Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Moving_Average__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Moving_Average__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Moving_Average__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.74s)\n",
            "\n",
            "--- Saving results for Moving_Average__Raw to Excel ---\n",
            "--- Results for Moving_Average__Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor Moving_Average_20D processing time: 29.72s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Exponential_MA_20D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Exponential_MA_20D. (13.92s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Exponential_MA_20D' for analysis.\n",
            "Clean aligned data ready for Exponential_MA__Raw. Shape: (98890, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Exponential_MA__Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Exponential_MA__Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Exponential_MA__Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.94s)\n",
            "\n",
            "--- Saving results for Exponential_MA__Raw to Excel ---\n",
            "--- Results for Exponential_MA__Raw saved (9 sheets). (0.24s)---\n",
            "--- Factor Exponential_MA_20D processing time: 30.10s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: RSI_14D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                          \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for RSI_14D. (13.37s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'RSI_14D' for analysis.\n",
            "Clean aligned data ready for RSI_14D_Raw. Shape: (98531, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing RSI_14D_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing RSI_14D_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing RSI_14D_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.88s)\n",
            "\n",
            "--- Saving results for RSI_14D_Raw to Excel ---\n",
            "--- Results for RSI_14D_Raw saved (9 sheets). (0.26s)---\n",
            "--- Factor RSI_14D processing time: 29.51s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Bollinger_Ratio_LB_20D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                        \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Bollinger_Ratio_LB_20D. (13.30s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Bollinger_Ratio_LB_20D' for analysis.\n",
            "Clean aligned data ready for Bollinger_Ratio_Raw. Shape: (98033, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Bollinger_Ratio_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Bollinger_Ratio_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Bollinger_Ratio_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.57s)\n",
            "\n",
            "--- Saving results for Bollinger_Ratio_Raw to Excel ---\n",
            "--- Results for Bollinger_Ratio_Raw saved (9 sheets). (0.25s)---\n",
            "--- Factor Bollinger_Ratio_LB_20D processing time: 29.13s ---\n",
            "\n",
            "\n",
            "==================== Processing Factor: Stochastic_Oscillator_D_14D ====================\n",
            "\n",
            "--- Performing Factor Neutralization ---\n",
            "Running neutralization regression day by day...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                              \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Neutralization completed for Stochastic_Oscillator_D_14D. (12.64s)\n",
            "\n",
            "--- Starting Factor Analysis ---\n",
            "INFO: Using RAW factor 'Stochastic_Oscillator_D_14D' for analysis.\n",
            "Clean aligned data ready for Stochastic_Osci_Raw. Shape: (98531, 4)\n",
            "Calculating IC Decay...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Calculating Quantile Turnover ---\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 1D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 3D_fwd_ret =====\n",
            "\n",
            "===== Analyzing Stochastic_Osci_Raw vs 5D_fwd_ret =====\n",
            "Analysis calculations finished. (15.83s)\n",
            "\n",
            "--- Saving results for Stochastic_Osci_Raw to Excel ---\n",
            "--- Results for Stochastic_Osci_Raw saved (9 sheets). (0.25s)---\n",
            "--- Factor Stochastic_Oscillator_D_14D processing time: 28.72s ---\n",
            "\n",
            "All factors processed. Finalizing Excel file: factor_analysis_output_combined/combined_factor_analysis_results.xlsx\n",
            "\n",
            "=============================================\n",
            "=== Combined Factor Analysis Script Finished ===\n",
            "=============================================\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "# import matplotlib.pyplot as plt # Keep commented unless plotting is explicitly re-enabled\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.regression.rolling import RollingOLS # Correct import for RollingOLS\n",
        "from statsmodels.tools.sm_exceptions import MissingDataError # Import specific error\n",
        "from numpy.linalg import LinAlgError # Import specific error\n",
        "import yfinance as yf\n",
        "from datetime import timedelta\n",
        "import traceback # For detailed error reporting\n",
        "from tqdm import tqdm # Use standard tqdm\n",
        "import warnings # To suppress specific warnings if needed\n",
        "import os # For path handling\n",
        "import math # For sqrt\n",
        "import openpyxl # Explicitly import for ExcelWriter engine check\n",
        "import time # Can be useful for adding delays\n",
        "\n",
        "# --- Suppress Warnings ---\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
        "warnings.simplefilter(\"ignore\", category=pd.errors.PerformanceWarning) # Suppress PerformanceWarning if desired\n",
        "pd.options.mode.chained_assignment = None # Suppress SettingWithCopyWarning ('warn' or None)\n",
        "\n",
        "# --- Configuration ---\n",
        "_start_date_str = '2020-03-24'\n",
        "_end_date_str = '2025-03-24'\n",
        "target_timezone = 'UTC'\n",
        "\n",
        "# Analysis Parameters\n",
        "benchmark_ticker = \"^HSI\"\n",
        "analysis_periods_str = ['1D_fwd_ret', '3D_fwd_ret', '5D_fwd_ret']\n",
        "fwd_ret_periods_int = tuple(int(p.split('D')[0]) for p in analysis_periods_str)\n",
        "num_quantiles = 5\n",
        "ic_method = 'spearman' # 'spearman' or 'pearson'\n",
        "neutralization_lookback = 60 # For style factors & neutralization lookback\n",
        "MAX_DECAY_LAG = 20\n",
        "# Lookbacks for style factors (keep generic longest lookback calculation)\n",
        "longest_lookback_generic = neutralization_lookback # Adjust if other lookbacks needed for style/data fetch\n",
        "\n",
        "# Output Configuration\n",
        "output_dir = \"factor_analysis_output_combined\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Define the SINGLE output Excel file name\n",
        "combined_output_filename = os.path.join(output_dir, \"combined_factor_analysis_results.xlsx\")\n",
        "\n",
        "# --- Date Handling ---\n",
        "try:\n",
        "    start_date_naive = pd.to_datetime(_start_date_str)\n",
        "    end_date_naive = pd.to_datetime(_end_date_str)\n",
        "    start_date = start_date_naive.tz_localize(target_timezone)\n",
        "    end_date = end_date_naive.tz_localize(target_timezone)\n",
        "    print(f\"INFO: Start date localized to {target_timezone}: {start_date}\")\n",
        "    print(f\"INFO: End date localized to {target_timezone}: {end_date}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Could not localize start/end dates to timezone '{target_timezone}'. Error: {e}\")\n",
        "    raise ValueError(\"Failed to set timezone for start/end dates\") from e\n",
        "\n",
        "# --- Placeholder for Dynamic Universe ---\n",
        "def get_index_constituents_historical(index_ticker, date_str):\n",
        "    print(f\"Attempting to get constituents for {index_ticker} on {date_str}.\")\n",
        "    #print(\"         This is a placeholder - yfinance lacks this feature.\")\n",
        "    #print(\"         Returning a default hardcoded list for now.\")\n",
        "    default_assets = [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', \n",
        "                  \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', \n",
        "                  \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                  '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', \n",
        "                  '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK']\n",
        "    # Add more realistic tickers if possible for better testing\n",
        "    # default_assets.extend([\"1299.HK\", \"2318.HK\", \"0011.HK\", \"0003.HK\", \"0016.HK\", \"0012.HK\"])\n",
        "    if index_ticker == \"^HSI\": return default_assets\n",
        "    else: return []\n",
        "\n",
        "target_index = \"^HSI\"\n",
        "print(f\"\\n--- Attempting to define universe based on index: {target_index} ---\")\n",
        "assets = get_index_constituents_historical(target_index, _start_date_str)\n",
        "if not assets:\n",
        "    print(f\"ERROR: Could not determine assets for index {target_index}. Falling back to hardcoded list.\")\n",
        "    assets =  [\"0001.HK\", \"0002.HK\", \"0003.HK\", \"0005.HK\", \"0006.HK\", \"0011.HK\", '0012.HK', '0016.HK', '0027.HK', '0066.HK', '0101.HK', '0175.HK', '0241.HK', '0267.HK', '0285.HK', '0288.HK', '0291.HK', '0316.HK', \n",
        "                \"0322.HK\", '0386.HK', '0388.HK', '0669.HK', '0688.HK', '0700.HK', '0762.HK', '0823.HK', '0836.HK', '0857.HK', '0868.HK', '0881.HK', '0883.HK', '0939.HK', '0941.HK', '0960.HK', '0968.HK', '0981.HK', \n",
        "                \"0992.HK\", \"1024.HK\", '1038.HK', '1044.HK', '1088.HK', \"1093.HK\", '1099.HK', '1109.HK', '1113.HK', '1177.HK', '1209.HK', '1211.HK', '1299.HK', '1378.HK', '1398.HK', '1810.HK', '1876.HK', '1928.HK',\n",
        "                '1929.HK', '1997.HK', '2015.HK', '2020.HK', '2269.HK', '2313.HK', '2318.HK', '2319.HK', '2331.HK', '2359.HK', '2382.HK', '2388.HK', '2628.HK', '2688.HK', '2899.HK', '3690.HK', '3692.HK', '3968.HK', \n",
        "                '3988.HK', '6618.HK', '6690.HK', '6862.HK', '9618.HK', '9633.HK', '9888.HK', '9901.HK', '9961.HK', '9988.HK', '9999.HK']\n",
        "\n",
        "assets = sorted(list(set(assets))) # Ensure unique and sorted\n",
        "print(f\"INFO: Using asset universe (Count: {len(assets)}): {assets[:10]}...\")\n",
        "\n",
        "# --- Create Target Business Day Index ---\n",
        "try:\n",
        "    import pandas_market_calendars as mcal\n",
        "    hk_calendar = mcal.get_calendar('XHKG')\n",
        "    max_fwd_buffer_days = max(fwd_ret_periods_int) + 10 if fwd_ret_periods_int else 10\n",
        "    # Extend fetch range slightly more for lookbacks and forward returns\n",
        "    calendar_start_naive = start_date_naive - timedelta(days=longest_lookback_generic + 40) # Slightly longer buffer\n",
        "    calendar_end_naive = end_date_naive + timedelta(days=max(max_fwd_buffer_days, MAX_DECAY_LAG + 15)) # Slightly longer buffer\n",
        "    schedule = hk_calendar.schedule(start_date=calendar_start_naive, end_date=calendar_end_naive)\n",
        "    fetch_dates_index_raw = pd.to_datetime(schedule.index).tz_localize(schedule.index.tz)\n",
        "    if fetch_dates_index_raw.tz is None: fetch_dates_index_raw = pd.to_datetime(schedule.index).tz_localize('UTC', ambiguous='infer', nonexistent='shift_forward')\n",
        "    fetch_dates_index = fetch_dates_index_raw.tz_convert(target_timezone).drop_duplicates().sort_values() # Ensure unique & sorted early\n",
        "    dates_index = fetch_dates_index[(fetch_dates_index >= start_date) & (fetch_dates_index <= end_date)].drop_duplicates().sort_values()\n",
        "    print(f\"Using pandas_market_calendars for HK business days. Full fetch range index length: {len(fetch_dates_index)}\")\n",
        "except ImportError:\n",
        "    print(\"WARNING: pandas_market_calendars not found. Using pd.date_range(freq='B'). This might include holidays.\")\n",
        "    max_fwd_period = max(fwd_ret_periods_int) if fwd_ret_periods_int else 0\n",
        "    fetch_start_dt_b = start_date - pd.Timedelta(days=longest_lookback_generic + 40) # Adjusted buffer\n",
        "    fetch_end_dt_b = end_date + pd.Timedelta(days=max(max_fwd_period, MAX_DECAY_LAG) + 15) # Adjusted buffer\n",
        "    fetch_dates_index = pd.date_range(start=fetch_start_dt_b, end=fetch_end_dt_b, freq='B', tz=target_timezone).drop_duplicates().sort_values()\n",
        "    dates_index = fetch_dates_index[(fetch_dates_index >= start_date) & (fetch_dates_index <= end_date)].drop_duplicates().sort_values()\n",
        "\n",
        "if dates_index.empty:\n",
        "    raise ValueError(f\"ERROR: Target dates_index is empty after filtering between {start_date} and {end_date}.\")\n",
        "print(f\"Target Analysis Date Range: {dates_index.min()} to {dates_index.max()} ({len(dates_index)} analysis days)\")\n",
        "\n",
        "# --- Data Fetching (Prices and Total Volume) ---\n",
        "print(\"\\n--- Downloading Price and Total Volume Data ---\")\n",
        "prices_lookback = pd.DataFrame()\n",
        "volumes_lookback = pd.DataFrame()\n",
        "benchmark_prices_lookback = pd.Series(dtype=float)\n",
        "try:\n",
        "    fetch_start_str = fetch_dates_index.min().strftime('%Y-%m-%d')\n",
        "    fetch_end_str = (fetch_dates_index.max() + pd.Timedelta(days=1)).strftime('%Y-%m-%d') # Add 1 day for yf end date convention\n",
        "    print(f\"Fetching data from {fetch_start_str} to {fetch_end_str} for {len(assets)} assets + benchmark {benchmark_ticker}...\")\n",
        "\n",
        "    # Fetch asset data\n",
        "    data_assets = yf.download(assets, start=fetch_start_str, end=fetch_end_str, progress=True, timeout=180, group_by='ticker')\n",
        "\n",
        "    # Fetch benchmark data\n",
        "    data_benchmark = yf.download(benchmark_ticker, start=fetch_start_str, end=fetch_end_str, progress=False)\n",
        "\n",
        "    # Process Asset Data\n",
        "    prices_list = []\n",
        "    volumes_list = []\n",
        "    valid_assets = [] # Keep track of assets with successfully downloaded data\n",
        "    if not data_assets.empty:\n",
        "        # Check if data_assets index needs converting (can happen with yfinance sometimes)\n",
        "        if not isinstance(data_assets.index, pd.DatetimeIndex):\n",
        "             try: data_assets.index = pd.to_datetime(data_assets.index)\n",
        "             except: print(\"WARN: Could not convert downloaded asset data index to DatetimeIndex.\")\n",
        "\n",
        "        for asset in assets:\n",
        "            try:\n",
        "                # Access asset data robustly\n",
        "                if isinstance(data_assets.columns, pd.MultiIndex):\n",
        "                    if asset in data_assets.columns.get_level_values(0):\n",
        "                       asset_data = data_assets[asset]\n",
        "                    else:\n",
        "                       print(f\"WARN: No data returned for {asset} in multi-index result.\")\n",
        "                       continue\n",
        "                elif len(assets) == 1 and asset == assets[0]: # Handle case where only one asset was requested (no multi-index)\n",
        "                     asset_data = data_assets\n",
        "                else: # Should not happen if group_by='ticker' worked for multiple assets\n",
        "                     print(f\"WARN: Unexpected data structure for {asset}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                # Select price and volume\n",
        "                adj_close_key = 'Adj Close' if 'Adj Close' in asset_data.columns else 'Close'\n",
        "                if adj_close_key not in asset_data.columns or 'Volume' not in asset_data.columns:\n",
        "                     print(f\"WARN: Missing '{adj_close_key}' or 'Volume' for {asset}. Skipping.\")\n",
        "                     continue\n",
        "                price_col = asset_data[adj_close_key]\n",
        "                volume_col = asset_data['Volume']\n",
        "\n",
        "                # Check for sufficient non-NaN data\n",
        "                if not price_col.dropna().empty: # Check if not ALL NaN\n",
        "                    prices_list.append(price_col.rename(asset))\n",
        "                    volumes_list.append(volume_col.rename(asset))\n",
        "                    valid_assets.append(asset)\n",
        "                else:\n",
        "                    print(f\"WARN: Price data for {asset} is all NaN.\")\n",
        "\n",
        "            except KeyError:\n",
        "                print(f\"WARN: KeyError accessing data for {asset}. Ticker might be invalid or delisted for the period.\")\n",
        "            except Exception as e_asset:\n",
        "                print(f\"WARN: Could not process data for {asset}. Error: {e_asset}\")\n",
        "\n",
        "    if prices_list:\n",
        "        prices_raw = pd.concat(prices_list, axis=1)\n",
        "        volumes_raw = pd.concat(volumes_list, axis=1)\n",
        "\n",
        "        # Convert index to datetime and localize if needed\n",
        "        if not isinstance(prices_raw.index, pd.DatetimeIndex): prices_raw.index = pd.to_datetime(prices_raw.index)\n",
        "        if prices_raw.index.tz is None: prices_raw.index = prices_raw.index.tz_localize('UTC', ambiguous='infer', nonexistent='shift_forward')\n",
        "        prices_raw = prices_raw.tz_convert(target_timezone)\n",
        "\n",
        "        if not isinstance(volumes_raw.index, pd.DatetimeIndex): volumes_raw.index = pd.to_datetime(volumes_raw.index)\n",
        "        if volumes_raw.index.tz is None: volumes_raw.index = volumes_raw.index.tz_localize('UTC', ambiguous='infer', nonexistent='shift_forward')\n",
        "        volumes_raw = volumes_raw.tz_convert(target_timezone)\n",
        "\n",
        "        # Reindex to our full business day index and forward fill prices, fillna(0) volumes\n",
        "        prices_lookback = prices_raw.reindex(fetch_dates_index).ffill()\n",
        "        volumes_lookback = volumes_raw.reindex(fetch_dates_index).fillna(0)\n",
        "        print(f\"Asset price/volume data processed. Shape: {prices_lookback.shape}\")\n",
        "\n",
        "        # --- Crucial: Update asset list to only include those successfully downloaded ---\n",
        "        original_asset_count = len(assets)\n",
        "        assets = sorted(valid_assets) # Update the global assets list\n",
        "        if len(assets) < original_asset_count:\n",
        "            print(f\"INFO: Asset list updated to {len(assets)} tickers with valid data.\")\n",
        "        if not assets: # Check if asset list became empty\n",
        "             print(\"CRITICAL ERROR: No assets remaining after data download/validation. Exiting.\")\n",
        "             exit()\n",
        "        # -------------------------------------------------------------------------------\n",
        "\n",
        "    else:\n",
        "        print(\"ERROR: No valid asset price data could be fetched or processed. Exiting.\")\n",
        "        exit() # Exit if no asset data\n",
        "\n",
        "    # Process Benchmark Data\n",
        "    if not data_benchmark.empty:\n",
        "        if not isinstance(data_benchmark.index, pd.DatetimeIndex): data_benchmark.index = pd.to_datetime(data_benchmark.index)\n",
        "        adj_close_key_bm = 'Adj Close' if 'Adj Close' in data_benchmark.columns else 'Close'\n",
        "        if adj_close_key_bm not in data_benchmark.columns:\n",
        "             print(f\"ERROR: Benchmark price column ('{adj_close_key_bm}') not found.\")\n",
        "             # Create empty series as fallback\n",
        "             benchmark_prices_lookback = pd.Series(dtype=float, index=fetch_dates_index, name=benchmark_ticker)\n",
        "        else:\n",
        "            benchmark_prices_raw = data_benchmark[adj_close_key_bm]\n",
        "            if benchmark_prices_raw.index.tz is None: benchmark_prices_raw.index = benchmark_prices_raw.index.tz_localize('UTC', ambiguous='infer', nonexistent='shift_forward')\n",
        "            benchmark_prices_raw = benchmark_prices_raw.tz_convert(target_timezone)\n",
        "            benchmark_prices_lookback = benchmark_prices_raw.reindex(fetch_dates_index).ffill()\n",
        "            print(f\"Benchmark data processed. Length: {len(benchmark_prices_lookback)}\")\n",
        "    else:\n",
        "        print(\"ERROR: Benchmark data could not be fetched.\")\n",
        "        # Create empty series if benchmark fetch failed\n",
        "        benchmark_prices_lookback = pd.Series(dtype=float, index=fetch_dates_index, name=benchmark_ticker)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR during data download: {e}\"); traceback.print_exc()\n",
        "    print(\"CRITICAL ERROR: Data download failed. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Create dataframes for the analysis period by slicing lookback data\n",
        "# Ensure slicing uses the potentially updated 'assets' list\n",
        "prices = prices_lookback.loc[dates_index, assets].copy()\n",
        "volumes = volumes_lookback.loc[dates_index, assets].copy()\n",
        "benchmark_prices = benchmark_prices_lookback.loc[dates_index].copy()\n",
        "\n",
        "# --- Robust check for empty or all-NaN core data ---\n",
        "prices_all_nan = False\n",
        "if not prices.empty:\n",
        "    prices_all_nan = prices.isna().all().all() # Check if ALL values are NaN\n",
        "\n",
        "benchmark_all_nan = False\n",
        "if not benchmark_prices.empty and isinstance(benchmark_prices, pd.Series): # Ensure it's a Series\n",
        "    benchmark_all_nan = benchmark_prices.isna().all() # Check if ALL values are NaN\n",
        "\n",
        "if prices.empty or benchmark_prices.empty or prices_all_nan or benchmark_all_nan:\n",
        "     print(\"CRITICAL ERROR: Prices or Benchmark data is invalid (empty or all NaN) for the analysis period. Exiting.\")\n",
        "     exit()\n",
        "# --- End of robust check ---\n",
        "\n",
        "\n",
        "# --- Fetch Industry Data ---\n",
        "print(\"\\n--- Fetching Industry Classification Data ---\")\n",
        "def fetch_industry_data(tickers):\n",
        "    industry_dict = {}\n",
        "    missing_industries = []\n",
        "    for ticker_str in tqdm(tickers, desc=\"Fetching Industries\"):\n",
        "        try:\n",
        "            ticker_obj = yf.Ticker(ticker_str)\n",
        "            # info_data = ticker_obj.fast_info # Potentially faster, fewer fields\n",
        "            info_data = ticker_obj.info # Slower but more comprehensive\n",
        "            industry = info_data.get('industry', 'Unknown')\n",
        "            sector = info_data.get('sector', 'Unknown') # Get sector too\n",
        "            # Prefer industry, fall back to sector, then Unknown\n",
        "            if industry in [None, '', 'N/A', 'Unknown']:\n",
        "                 industry = sector if sector not in [None, '', 'N/A', 'Unknown'] else 'Unknown'\n",
        "            final_industry = industry if industry is not None else 'Unknown' # Ensure value is not None\n",
        "\n",
        "            if final_industry == 'Unknown': missing_industries.append(ticker_str)\n",
        "            industry_dict[ticker_str] = final_industry\n",
        "            time.sleep(0.05) # Small delay to avoid potential rate limiting\n",
        "        except Exception as e_ind:\n",
        "            print(f\"WARN: Error fetching industry for {ticker_str}: {e_ind}\") # Show specific error\n",
        "            industry_dict[ticker_str] = 'Unknown'\n",
        "            missing_industries.append(ticker_str)\n",
        "\n",
        "    if missing_industries: print(f\"WARNING: Could not reliably fetch industry/sector for: {list(set(missing_industries))}\")\n",
        "    return pd.Series(industry_dict, name='industry')\n",
        "\n",
        "if assets: # Only fetch if we have assets\n",
        "    asset_industries = fetch_industry_data(assets)\n",
        "    industry_dummies_static = pd.DataFrame()\n",
        "    if not asset_industries.empty:\n",
        "        # Create dummies, ensuring they align with the final 'assets' list\n",
        "        industry_dummies_static = pd.get_dummies(asset_industries.reindex(assets).fillna('Unknown'), dummy_na=False, prefix='Ind').astype(int)\n",
        "        # Drop 'Ind_Unknown' if other industries exist and it's all zero, or if only Unknown exists keep it.\n",
        "        if 'Ind_Unknown' in industry_dummies_static.columns and len(industry_dummies_static.columns) > 1:\n",
        "             if not industry_dummies_static['Ind_Unknown'].any():\n",
        "                  industry_dummies_static = industry_dummies_static.drop('Ind_Unknown', axis=1)\n",
        "        industry_dummies_static.index.name = 'asset'\n",
        "        print(f\"Created Static Industry Dummies shape: {industry_dummies_static.shape}\")\n",
        "        if industry_dummies_static.empty:\n",
        "             print(\"WARN: Industry dummies became empty after processing (e.g., only 'Unknown' dropped). Creating default.\")\n",
        "             industry_dummies_static = pd.DataFrame({'Ind_NoIndustry': 1}, index=assets).astype(int)\n",
        "    else:\n",
        "        print(\"WARNING: Could not create industry dummies (fetch returned empty). Creating default.\")\n",
        "        industry_dummies_static = pd.DataFrame({'Ind_NoIndustry': 1}, index=assets).astype(int)\n",
        "        industry_dummies_static.index.name = 'asset'\n",
        "else:\n",
        "    print(\"WARNING: No assets defined, skipping industry fetch.\")\n",
        "    asset_industries = pd.Series(dtype=str, name='industry')\n",
        "    industry_dummies_static = pd.DataFrame(index=pd.Index([], name='asset')) # Ensure empty df has index\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# === LOAD OR DEFINE YOUR PRE-CALCULATED FACTORS HERE ===\n",
        "# ================================================================\n",
        "print(\"\\n--- Loading/Defining Pre-calculated Factors ---\")\n",
        "\n",
        "# --- INPUT REQUIRED ---\n",
        "# Option 1: Load from file (RECOMMENDED)\n",
        "LOAD_FROM_FILE = True # SET TO TRUE TO LOAD FROM FILE\n",
        "factors_file_path = \"processed_alpha_data_values.csv\" # OR .csv, .pkl etc.\n",
        "# Expected format: See FORMAT 1 or FORMAT 2 descriptions below.\n",
        "\n",
        "# Option 2: Define programmatically (like the dummy example)\n",
        "CREATE_DUMMY_FACTORS = True # Set to False if loading from file\n",
        "\n",
        "factors_input_df = pd.DataFrame() # Initialize\n",
        "\n",
        "if LOAD_FROM_FILE:\n",
        "    print(f\"Attempting to load factors from: {factors_file_path}\")\n",
        "    if not os.path.exists(factors_file_path):\n",
        "         print(f\"ERROR: Factors file not found at {factors_file_path}\")\n",
        "    else:\n",
        "        try:\n",
        "            # Example loading parquet (adjust based on your file type)\n",
        "            if factors_file_path.endswith(\".parquet\"):\n",
        "                factors_input_df = pd.read_parquet(factors_file_path)\n",
        "            elif factors_file_path.endswith(\".csv\"):\n",
        "                # Adjust read_csv parameters as needed (e.g., index_col, parse_dates)\n",
        "                # Assuming format 2 (stacked) for CSV example:\n",
        "                factors_input_df = pd.read_csv(factors_file_path, index_col=[0, 1], parse_dates=[0])\n",
        "                # Set index names if not read automatically\n",
        "                if factors_input_df.index.names != ['date', 'asset']:\n",
        "                     print(\"WARN: Setting loaded CSV index names to ['date', 'asset'].\")\n",
        "                     factors_input_df.index.names = ['date', 'asset']\n",
        "            elif factors_file_path.endswith(\".pkl\"):\n",
        "                factors_input_df = pd.read_pickle(factors_file_path)\n",
        "            else:\n",
        "                print(f\"ERROR: Unsupported file format: {factors_file_path}\")\n",
        "\n",
        "            if not factors_input_df.empty:\n",
        "                print(f\"Successfully loaded factors from file. Initial shape: {factors_input_df.shape}\")\n",
        "\n",
        "                # --- Post-load processing based on format ---\n",
        "                # Check if loaded data is Format 2 (stacked: Index=(date, asset), Columns=FactorNames)\n",
        "                if isinstance(factors_input_df.index, pd.MultiIndex) and list(factors_input_df.index.names) == ['date', 'asset']:\n",
        "                    print(\"INFO: Loaded data appears to be in Format 2 (Stacked). Unstacking...\")\n",
        "                    try:\n",
        "                        factors_input_df_wide = factors_input_df.unstack(level='asset')\n",
        "                        factors_input_df_wide.columns = pd.MultiIndex.from_tuples(\n",
        "                            [(col_name, asset_name) for col_name, asset_name in factors_input_df_wide.columns],\n",
        "                            names=['factor_name', 'asset']\n",
        "                        )\n",
        "                        factors_input_df = factors_input_df_wide # Overwrite with Format 1\n",
        "                        print(\"Successfully unstacked factors to Format 1 (Wide).\")\n",
        "                    except Exception as e_unstack_load:\n",
        "                        print(f\"ERROR: Could not unstack the loaded factor DataFrame: {e_unstack_load}\")\n",
        "                        factors_input_df = pd.DataFrame() # Invalidate on error\n",
        "                # Assume loaded data is already Format 1 (wide: Index=date, Columns=(factor_name, asset))\n",
        "                elif isinstance(factors_input_df.index, pd.DatetimeIndex) and isinstance(factors_input_df.columns, pd.MultiIndex):\n",
        "                    print(\"INFO: Loaded data appears to be in Format 1 (Wide).\")\n",
        "                    # Ensure column level names are correct\n",
        "                    if list(factors_input_df.columns.names) != ['factor_name', 'asset']:\n",
        "                        print(\"WARN: Renaming columns to ['factor_name', 'asset']. Please verify.\")\n",
        "                        factors_input_df.columns.names = ['factor_name', 'asset']\n",
        "                else:\n",
        "                    print(\"ERROR: Loaded DataFrame format is not recognized as Format 1 or Format 2.\")\n",
        "                    factors_input_df = pd.DataFrame() # Invalidate\n",
        "\n",
        "        except Exception as e_load:\n",
        "            print(f\"ERROR: Failed to load or process factors file: {e_load}\")\n",
        "            traceback.print_exc()\n",
        "            factors_input_df = pd.DataFrame()\n",
        "\n",
        "elif CREATE_DUMMY_FACTORS:\n",
        "    # Create dummy stacked data for demonstration:\n",
        "    print(\"INFO: Creating dummy factor data for demonstration...\")\n",
        "    if not dates_index.empty and assets:\n",
        "        multi_idx = pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset'])\n",
        "        dummy_data = {\n",
        "            'Factor_Dummy_1': np.random.randn(len(multi_idx)),\n",
        "            'Factor_Dummy_2': np.random.rand(len(multi_idx)) - 0.5\n",
        "        }\n",
        "        factors_input_df_stacked = pd.DataFrame(dummy_data, index=multi_idx)\n",
        "        print(f\"Dummy stacked factors created. Shape: {factors_input_df_stacked.shape}\")\n",
        "\n",
        "        # --- Convert FORMAT 2 (Stacked) to FORMAT 1 (Wide - Preferred by the script) ---\n",
        "        try:\n",
        "            factors_input_df = factors_input_df_stacked.unstack(level='asset')\n",
        "            factors_input_df.columns = pd.MultiIndex.from_tuples(\n",
        "                [(col_name, asset_name) for col_name, asset_name in factors_input_df.columns],\n",
        "                names=['factor_name', 'asset']\n",
        "            )\n",
        "            print(\"Successfully unstacked dummy factors to Format 1 (Wide).\")\n",
        "        except Exception as e_unstack_dummy:\n",
        "            print(f\"ERROR: Could not unstack the dummy factor DataFrame: {e_unstack_dummy}\")\n",
        "            factors_input_df = pd.DataFrame() # Assign empty df on error\n",
        "    else:\n",
        "        print(\"ERROR: Cannot create dummy factors - dates_index or assets are empty.\")\n",
        "        factors_input_df = pd.DataFrame()\n",
        "else:\n",
        "    print(\"INFO: No factor loading or creation specified.\")\n",
        "\n",
        "\n",
        "# --- Validation and Final Preparation ---\n",
        "precalculated_factors_df = pd.DataFrame() # Initialize final df\n",
        "\n",
        "if not factors_input_df.empty:\n",
        "    # Ensure index is DatetimeIndex and has correct timezone\n",
        "    if not isinstance(factors_input_df.index, pd.DatetimeIndex):\n",
        "        try:\n",
        "            factors_input_df.index = pd.to_datetime(factors_input_df.index)\n",
        "        except Exception as e_conv:\n",
        "            print(f\"ERROR: Could not convert factor index to DatetimeIndex: {e_conv}. Invalidating factors.\")\n",
        "            factors_input_df = pd.DataFrame()\n",
        "\n",
        "    if not factors_input_df.empty: # Check again after potential invalidation\n",
        "        if factors_input_df.index.tz is None:\n",
        "            try:\n",
        "                print(\"INFO: Localizing factor index timezone...\")\n",
        "                factors_input_df.index = factors_input_df.index.tz_localize(target_timezone, ambiguous='infer', nonexistent='shift_forward')\n",
        "            except TypeError: # Already localized\n",
        "                 pass\n",
        "            except Exception as e_tz:\n",
        "                print(f\"ERROR: Could not localize factor index timezone: {e_tz}. Invalidating factors.\")\n",
        "                factors_input_df = pd.DataFrame()\n",
        "\n",
        "        if not factors_input_df.empty and factors_input_df.index.tz != target_timezone:\n",
        "            try:\n",
        "                print(\"INFO: Converting factor index timezone...\")\n",
        "                factors_input_df.index = factors_input_df.index.tz_convert(target_timezone)\n",
        "            except Exception as e_tz_conv:\n",
        "                print(f\"ERROR: Could not convert factor index timezone: {e_tz_conv}. Invalidating factors.\")\n",
        "                factors_input_df = pd.DataFrame()\n",
        "\n",
        "    # Ensure columns are MultiIndex ['factor_name', 'asset']\n",
        "    if not factors_input_df.empty:\n",
        "        if isinstance(factors_input_df.columns, pd.MultiIndex) and list(factors_input_df.columns.names) == ['factor_name', 'asset']:\n",
        "             # Reindex to ensure all factors/assets/dates are present\n",
        "             factor_names_present = factors_input_df.columns.get_level_values('factor_name').unique()\n",
        "             # Ensure assets used for reindexing are the ones we have price data for\n",
        "             target_multi_columns = pd.MultiIndex.from_product([factor_names_present, assets], names=['factor_name', 'asset'])\n",
        "\n",
        "             print(f\"Reindexing loaded factors to match analysis dates ({len(dates_index)}) and assets ({len(assets)})...\")\n",
        "             # Reindex BOTH index and columns to match the analysis scope\n",
        "             precalculated_factors_df = factors_input_df.reindex(index=dates_index, columns=target_multi_columns)\n",
        "             # Check for excessive NaNs after reindexing\n",
        "             nan_frac = precalculated_factors_df.isna().mean().mean() if not precalculated_factors_df.empty else 1.0\n",
        "             if precalculated_factors_df.isna().all().all():\n",
        "                  print(\"CRITICAL WARN: Factor DataFrame is ALL NaNs after reindexing. Check date/asset alignment. Analysis will likely fail.\")\n",
        "             elif nan_frac > 0.9: # Example threshold\n",
        "                  print(f\"WARN: Factor DataFrame has >90% NaNs ({nan_frac:.1%}) after reindexing.\")\n",
        "\n",
        "             print(f\"Final precalculated factors DataFrame ready. Shape: {precalculated_factors_df.shape}\")\n",
        "             available_factors = precalculated_factors_df.columns.get_level_values('factor_name').unique().tolist()\n",
        "             print(f\"Available factors: {available_factors}\")\n",
        "             if not available_factors:\n",
        "                  print(\"ERROR: No factor names found after processing. Invalidating.\")\n",
        "                  precalculated_factors_df = pd.DataFrame()\n",
        "\n",
        "        else:\n",
        "             print(\"ERROR: Processed factor DataFrame columns are not MultiIndex named ['factor_name', 'asset']. Invalidating factors.\")\n",
        "             precalculated_factors_df = pd.DataFrame()\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: No factor input data loaded or created.\")\n",
        "    precalculated_factors_df = pd.DataFrame()\n",
        "\n",
        "# Final check before analysis loop\n",
        "if precalculated_factors_df.empty:\n",
        "     print(\"\\nCRITICAL ERROR: The precalculated_factors_df is empty or invalid after loading/processing. Cannot proceed with analysis.\")\n",
        "     exit()\n",
        "# ================================================================\n",
        "# === END OF FACTOR LOADING SECTION ===\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "# --- Calculate Style Factors (Do ONCE before loop) ---\n",
        "print(\"\\n--- Calculating Style Factors (Beta, Size Proxy, Liquidity Proxy, Residual Volatility) ---\")\n",
        "# Initialize with correct index names BUT NO DATA YET\n",
        "style_factors = pd.DataFrame(index=pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset'])) # Base structure\n",
        "style_factors_calculated = {} # Store components temporarily\n",
        "\n",
        "min_periods_neut = max(10, neutralization_lookback // 2)\n",
        "try:\n",
        "    # Use lookback dataframes here\n",
        "    print(f\"Calculating style factors using {neutralization_lookback}-day lookback...\")\n",
        "\n",
        "    # 1. Size/Liquidity Proxy: Log of rolling average dollar volume\n",
        "    if not prices_lookback.empty and not volumes_lookback.empty:\n",
        "        dollar_volume_lb = prices_lookback.loc[:, assets] * volumes_lookback.loc[:, assets] # Ensure asset alignment\n",
        "        rolling_dollar_vol_lb = dollar_volume_lb.rolling(neutralization_lookback, min_periods=min_periods_neut).mean()\n",
        "        epsilon = 1e-9 # Smaller epsilon\n",
        "        size_liquidity_proxy_df = np.log1p(rolling_dollar_vol_lb + epsilon)\n",
        "        size_liquidity_proxy_df = size_liquidity_proxy_df.replace([np.inf, -np.inf], np.nan)\n",
        "        # Slice to analysis dates *before* stacking\n",
        "        size_liq_proxy_stacked = size_liquidity_proxy_df.loc[dates_index, assets].stack(future_stack=True).rename('size_liquidity_proxy')\n",
        "        size_liq_proxy_stacked.index.names = ['date', 'asset'] # Set Index Names\n",
        "        if not size_liq_proxy_stacked.dropna().empty: # Check if not all NaN\n",
        "            style_factors_calculated['size_liquidity_proxy'] = size_liq_proxy_stacked\n",
        "            print(\" - Calculated Size/Liquidity Proxy.\")\n",
        "        else: print(\"WARN: Size/Liquidity Proxy resulted in empty or all-NaN series.\")\n",
        "    else: print(\"WARN: Skipping Size/Liquidity Proxy calc due to missing price/volume lookback data.\")\n",
        "\n",
        "\n",
        "    # 2. Beta: Rolling regression against benchmark\n",
        "    if not prices_lookback.empty and not benchmark_prices_lookback.dropna().empty: # Check benchmark has data\n",
        "        asset_returns_lb = prices_lookback.loc[:, assets].pct_change() # Ensure asset alignment\n",
        "        benchmark_returns_lb = benchmark_prices_lookback.pct_change()\n",
        "\n",
        "        # Ensure benchmark returns are not all NaN before proceeding\n",
        "        if benchmark_returns_lb.dropna().empty:\n",
        "             print(\"WARN: Benchmark returns are all NaN in lookback period. Skipping Beta calculation.\")\n",
        "        else:\n",
        "            aligned_benchmark_ret_lb = benchmark_returns_lb.reindex(asset_returns_lb.index).ffill()\n",
        "            X_beta_base = sm.add_constant(aligned_benchmark_ret_lb.dropna()) # Prepare RHS once\n",
        "            betas = {} # Re-initialize dict for beta results specifically\n",
        "\n",
        "            print(\" - Calculating Beta (this may take a while)...\")\n",
        "            with tqdm(total=len(assets), desc=\"Calculating Beta\", leave=False) as pbar:\n",
        "                for asset in assets:\n",
        "                    y_beta = asset_returns_lb[asset].dropna()\n",
        "\n",
        "                    # --- Robust Beta Calculation Start ---\n",
        "                    if y_beta.empty or X_beta_base.empty:\n",
        "                        betas[asset] = pd.Series(np.nan, index=dates_index, name=asset) # Assign NaN series aligned with main index\n",
        "                        pbar.update(1)\n",
        "                        continue # Skip to next asset\n",
        "\n",
        "                    common_idx_beta = X_beta_base.index.intersection(y_beta.index)\n",
        "\n",
        "                    if len(common_idx_beta) >= neutralization_lookback: # Use >= lookback for min_nobs logic\n",
        "                        X_beta_aligned = X_beta_base.loc[common_idx_beta]\n",
        "                        y_beta_aligned = y_beta.loc[common_idx_beta]\n",
        "\n",
        "                        if y_beta_aligned.empty or X_beta_aligned.empty:\n",
        "                             betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                             pbar.update(1)\n",
        "                             continue\n",
        "\n",
        "                        try:\n",
        "                            # Use imported RollingOLS directly\n",
        "                            rols = RollingOLS(endog=y_beta_aligned, exog=X_beta_aligned,\n",
        "                                             window=neutralization_lookback, min_nobs=min_periods_neut)\n",
        "                            results = rols.fit()\n",
        "                            # Check if params DataFrame is not empty and has enough columns\n",
        "                            if not results.params.empty and results.params.shape[1] > 1:\n",
        "                                 beta_series = results.params.iloc[:, 1] # Beta coeff index 1\n",
        "                                 # Reindex to target dates_index AFTER calculation for this asset\n",
        "                                 betas[asset] = beta_series.reindex(dates_index).ffill().bfill()\n",
        "                            else:\n",
        "                                 #print(f\"WARN [{asset}]: RollingOLS params empty or misshaped.\")\n",
        "                                 betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "\n",
        "                        except IndexError: # Catch index error if params structure unexpected\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS IndexError (likely bad fit).\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                        except MissingDataError: # Catch if not enough observations for a window\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS MissingDataError.\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                        except LinAlgError: # Catch linear algebra errors (e.g., singular matrix)\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS LinAlgError.\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                        except ValueError as e_ols_val: # Catch potential value errors during fit\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS ValueError: {e_ols_val}\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                        except Exception as e_beta_sm: # Catch other unexpected errors\n",
        "                            #print(f\"WARN [{asset}]: RollingOLS failed unexpectedly: {e_beta_sm}\")\n",
        "                            betas[asset] = pd.Series(np.nan, index=dates_index, name=asset)\n",
        "                    else:\n",
        "                        # Not enough common data points for reliable rolling beta\n",
        "                        betas[asset] = pd.Series(np.nan, index=dates_index, name=asset) # Assign NaN series\n",
        "                    pbar.update(1)\n",
        "                    # --- Robust Beta Calculation End ---\n",
        "\n",
        "            # --- Concatenate Beta results ---\n",
        "            if betas: # Check if the betas dictionary is not empty\n",
        "                try:\n",
        "                    # Filter out any potential non-Series items just in case\n",
        "                    valid_betas = {k: v for k, v in betas.items() if isinstance(v, pd.Series)}\n",
        "                    if valid_betas:\n",
        "                        beta_df = pd.concat(valid_betas.values(), axis=1, keys=valid_betas.keys()) # Use values and keys\n",
        "                        beta_df.columns.name = 'asset' # Name the column index\n",
        "                        # Stack the dataframe (already indexed by dates_index)\n",
        "                        beta_stacked = beta_df.stack(future_stack=True).rename('beta')\n",
        "                        beta_stacked.index.names = ['date', 'asset'] # Set Index Names\n",
        "                        if not beta_stacked.dropna().empty: # Check if not all NaN\n",
        "                            style_factors_calculated['beta'] = beta_stacked\n",
        "                            print(\" - Calculated Beta.\")\n",
        "                        else: print(\"WARN: Beta calculation resulted in empty or all-NaN series after stacking.\")\n",
        "                    else: print(\"WARN: No valid beta Series were generated.\")\n",
        "                except ValueError as e_concat_beta:\n",
        "                     print(f\"ERROR concatenating beta results: {e_concat_beta}\")\n",
        "                     print(\"WARN: Skipping Beta factor due to concatenation error.\")\n",
        "            else:\n",
        "                print(\"WARN: No beta values could be calculated for any asset.\")\n",
        "    else: print(\"WARN: Skipping Beta calc due to missing price/benchmark lookback data.\")\n",
        "\n",
        "\n",
        "    # 3. Residual Volatility: Rolling std dev of returns\n",
        "    if not prices_lookback.empty:\n",
        "        if 'asset_returns_lb' not in locals(): # Calculate if not done for beta\n",
        "             asset_returns_lb = prices_lookback.loc[:, assets].pct_change() # Ensure asset alignment\n",
        "        rolling_std_ret = asset_returns_lb.rolling(neutralization_lookback, min_periods=min_periods_neut).std()\n",
        "        # Slice to analysis dates *before* stacking\n",
        "        res_vol_stacked = rolling_std_ret.loc[dates_index, assets].stack(future_stack=True).rename('residual_vol')\n",
        "        res_vol_stacked.index.names = ['date', 'asset'] # Set Index Names\n",
        "        if not res_vol_stacked.dropna().empty: # Check if not all NaN\n",
        "             style_factors_calculated['residual_vol'] = res_vol_stacked\n",
        "             print(\" - Calculated Residual Volatility.\")\n",
        "        else: print(\"WARN: Residual Volatility calculation resulted in empty or all-NaN series.\")\n",
        "    else: print(\"WARN: Skipping Residual Volatility calc due to missing price lookback data.\")\n",
        "\n",
        "\n",
        "    # --- Combine all calculated factors at the end ---\n",
        "    if style_factors_calculated:\n",
        "         # Ensure all components are Series before concat\n",
        "         valid_components = {k: v for k, v in style_factors_calculated.items() if isinstance(v, pd.Series)}\n",
        "         if valid_components:\n",
        "             style_factors = pd.concat(valid_components.values(), axis=1) # Combine valid Series into DF\n",
        "             # Reindex just in case some date/asset combos were missing in all factors\n",
        "             style_factors = style_factors.reindex(pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset']))\n",
        "             print(f\"Style factors calculation finished. Final Shape: {style_factors.shape}\")\n",
        "         else:\n",
        "              print(\"WARN: No valid style factor components were calculated.\")\n",
        "              style_factors = pd.DataFrame(index=pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset']))\n",
        "    else:\n",
        "         print(\"WARN: No style factors were successfully calculated.\")\n",
        "         style_factors = pd.DataFrame(index=pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset']))\n",
        "\n",
        "\n",
        "except Exception as e_style:\n",
        "    print(f\"ERROR calculating style factors: {e_style}\")\n",
        "    traceback.print_exc()\n",
        "    style_factors = pd.DataFrame(index=pd.MultiIndex.from_product([dates_index, assets], names=['date', 'asset']))\n",
        "    print(\"WARN: Style factors calculation failed. Proceeding without them for neutralization.\")\n",
        "\n",
        "\n",
        "# --- Analysis Function Definitions ---\n",
        "print(\"\\n--- Defining/Importing Analysis Functions ---\")\n",
        "\n",
        "def calculate_forward_returns(prices_df, periods):\n",
        "    \"\"\"Calculates forward returns for multiple periods. Corrected version 3.\"\"\"\n",
        "    # prices_df: Index=date, Columns=assets\n",
        "    all_fwd_returns = {} # Store DataFrames for each period\n",
        "\n",
        "    if prices_df.empty:\n",
        "         print(\"ERROR [Fwd Ret]: Input prices_df is empty.\")\n",
        "         return pd.DataFrame(index=pd.MultiIndex([[],[]], [[],[]], names=['date','asset']), columns=analysis_periods_str)\n",
        "\n",
        "    for p in periods:\n",
        "        fwd_ret_col_name = f'{p}D_fwd_ret'\n",
        "        # Calculate returns for all assets for this period 'p'\n",
        "        shifted_price = prices_df.shift(-p)\n",
        "        # Ensure alignment before division, handle potential NaNs gracefully\n",
        "        returns_p = (shifted_price / prices_df - 1).replace([np.inf, -np.inf], np.nan)\n",
        "        all_fwd_returns[fwd_ret_col_name] = returns_p\n",
        "\n",
        "    if not all_fwd_returns:\n",
        "        print(\"WARN [Fwd Ret]: No forward returns calculated.\")\n",
        "        return pd.DataFrame(index=pd.MultiIndex([[],[]], [[],[]], names=['date','asset']), columns=analysis_periods_str)\n",
        "\n",
        "    # Concat creates MultiIndex columns: ('1D_fwd_ret', 'asset1'), ('3D_fwd_ret', 'asset1'), ...\n",
        "    combined_fwd_returns_wide = pd.concat(all_fwd_returns, axis=1)\n",
        "    combined_fwd_returns_wide.columns.names = ['period', 'asset'] # Name the column levels\n",
        "\n",
        "    # Stack the 'asset' level from columns to index to get format:\n",
        "    # Index = MultiIndex('date', 'asset'), Columns = Index(['1D_fwd_ret', '3D_fwd_ret', ...])\n",
        "    fwd_returns_stacked = combined_fwd_returns_wide.stack(level='asset', future_stack=True)\n",
        "    fwd_returns_stacked.index.names = ['date', 'asset'] # Ensure final index names are correct\n",
        "    # Ensure columns are named correctly (should be the periods after stacking 'asset')\n",
        "    fwd_returns_stacked.columns.name = 'period' # Name the columns index\n",
        "\n",
        "    return fwd_returns_stacked\n",
        "\n",
        "def get_quantile_assignments(factor_df, num_quantiles=5):\n",
        "    \"\"\"Assigns assets to quantiles based on factor values for each date.\"\"\"\n",
        "    # Input factor_df: Index=(date, asset), Column='factor'\n",
        "    if factor_df.empty: return pd.DataFrame(columns=['quantile'], index=factor_df.index) # Handle empty input\n",
        "\n",
        "    factor_col_name = 'factor'\n",
        "    if factor_col_name not in factor_df.columns:\n",
        "        if isinstance(factor_df, pd.Series) and factor_df.name == factor_col_name:\n",
        "             factor_df = factor_df.to_frame()\n",
        "        elif not factor_df.empty: # Try using the first column if name isn't 'factor'\n",
        "             original_col = factor_df.columns[0]\n",
        "             factor_df = factor_df[[original_col]].rename(columns={original_col: factor_col_name})\n",
        "        else: # Cannot proceed if empty and no factor column\n",
        "            return pd.DataFrame(columns=['quantile'], index=factor_df.index)\n",
        "\n",
        "    # Use transform to handle broadcasting within groups safely\n",
        "    quantiles = factor_df.groupby(level='date')[factor_col_name].transform(\n",
        "        lambda x: pd.qcut(x, num_quantiles, labels=False, duplicates='drop')\n",
        "    ) + 1 # Labels 1 to N\n",
        "    quantiles = quantiles.rename('quantile')\n",
        "\n",
        "    return quantiles.to_frame() # Return as DataFrame\n",
        "\n",
        "\n",
        "def quantile_analysis(analysis_data, factor_display_name, num_quantiles=5, ret_col='1D_fwd_ret'):\n",
        "    \"\"\"Performs quantile return analysis.\"\"\"\n",
        "    # analysis_data: Index=(date, asset), Columns=['factor', ret_col]\n",
        "    if ret_col not in analysis_data.columns:\n",
        "        print(f\"WARN [Quantile Analysis]: Return column '{ret_col}' not found.\")\n",
        "        return None, None\n",
        "    if 'factor' not in analysis_data.columns:\n",
        "        print(f\"WARN [Quantile Analysis]: Factor column 'factor' not found.\")\n",
        "        return None, None\n",
        "    if analysis_data.empty or analysis_data[['factor', ret_col]].isna().all().all():\n",
        "         print(f\"WARN [Quantile Analysis]: Input data empty or all NaN for {factor_display_name}/{ret_col}.\")\n",
        "         return None, None\n",
        "\n",
        "    quantile_assignments = get_quantile_assignments(analysis_data[['factor']], num_quantiles)\n",
        "    if quantile_assignments.empty or quantile_assignments['quantile'].isna().all():\n",
        "         print(f\"WARN [Quantile Analysis]: Could not assign quantiles for {factor_display_name}.\")\n",
        "         return None, None\n",
        "\n",
        "    data_with_quantiles = analysis_data.join(quantile_assignments, how='inner').dropna(subset=['quantile'])\n",
        "    if data_with_quantiles.empty: # Check after join/dropna\n",
        "         print(f\"WARN [Quantile Analysis]: Data empty after joining quantiles for {factor_display_name}.\")\n",
        "         return None, None\n",
        "\n",
        "    # Mean return per quantile (averaged over time)\n",
        "    mean_ret_by_quantile = data_with_quantiles.groupby('quantile')[ret_col].mean()\n",
        "\n",
        "    # Cumulative return per quantile\n",
        "    daily_mean_ret_by_q = data_with_quantiles.groupby(['date', 'quantile'])[ret_col].mean().unstack(level='quantile')\n",
        "    # Fill missing daily quantile returns (e.g., if a quantile had no members) with 0 for cumulative calc\n",
        "    daily_mean_ret_by_q = daily_mean_ret_by_q.fillna(0)\n",
        "    # Calculate geometric cumulative returns\n",
        "    cumulative_ret_by_q = (1 + daily_mean_ret_by_q).cumprod() - 1\n",
        "\n",
        "    return mean_ret_by_quantile, cumulative_ret_by_q\n",
        "\n",
        "\n",
        "def calculate_quantile_turnover(quantile_assignments, num_quantiles=5):\n",
        "    \"\"\"Calculates quantile turnover.\"\"\"\n",
        "    # quantile_assignments: Index=(date, asset), Column='quantile'\n",
        "    if quantile_assignments.empty or quantile_assignments['quantile'].isna().all():\n",
        "        print(\"WARN [Turnover]: Input quantile assignments are empty or all NaN.\")\n",
        "        return pd.DataFrame() # Return empty df\n",
        "\n",
        "    turnover_results = {}\n",
        "    quantiles_unstacked = quantile_assignments['quantile'].unstack(level='asset')\n",
        "\n",
        "    # Ensure index is sorted for shift to work correctly\n",
        "    quantiles_unstacked = quantiles_unstacked.sort_index()\n",
        "\n",
        "    for q in range(1, num_quantiles + 1):\n",
        "        quantile_members = (quantiles_unstacked == q)\n",
        "        prev_members = quantile_members.shift(1)\n",
        "\n",
        "        # Align and stack, keeping only days where both current and previous exist\n",
        "        combined = pd.concat(\n",
        "            [quantile_members.stack(future_stack=True).rename('current'),\n",
        "             prev_members.stack(future_stack=True).rename('previous')],\n",
        "            axis=1\n",
        "        ).dropna() # Drop rows where either is NaN (i.e., first day, or if assets change)\n",
        "\n",
        "        if combined.empty:\n",
        "             # Handle case with only one day of data or no overlap\n",
        "             daily_turnover_series = pd.Series(np.nan, index=quantiles_unstacked.index)\n",
        "        else:\n",
        "            def daily_turnover(group):\n",
        "                # Check if group is empty or has wrong structure\n",
        "                if group.empty or not all(c in group.columns for c in ['current', 'previous']):\n",
        "                    return np.nan\n",
        "\n",
        "                stayed = (group['current'] & group['previous']).sum()\n",
        "                entered = (group['current'] & ~group['previous']).sum()\n",
        "                exited = (~group['current'] & group['previous']).sum()\n",
        "                total_current = group['current'].sum()\n",
        "                total_previous = group['previous'].sum()\n",
        "\n",
        "                avg_size = (total_current + total_previous) / 2.0\n",
        "                if avg_size < 1e-6: return 0.0 # Handle near-zero avg size\n",
        "\n",
        "                # Using: max(entered, exited) / avg_size\n",
        "                traded = max(entered, exited)\n",
        "                return traded / avg_size if avg_size > 0 else 0.0\n",
        "\n",
        "            # Apply daily turnover calculation\n",
        "            daily_turnover_series = combined.groupby(level='date').apply(daily_turnover)\n",
        "            # Reindex to original dates index to include days with NaN turnover\n",
        "            daily_turnover_series = daily_turnover_series.reindex(quantiles_unstacked.index)\n",
        "\n",
        "\n",
        "        turnover_results[f'Q{q}_Turnover'] = daily_turnover_series\n",
        "\n",
        "    turnover_df = pd.DataFrame(turnover_results)\n",
        "    if not turnover_df.empty:\n",
        "        turnover_df['Mean_Turnover'] = turnover_df.mean(axis=1)\n",
        "    return turnover_df\n",
        "\n",
        "\n",
        "def calculate_ic(analysis_data, factor_display_name, ret_col='1D_fwd_ret', method='spearman'):\n",
        "    \"\"\"Calculates Information Coefficient (IC).\"\"\"\n",
        "    # analysis_data: Index=(date, asset), Columns=['factor', ret_col]\n",
        "    if ret_col not in analysis_data.columns or 'factor' not in analysis_data.columns:\n",
        "         print(f\"WARN [IC]: Missing 'factor' or '{ret_col}' for {factor_display_name}\")\n",
        "         return None, None\n",
        "    if analysis_data.empty or analysis_data[['factor', ret_col]].isna().all().all():\n",
        "         print(f\"WARN [IC]: Input data empty or all NaN for {factor_display_name}/{ret_col}.\")\n",
        "         return None, None\n",
        "\n",
        "    def ic_calc(group):\n",
        "        group_clean = group[['factor', ret_col]].dropna()\n",
        "        if len(group_clean) < 3: return np.nan # Need >= 3 points for reliable correlation? Usually 2 is min.\n",
        "        try:\n",
        "            # Check for zero variance before calculating correlation\n",
        "            factor_std_dev = group_clean['factor'].std()\n",
        "            ret_std_dev = group_clean[ret_col].std()\n",
        "            if pd.isna(factor_std_dev) or factor_std_dev < 1e-9 or pd.isna(ret_std_dev) or ret_std_dev < 1e-9:\n",
        "                 return 0.0 # Treat constant series as zero correlation\n",
        "\n",
        "            if method == 'spearman':\n",
        "                coeff, p_val = spearmanr(group_clean['factor'], group_clean[ret_col])\n",
        "                return coeff if pd.notna(coeff) else 0.0 # Return 0 if spearman returns NaN\n",
        "            elif method == 'pearson':\n",
        "                coeff = group_clean['factor'].corr(group_clean[ret_col], method='pearson')\n",
        "                return coeff if pd.notna(coeff) else 0.0 # Return 0 if pearson returns NaN\n",
        "            else: return np.nan\n",
        "        except ValueError: # Handle other potential errors (e.g., from spearmanr)\n",
        "             return np.nan\n",
        "\n",
        "    daily_ic = analysis_data.groupby(level='date').apply(ic_calc)\n",
        "    daily_ic.name = f'IC_{method}' # Rename the resulting Series\n",
        "\n",
        "    # Summarize IC\n",
        "    ic_mean = daily_ic.mean()\n",
        "    ic_std = daily_ic.std()\n",
        "    icir = ic_mean / ic_std if pd.notna(ic_std) and ic_std > 1e-9 else np.nan # Avoid div by zero/tiny std\n",
        "    hit_rate = (daily_ic > 1e-9).mean() if not daily_ic.dropna().empty else np.nan # Use > small epsilon for hit rate\n",
        "    obs_days = daily_ic.count() # Count non-NaN IC days\n",
        "\n",
        "    ic_summary = pd.Series({\n",
        "        'Mean IC': ic_mean,\n",
        "        'Std Dev IC': ic_std,\n",
        "        'ICIR': icir,\n",
        "        'Hit Rate (>0)': hit_rate,\n",
        "        'Observations (Days)': obs_days\n",
        "    }, name=ret_col) # Use ret_col as the Series name\n",
        "\n",
        "    return ic_summary, daily_ic.to_frame() # Return daily IC as DataFrame\n",
        "\n",
        "\n",
        "def calculate_factor_returns(analysis_data, factor_display_name, ret_col='1D_fwd_ret'):\n",
        "    \"\"\"Calculates factor returns (e.g., long/short portfolio based on factor).\"\"\"\n",
        "    # analysis_data: Index=(date, asset), Columns=['factor', ret_col]\n",
        "    if ret_col not in analysis_data.columns or 'factor' not in analysis_data.columns:\n",
        "        print(f\"WARN [Factor Returns]: Missing 'factor' or '{ret_col}' for {factor_display_name}\")\n",
        "        return None, None, None, None\n",
        "    if analysis_data.empty or analysis_data[['factor', ret_col]].isna().all().all():\n",
        "         print(f\"WARN [Factor Returns]: Input data empty or all NaN for {factor_display_name}/{ret_col}.\")\n",
        "         return None, None, None, None\n",
        "\n",
        "    # 1. Standardize Factor (cross-sectionally)\n",
        "    factor_std = analysis_data.groupby(level='date')['factor'].transform(\n",
        "        lambda x: (x - x.mean()) / x.std() if pd.notna(x.std()) and x.std() > 1e-9 else (x - x.mean()) # Handle zero/tiny std dev\n",
        "    ).fillna(0) # Fill NaNs after standardization (e.g., single asset days) with 0 weight\n",
        "\n",
        "    # 2. Calculate Weighted Return for each day\n",
        "    analysis_data_temp = analysis_data[[ret_col]].copy() # Only need return col\n",
        "    analysis_data_temp['factor_std'] = factor_std\n",
        "    analysis_data_temp['weighted_ret'] = analysis_data_temp['factor_std'] * analysis_data_temp[ret_col]\n",
        "\n",
        "    # --- Daily Factor Return: Weighted average return ---\n",
        "    # Sum of (weight * return) / Sum of abs(weights) <-- For dollar neutral L/S\n",
        "    sum_weighted_ret = analysis_data_temp.groupby(level='date')['weighted_ret'].sum()\n",
        "    sum_abs_weights = analysis_data_temp.groupby(level='date')['factor_std'].apply(lambda x: x.abs().sum())\n",
        "    # Avoid division by zero/NaN if sum of abs weights is zero/NaN for a day\n",
        "    daily_factor_return = (sum_weighted_ret / sum_abs_weights.replace(0, np.nan)).dropna()\n",
        "    daily_factor_return.name = 'factor_daily_ret'\n",
        "\n",
        "\n",
        "    # 3. Calculate Cumulative Return\n",
        "    cumulative_factor_return = pd.Series(index=daily_factor_return.index, dtype=float)\n",
        "    if not daily_factor_return.empty:\n",
        "        cumulative_factor_return = (1 + daily_factor_return).cumprod() - 1\n",
        "    cumulative_factor_return.name = 'factor_cum_ret'\n",
        "\n",
        "    # 4. Calculate Annualized Statistics\n",
        "    ann_factor = 252 # Assuming 252 trading days per year\n",
        "    num_days = len(daily_factor_return)\n",
        "    ann_ret, ann_vol, sharpe = np.nan, np.nan, np.nan # Defaults\n",
        "    if num_days > 5: # Require min days for meaningful stats\n",
        "         mean_daily_ret = daily_factor_return.mean()\n",
        "         std_daily_ret = daily_factor_return.std()\n",
        "         if pd.notna(mean_daily_ret): ann_ret = mean_daily_ret * ann_factor\n",
        "         if pd.notna(std_daily_ret) and std_daily_ret > 1e-9: # Avoid div by tiny std\n",
        "              ann_vol = std_daily_ret * np.sqrt(ann_factor)\n",
        "              if pd.notna(ann_ret) and ann_vol > 1e-9 : sharpe = ann_ret / ann_vol # Ensure vol > 0\n",
        "\n",
        "    ann_stats = pd.Series({\n",
        "        'Annualized Return': ann_ret,\n",
        "        'Annualized Volatility': ann_vol,\n",
        "        'Sharpe Ratio': sharpe,\n",
        "        'Observations (Days)': num_days\n",
        "    }, name=ret_col) # Use ret_col as the Series name\n",
        "\n",
        "    return daily_factor_return.to_frame(), cumulative_factor_return.to_frame(), None, ann_stats # Placeholder for drawdown\n",
        "\n",
        "# --- FIX IS HERE ---\n",
        "def calculate_forward_returns_for_decay(prices_df, max_lag):\n",
        "    \"\"\"Calculates forward returns for multiple lags up to max_lag.\"\"\"\n",
        "    fwd_rets_dict = {}\n",
        "    print(f\"Calculating fwd returns for decay (1 to {max_lag} days)...\")\n",
        "    if prices_df.empty:\n",
        "         print(\"WARN [Decay FwdRets]: Input prices_df is empty.\")\n",
        "         return fwd_rets_dict\n",
        "\n",
        "    shifted_prices = {lag: prices_df.shift(-lag) for lag in range(1, max_lag + 1)}\n",
        "    with tqdm(total=max_lag, desc=\"Fwd Returns Decay\", leave=False) as pbar: # Set leave=False\n",
        "        for lag in range(1, max_lag + 1):\n",
        "            ret_col_name = f'{lag}D_fwd_ret'\n",
        "            fwd_ret_lag = (shifted_prices[lag] / prices_df - 1).replace([np.inf, -np.inf], np.nan)\n",
        "            # Stack to get (date, asset) index\n",
        "            fwd_ret_stacked = fwd_ret_lag.stack(future_stack=True).rename(ret_col_name)\n",
        "            # <<< FIX: Set index names >>>\n",
        "            fwd_ret_stacked.index.names = ['date', 'asset']\n",
        "            # <<< END FIX >>>\n",
        "            if not fwd_ret_stacked.dropna().empty: # Check not all NaN\n",
        "                fwd_rets_dict[lag] = fwd_ret_stacked.dropna() # Store cleaned series\n",
        "            pbar.update(1)\n",
        "    print(f\"Finished calculating {len(fwd_rets_dict)} forward returns for decay.\")\n",
        "    return fwd_rets_dict # Dict: {lag: Series(Index=(date,asset), Value=ret)}\n",
        "# --- END FIX ---\n",
        "\n",
        "def calculate_ic_decay(factor_series_clean, fwd_returns_for_decay_dict, max_lag, method='spearman'):\n",
        "    \"\"\"Calculates IC decay over multiple forward return periods.\"\"\"\n",
        "    # factor_series_clean: Series, Index=(date, asset), Name='factor'\n",
        "    ic_decay_values = {}\n",
        "    print(\"Calculating IC Decay...\")\n",
        "    if factor_series_clean.empty or not fwd_returns_for_decay_dict:\n",
        "         print(\"WARN [IC Decay]: Factor series empty or no fwd returns provided.\")\n",
        "         return pd.Series(dtype=float, name=f'Mean_IC_{method}_Decay').rename_axis('Lag (Days)')\n",
        "\n",
        "    with tqdm(total=max_lag, desc=\"IC Decay\", leave=False) as pbar: # Set leave=False\n",
        "        for lag in range(1, max_lag + 1):\n",
        "            result_ic = np.nan # Default\n",
        "            if lag in fwd_returns_for_decay_dict:\n",
        "                fwd_ret_lag = fwd_returns_for_decay_dict[lag]\n",
        "                if not fwd_ret_lag.empty:\n",
        "                    # Ensure both series are frames for merge (safer)\n",
        "                    factor_frame = factor_series_clean.to_frame()\n",
        "                    ret_frame = fwd_ret_lag.to_frame()\n",
        "                    # <<< Check index names before merge for debugging >>>\n",
        "                    # print(f\"DEBUG IC Decay Lag {lag}: Factor index names: {factor_frame.index.names}, Ret index names: {ret_frame.index.names}\")\n",
        "                    # <<< End Debug >>>\n",
        "                    try:\n",
        "                        aligned_decay = pd.merge(factor_frame, ret_frame,\n",
        "                                                 left_index=True, right_index=True, how='inner')\n",
        "                        aligned_decay = aligned_decay.dropna() # Drop rows with NaNs in either column\n",
        "\n",
        "                        if len(aligned_decay) > 2: # Need enough points\n",
        "                            # Calculate mean daily IC for this lag\n",
        "                            def ic_calc_decay(group):\n",
        "                                if len(group) < 3: return np.nan\n",
        "                                try:\n",
        "                                     # Check variance again\n",
        "                                    factor_std_dev = group['factor'].std()\n",
        "                                    ret_std_dev = group[fwd_ret_lag.name].std()\n",
        "                                    if pd.isna(factor_std_dev) or factor_std_dev < 1e-9 or pd.isna(ret_std_dev) or ret_std_dev < 1e-9:\n",
        "                                         return 0.0 # Treat constant series as zero correlation\n",
        "\n",
        "                                    if method == 'spearman':\n",
        "                                        coeff, p_val = spearmanr(group['factor'], group[fwd_ret_lag.name])\n",
        "                                        return coeff if pd.notna(coeff) else 0.0\n",
        "                                    elif method == 'pearson':\n",
        "                                        coeff = group['factor'].corr(group[fwd_ret_lag.name], method='pearson')\n",
        "                                        return coeff if pd.notna(coeff) else 0.0\n",
        "                                    else: return np.nan\n",
        "                                except ValueError: return np.nan # Handle other errors (e.g. spearmanr issue)\n",
        "\n",
        "                            daily_ic_lag = aligned_decay.groupby(level='date').apply(ic_calc_decay)\n",
        "                            result_ic = daily_ic_lag.mean() # Store the mean IC for this lag\n",
        "                    except ValueError as e_merge_decay: # Catch specific merge errors\n",
        "                         print(f\"ERROR [IC Decay Lag {lag}]: Merge failed - {e_merge_decay}. Skipping lag.\")\n",
        "                         result_ic = np.nan # Ensure NaN if merge fails\n",
        "                    except Exception as e_decay_calc: # Catch other errors during calculation\n",
        "                         print(f\"ERROR [IC Decay Lag {lag}]: Calculation failed - {e_decay_calc}. Skipping lag.\")\n",
        "                         result_ic = np.nan\n",
        "\n",
        "            ic_decay_values[lag] = result_ic\n",
        "            pbar.update(1)\n",
        "\n",
        "    ic_decay_series = pd.Series(ic_decay_values, name=f'Mean_IC_{method}_Decay')\n",
        "    ic_decay_series.index.name = 'Lag (Days)'\n",
        "    return ic_decay_series\n",
        "\n",
        "# --- END OF ANALYSIS FUNCTION DEFINITIONS ---\n",
        "\n",
        "\n",
        "# --- Helper function to save results to Excel ---\n",
        "# Defined once before the loop starts\n",
        "def save_to_excel_combined(df_to_save, base_sheet_name, factor_disp_name, writer_obj):\n",
        "     \"\"\"Saves a dataframe to a sheet in the combined Excel file, handling naming and timezones.\"\"\"\n",
        "     sheet_name_raw = f\"{factor_disp_name}_{base_sheet_name}\"\n",
        "     if len(sheet_name_raw) > 31:\n",
        "          max_len, len_base, len_underscore = 31, len(base_sheet_name), 1\n",
        "          available_for_factor = max_len - len_base - len_underscore\n",
        "          if available_for_factor < 3: sheet_name = sheet_name_raw[:max_len] # Min 3 chars for factor part\n",
        "          else: sheet_name = f\"{factor_disp_name[:available_for_factor]}_{base_sheet_name}\"\n",
        "          print(f\"WARN: Sheet name '{sheet_name_raw}' > 31 chars. Truncated to '{sheet_name}'.\")\n",
        "     else: sheet_name = sheet_name_raw\n",
        "\n",
        "     if df_to_save is not None and not df_to_save.empty:\n",
        "          try:\n",
        "               df_copy = df_to_save.copy()\n",
        "               # Remove timezone info for Excel compatibility\n",
        "               if isinstance(df_copy.index, pd.DatetimeIndex): df_copy.index = df_copy.index.tz_localize(None)\n",
        "               if isinstance(df_copy.columns, pd.DatetimeIndex): df_copy.columns = df_copy.columns.tz_localize(None)\n",
        "               if isinstance(df_copy.index, pd.MultiIndex):\n",
        "                   new_levels = [lvl.tz_localize(None) if isinstance(lvl, pd.DatetimeIndex) else lvl for lvl in df_copy.index.levels]\n",
        "                   df_copy.index = df_copy.index.set_levels(new_levels)\n",
        "               if isinstance(df_copy.columns, pd.MultiIndex):\n",
        "                   new_levels = [lvl.tz_localize(None) if isinstance(lvl, pd.DatetimeIndex) else lvl for lvl in df_copy.columns.levels]\n",
        "                   df_copy.columns = df_copy.columns.set_levels(new_levels)\n",
        "\n",
        "               df_copy.to_excel(writer_obj, sheet_name=sheet_name)\n",
        "               # print(f\"DEBUG: Saved sheet '{sheet_name}'\") # Optional debug\n",
        "               return True # Indicate sheet was saved\n",
        "          except Exception as e_save: print(f\"ERROR saving sheet '{sheet_name}': {e_save}\")\n",
        "     else: print(f\"INFO: No data to save for sheet '{sheet_name}'.\")\n",
        "     return False # Indicate sheet was not saved\n",
        "\n",
        "\n",
        "# --- Calculate Forward Returns for Analysis & IC Decay (Do ONCE before loop) ---\n",
        "if prices.empty or prices.isna().all().all():\n",
        "    print(\"ERROR: Price data is empty or all NaN. Cannot calculate forward returns. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\nCalculating forward returns for analysis periods: {fwd_ret_periods_int} days...\")\n",
        "forward_returns_df_stacked = calculate_forward_returns(prices.copy(), periods=fwd_ret_periods_int)\n",
        "\n",
        "if forward_returns_df_stacked.empty:\n",
        "    print(\"ERROR: Main forward returns calculation failed or resulted in empty data. Exiting.\")\n",
        "    exit()\n",
        "# Check if expected column names exist\n",
        "expected_ret_cols_present = all(col in forward_returns_df_stacked.columns for col in analysis_periods_str)\n",
        "if not expected_ret_cols_present:\n",
        "     print(f\"ERROR: Missing expected forward return columns in calculated df. Expected: {analysis_periods_str}, Found: {forward_returns_df_stacked.columns.tolist()}\")\n",
        "     # exit() # Exit or proceed carefully\n",
        "\n",
        "print(f\"\\nCalculating forward returns for IC decay (up to {MAX_DECAY_LAG} days)...\")\n",
        "fwd_returns_for_decay_dict = calculate_forward_returns_for_decay(prices.copy(), MAX_DECAY_LAG)\n",
        "if not fwd_returns_for_decay_dict:\n",
        "    print(\"WARN: Could not calculate forward returns for IC Decay. Decay analysis will be skipped.\")\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# === Starting Factor Analysis Loop ===\n",
        "# =======================================================\n",
        "print(f\"\\nPreparing single Excel output file: {combined_output_filename}\\n\")\n",
        "\n",
        "if precalculated_factors_df.empty:\n",
        "    print(\"ERROR: No pre-calculated factors found or loaded. Skipping analysis loop.\")\n",
        "else:\n",
        "    # --- Start Excel Writer context ---\n",
        "    overall_success = False # Flag to track if ANY sheet gets written\n",
        "    try:\n",
        "        with pd.ExcelWriter(combined_output_filename, engine='openpyxl') as writer:\n",
        "            unique_factor_names = precalculated_factors_df.columns.get_level_values('factor_name').unique()\n",
        "            print(f\"Analyzing {len(unique_factor_names)} factors found in the input DataFrame...\")\n",
        "\n",
        "            # --- Loop through each factor ---\n",
        "            for factor_name in unique_factor_names:\n",
        "                print(f\"\\n\\n{'='*20} Processing Factor: {factor_name} {'='*20}\")\n",
        "                factor_timer_start = time.time() # Timer for each factor\n",
        "                sheets_saved_this_factor = 0 # Count sheets for this factor\n",
        "\n",
        "                # --- Extract Raw Factor ---\n",
        "                try:\n",
        "                    raw_factor_df = precalculated_factors_df.xs(factor_name, level='factor_name', axis=1).copy()\n",
        "                    raw_factor_df.columns.name = 'asset'\n",
        "                    raw_factor_df.index.name = 'date'\n",
        "                except KeyError:\n",
        "                     print(f\"ERROR: Could not extract factor '{factor_name}' using xs. Skipping.\")\n",
        "                     continue\n",
        "                except Exception as e_extract:\n",
        "                     print(f\"ERROR: Unexpected error extracting factor '{factor_name}': {e_extract}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                if raw_factor_df.empty or raw_factor_df.isna().all().all():\n",
        "                    print(f\"WARN: Raw factor data for {factor_name} is empty or all NaNs after extraction. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # --- Factor Neutralization ---\n",
        "                print(\"\\n--- Performing Factor Neutralization ---\")\n",
        "                neut_timer_start = time.time()\n",
        "                neutralized_factor_df = pd.DataFrame(index=dates_index, columns=assets) # Reinitialize\n",
        "                neutralization_succeeded = False\n",
        "\n",
        "                # Check if any neutralization variables exist and align them\n",
        "                has_industry = False\n",
        "                industry_dummies_aligned = pd.DataFrame()\n",
        "                if 'industry_dummies_static' in locals() and not industry_dummies_static.empty:\n",
        "                     # Align index (assets) with the current final asset list\n",
        "                     industry_dummies_aligned = industry_dummies_static.reindex(assets).fillna(0)\n",
        "                     has_industry = not industry_dummies_aligned.empty\n",
        "\n",
        "                has_style = False\n",
        "                style_factors_aligned = pd.DataFrame()\n",
        "                if 'style_factors' in locals() and not style_factors.empty and not style_factors.isna().all().all():\n",
        "                     # Align style factors (which have MultiIndex date,asset) with raw_factor_df dates\n",
        "                     # And ensure assets match the final 'assets' list\n",
        "                     style_factors_aligned = style_factors.reindex(index=raw_factor_df.index, level='date')\n",
        "                     # Filter style factors to only include current assets\n",
        "                     valid_style_assets = style_factors_aligned.index.get_level_values('asset').unique().intersection(assets)\n",
        "                     if not valid_style_assets.empty:\n",
        "                         style_factors_aligned = style_factors_aligned[style_factors_aligned.index.get_level_values('asset').isin(valid_style_assets)]\n",
        "                         has_style = not style_factors_aligned.dropna(how='all').empty\n",
        "                     else: has_style = False\n",
        "\n",
        "\n",
        "                if not has_industry and not has_style:\n",
        "                    print(\"INFO: No neutralization variables available. Using raw factor.\")\n",
        "                    neutralized_factor_df = raw_factor_df.copy()\n",
        "                    neutralization_succeeded = False\n",
        "                else:\n",
        "                    print(\"Running neutralization regression day by day...\")\n",
        "                    neutralized_residuals_list = []\n",
        "                    # Use index from raw factor that has *some* data for iteration\n",
        "                    valid_dates_for_neut = raw_factor_df.dropna(how='all').index\n",
        "\n",
        "                    with tqdm(total=len(valid_dates_for_neut), desc=f\"Neutralizing {factor_name}\", leave=False) as pbar:\n",
        "                        for date in valid_dates_for_neut:\n",
        "                            factor_today = raw_factor_df.loc[date].dropna()\n",
        "                            if factor_today.empty:\n",
        "                                neutralized_residuals_list.append(pd.Series(np.nan, index=assets, name=date))\n",
        "                                pbar.update(1); continue\n",
        "\n",
        "                            X_list = []\n",
        "                            valid_assets_today = factor_today.index\n",
        "\n",
        "                            # Industry\n",
        "                            if has_industry:\n",
        "                                industry_today = industry_dummies_aligned.reindex(valid_assets_today).dropna(axis=1, how='all').fillna(0)\n",
        "                                # Drop industry dummies that are constant (e.g., all zero after reindex)\n",
        "                                industry_today = industry_today.loc[:, industry_today.nunique() > 1]\n",
        "                                if not industry_today.empty: X_list.append(industry_today)\n",
        "\n",
        "                            # Style Factors\n",
        "                            style_today_aligned_assets = pd.DataFrame() # Init empty\n",
        "                            if has_style and date in style_factors_aligned.index.get_level_values('date'):\n",
        "                                try:\n",
        "                                    style_today = style_factors_aligned.loc[pd.IndexSlice[date, :], :] # Use IndexSlice for robustness\n",
        "                                    if not style_today.empty:\n",
        "                                         # If only one style factor, it might be a Series, convert to frame\n",
        "                                         if isinstance(style_today, pd.Series): style_today = style_today.to_frame()\n",
        "\n",
        "                                         # Reindex style factors for today's valid assets and fill NaNs (e.g., with mean)\n",
        "                                         style_fill_value = style_today.mean() # Calculate mean before reindexing\n",
        "                                         style_today_aligned_assets = style_today.reindex(valid_assets_today, level='asset').fillna(style_fill_value)\n",
        "                                         # Drop style factors that are all NaN after reindexing/filling\n",
        "                                         style_today_aligned_assets = style_today_aligned_assets.dropna(axis=1, how='all') # Drop empty columns\n",
        "                                         if not style_today_aligned_assets.empty:\n",
        "                                             # Remove constant columns (important!) before adding model constant\n",
        "                                             non_const_cols = style_today_aligned_assets.loc[:, style_today_aligned_assets.nunique() > 1]\n",
        "                                             if not non_const_cols.empty: X_list.append(non_const_cols)\n",
        "                                except KeyError: pass # Date might not exist in aligned style factors\n",
        "                                except Exception as e_style_align:\n",
        "                                     print(f\"WARN: Error aligning style factors for {date}: {e_style_align}\")\n",
        "\n",
        "\n",
        "                            if not X_list:\n",
        "                                residuals_today = factor_today\n",
        "                            else:\n",
        "                                try:\n",
        "                                    X_today = pd.concat(X_list, axis=1).astype(float) # Ensure float type\n",
        "                                    # Align Y (factor) and X (exposures) on common assets\n",
        "                                    common_assets = factor_today.index.intersection(X_today.index)\n",
        "                                    if common_assets.empty: # Handle case where no assets overlap after considering exposures\n",
        "                                         residuals_today = factor_today # Fallback to raw\n",
        "                                    else:\n",
        "                                        Y_aligned = factor_today.loc[common_assets].astype(float)\n",
        "                                        X_aligned = X_today.loc[common_assets]\n",
        "\n",
        "                                        # Drop rows/cols with all NaNs AFTER alignment (robustness)\n",
        "                                        X_aligned = X_aligned.dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
        "                                        Y_aligned = Y_aligned.loc[X_aligned.index] # Re-align Y\n",
        "\n",
        "                                        # Check for sufficient data points vs predictors\n",
        "                                        if Y_aligned.empty or X_aligned.empty or len(Y_aligned) <= X_aligned.shape[1]:\n",
        "                                            residuals_today = factor_today # Fallback\n",
        "                                        else:\n",
        "                                            X_w_const = sm.add_constant(X_aligned, has_constant='add')\n",
        "                                            model = sm.OLS(Y_aligned, X_w_const, missing='drop')\n",
        "                                            results = model.fit()\n",
        "                                            residuals_today = results.resid.reindex(Y_aligned.index).fillna(0) # Fill NaNs from regression with 0? Or keep NaN?\n",
        "\n",
        "                                except LinAlgError: # Handle cases like singular matrix\n",
        "                                     residuals_today = factor_today\n",
        "                                except ValueError as e_ols_val: # Handle dimension mismatches etc.\n",
        "                                     residuals_today = factor_today\n",
        "                                except Exception as e_ols:\n",
        "                                     print(f\"WARN: OLS failed unexpectedly for {factor_name} on {date}: {e_ols}\")\n",
        "                                     residuals_today = factor_today # Fallback to raw on error\n",
        "\n",
        "                            # Reindex residuals to full asset list, filling missing ones with NaN\n",
        "                            neutralized_residuals_list.append(residuals_today.reindex(assets).fillna(np.nan))\n",
        "                            pbar.update(1)\n",
        "\n",
        "                    # --- Combine daily neutralized results ---\n",
        "                    if neutralized_residuals_list:\n",
        "                         neutralized_factor_df_temp = pd.concat(neutralized_residuals_list, axis=1).T\n",
        "                         neutralized_factor_df_temp.index.name = 'date'\n",
        "                         # Reindex to ensure all analysis dates are present (fills missing dates with NaN)\n",
        "                         neutralized_factor_df = neutralized_factor_df_temp.reindex(dates_index)\n",
        "                         neutralization_succeeded = True\n",
        "                         print(f\"Neutralization completed for {factor_name}. ({(time.time() - neut_timer_start):.2f}s)\")\n",
        "                    else:\n",
        "                         print(f\"WARN: Neutralization yielded no results for {factor_name}. Using raw factor.\")\n",
        "                         neutralized_factor_df = raw_factor_df.copy()\n",
        "                         neutralization_succeeded = False\n",
        "\n",
        "\n",
        "                # --- Analysis Execution ---\n",
        "                print(\"\\n--- Starting Factor Analysis ---\")\n",
        "                analysis_timer_start = time.time()\n",
        "                factor_to_analyze_df = None\n",
        "                factor_source = \"None\"\n",
        "\n",
        "                # Decide which factor version to use for analysis\n",
        "                if neutralization_succeeded and not neutralized_factor_df.isna().all().all():\n",
        "                    factor_to_analyze_df = neutralized_factor_df.copy()\n",
        "                    factor_source = \"Neut\" # Shortened for sheet names\n",
        "                    print(f\"INFO: Using NEUTRALIZED factor '{factor_name}' for analysis.\")\n",
        "                elif not raw_factor_df.isna().all().all():\n",
        "                    factor_to_analyze_df = raw_factor_df.copy()\n",
        "                    factor_source = \"Raw\"\n",
        "                    print(f\"INFO: Using RAW factor '{factor_name}' for analysis.\")\n",
        "                else:\n",
        "                    print(f\"ERROR: No valid factor data (Raw or Neutralized) found for {factor_name}. Skipping analysis.\")\n",
        "                    continue\n",
        "\n",
        "                # --- Prepare for Analysis ---\n",
        "                analysis_performed = False\n",
        "                # Factor display name for sheet naming - keep it concise\n",
        "                factor_display_name = f\"{factor_name[:15]}_{factor_source}\" # Max 15 chars for factor part\n",
        "\n",
        "                # Stack the chosen factor (Index=date, Columns=assets) -> Series (Index=(date, asset))\n",
        "                factor_to_analyze_df.index.name = 'date'\n",
        "                factor_to_analyze_df.columns.name = 'asset'\n",
        "                factor_series = factor_to_analyze_df.stack(future_stack=True) # Use future_stack, dropna removed\n",
        "                factor_series.index.names = ['date', 'asset']\n",
        "                factor_series.rename('factor', inplace=True) # Ensure Series name is 'factor' for functions\n",
        "                factor_series_clean = factor_series.dropna() # Drop NaNs *after* stacking\n",
        "\n",
        "                # Align factor with forward returns (already stacked)\n",
        "                aligned_data = pd.DataFrame()\n",
        "                if factor_series_clean.empty:\n",
        "                    print(f\"ERROR: Factor series for {factor_display_name} is empty after dropna(). Skipping.\")\n",
        "                    continue\n",
        "                else:\n",
        "                    # Ensure forward returns are uniquely indexed if merging\n",
        "                    fwd_returns_unique = forward_returns_df_stacked[~forward_returns_df_stacked.index.duplicated(keep='first')]\n",
        "                    try:\n",
        "                        # Merge the factor Series (as frame) with the forward returns DataFrame\n",
        "                        aligned_data = pd.merge(factor_series_clean.to_frame(), fwd_returns_unique,\n",
        "                                                left_index=True, right_index=True, how='inner')\n",
        "                    except Exception as merge_err:\n",
        "                        print(f\"ERROR aligning data for {factor_display_name}: {merge_err}\")\n",
        "                        continue\n",
        "\n",
        "                # Final check on aligned data\n",
        "                # Drop rows where factor OR *any* of the analysis return periods are NaN\n",
        "                aligned_data_clean = aligned_data.dropna(subset=['factor'] + analysis_periods_str, how='any')\n",
        "\n",
        "                if aligned_data_clean.empty:\n",
        "                    print(f\"INFO: Skipping analysis for {factor_display_name} - no overlapping data.\")\n",
        "                    continue\n",
        "                else:\n",
        "                    print(f\"Clean aligned data ready for {factor_display_name}. Shape: {aligned_data_clean.shape}\")\n",
        "                    # Identify return columns actually available after merge/dropna\n",
        "                    available_ret_cols = [col for col in analysis_periods_str if col in aligned_data_clean.columns and not aligned_data_clean[col].isna().all()]\n",
        "                    if not available_ret_cols:\n",
        "                        print(f\"ERROR: No valid forward returns columns remain for {factor_display_name}. Skipping.\")\n",
        "                        continue\n",
        "\n",
        "                    # --- Initialize result containers ---\n",
        "                    all_ic_summaries, all_daily_ics = [], {}\n",
        "                    all_quantile_mean_rets, all_quantile_cum_rets = {}, {}\n",
        "                    factor_daily_returns_dict, cumulative_factor_returns_dict = {}, {}\n",
        "                    factor_analysis_summary = []\n",
        "\n",
        "                    # --- Calculate IC Decay ---\n",
        "                    ic_decay_results = pd.Series(dtype=float)\n",
        "                    if fwd_returns_for_decay_dict:\n",
        "                         # Pass the clean factor series (before alignment with specific returns)\n",
        "                         ic_decay_results = calculate_ic_decay(factor_series_clean.copy(), fwd_returns_for_decay_dict, MAX_DECAY_LAG, method=ic_method)\n",
        "\n",
        "                    # --- Calculate Quantile Turnover ---\n",
        "                    all_quantile_turnover = pd.DataFrame()\n",
        "                    print(\"\\n--- Calculating Quantile Turnover ---\")\n",
        "                    # Pass factor from aligned data, only need 'factor' column\n",
        "                    quantile_assignments = get_quantile_assignments(aligned_data_clean[['factor']].copy(), num_quantiles=num_quantiles)\n",
        "                    if not quantile_assignments.empty and not quantile_assignments['quantile'].isna().all():\n",
        "                        all_quantile_turnover = calculate_quantile_turnover(quantile_assignments, num_quantiles=num_quantiles)\n",
        "                    else: print(\"WARN: Could not calculate turnover due to empty/NaN quantile assignments.\")\n",
        "\n",
        "\n",
        "                    # --- Loop through Analysis Periods ---\n",
        "                    for ret_col in available_ret_cols:\n",
        "                        print(f\"\\n===== Analyzing {factor_display_name} vs {ret_col} =====\")\n",
        "                        # Subset data needed for this specific return period\n",
        "                        analysis_data_subset = aligned_data_clean[['factor', ret_col]].dropna()\n",
        "                        if analysis_data_subset.empty:\n",
        "                            print(f\"INFO: No valid data for {factor_display_name} vs {ret_col} after dropna.\")\n",
        "                            continue\n",
        "\n",
        "                        # --- Run Analyses ---\n",
        "                        try: # Add try-except around individual analyses\n",
        "                            mean_ret_q, cum_ret_q = quantile_analysis(analysis_data_subset.copy(), factor_display_name, num_quantiles=num_quantiles, ret_col=ret_col)\n",
        "                            if mean_ret_q is not None: all_quantile_mean_rets[ret_col] = mean_ret_q\n",
        "                            if cum_ret_q is not None: all_quantile_cum_rets[ret_col] = cum_ret_q\n",
        "\n",
        "                            ic_summary, daily_ic = calculate_ic(analysis_data_subset.copy(), factor_display_name, ret_col=ret_col, method=ic_method)\n",
        "                            if ic_summary is not None: all_ic_summaries.append(ic_summary)\n",
        "                            if daily_ic is not None: all_daily_ics[ret_col] = daily_ic\n",
        "\n",
        "                            factor_daily_ret, factor_cum_ret, _, factor_ann_stats = calculate_factor_returns(\n",
        "                                analysis_data_subset.copy(), factor_display_name, ret_col=ret_col\n",
        "                            )\n",
        "                            if factor_daily_ret is not None: factor_daily_returns_dict[ret_col] = factor_daily_ret\n",
        "                            if factor_cum_ret is not None: cumulative_factor_returns_dict[ret_col] = factor_cum_ret\n",
        "                            if factor_ann_stats is not None: factor_analysis_summary.append(factor_ann_stats)\n",
        "\n",
        "                            analysis_performed = True # Mark that at least one analysis ran\n",
        "                        except Exception as e_analyze_period:\n",
        "                             print(f\"ERROR during analysis of {factor_display_name} vs {ret_col}: {e_analyze_period}\")\n",
        "                             traceback.print_exc() # Print detailed error for this period\n",
        "\n",
        "                print(f\"Analysis calculations finished. ({(time.time() - analysis_timer_start):.2f}s)\")\n",
        "\n",
        "                # --- Save Results ---\n",
        "                if analysis_performed:\n",
        "                    print(f\"\\n--- Saving results for {factor_display_name} to Excel ---\")\n",
        "                    save_timer_start = time.time()\n",
        "                    # --- Save each result type using the helper defined outside the loop ---\n",
        "                    if all_ic_summaries: sheets_saved_this_factor += save_to_excel_combined(pd.concat(all_ic_summaries, axis=1), 'IC_Sum', factor_display_name, writer)\n",
        "                    if all_daily_ics: sheets_saved_this_factor += save_to_excel_combined(pd.concat(all_daily_ics, axis=1), 'IC_Daily', factor_display_name, writer)\n",
        "                    if 'ic_decay_results' in locals() and not ic_decay_results.empty: sheets_saved_this_factor += save_to_excel_combined(ic_decay_results.to_frame(), 'IC_Decay', factor_display_name, writer)\n",
        "                    if all_quantile_mean_rets: sheets_saved_this_factor += save_to_excel_combined(pd.concat(all_quantile_mean_rets, axis=1, join='outer').rename_axis('Quantile'), 'Q_MeanRet', factor_display_name, writer)\n",
        "                    if all_quantile_cum_rets:\n",
        "                        all_dfs_cum = []\n",
        "                        for ret_p, cum_df in all_quantile_cum_rets.items():\n",
        "                            if cum_df is not None and not cum_df.empty:\n",
        "                                cum_df.columns.name = 'Quantile'; cum_df.columns = pd.MultiIndex.from_product([[ret_p], cum_df.columns], names=['Return_Period', 'Quantile'])\n",
        "                                all_dfs_cum.append(cum_df)\n",
        "                        sheets_saved_this_factor += save_to_excel_combined(pd.concat(all_dfs_cum, axis=1, join='outer') if all_dfs_cum else pd.DataFrame(), 'Q_CumRet', factor_display_name, writer)\n",
        "                    if 'all_quantile_turnover' in locals() and not all_quantile_turnover.empty: sheets_saved_this_factor += save_to_excel_combined(all_quantile_turnover, 'Q_Turnover', factor_display_name, writer)\n",
        "                    if factor_analysis_summary: sheets_saved_this_factor += save_to_excel_combined(pd.concat(factor_analysis_summary, axis=1).rename_axis('Metric'), 'Fctr_Stats', factor_display_name, writer)\n",
        "                    if factor_daily_returns_dict: sheets_saved_this_factor += save_to_excel_combined(pd.concat(factor_daily_returns_dict, axis=1), 'Fctr_Ret', factor_display_name, writer)\n",
        "                    if cumulative_factor_returns_dict: sheets_saved_this_factor += save_to_excel_combined(pd.concat(cumulative_factor_returns_dict, axis=1), 'Fctr_CumRet', factor_display_name, writer)\n",
        "\n",
        "                    if sheets_saved_this_factor > 0:\n",
        "                        print(f\"--- Results for {factor_display_name} saved ({sheets_saved_this_factor} sheets). ({(time.time() - save_timer_start):.2f}s)---\")\n",
        "                        overall_success = True # Mark that at least one sheet was saved overall\n",
        "                    else:\n",
        "                        print(f\"--- No data frames were valid for saving for {factor_display_name}. ---\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"\\n--- No analysis performed for factor '{factor_display_name}'. No results saved. ---\")\n",
        "\n",
        "                print(f\"--- Factor {factor_name} processing time: {(time.time() - factor_timer_start):.2f}s ---\")\n",
        "            # --- End of loop through factors ---\n",
        "\n",
        "            if overall_success:\n",
        "                 print(f\"\\nAll factors processed. Finalizing Excel file: {combined_output_filename}\")\n",
        "            else:\n",
        "                 print(f\"\\nWARNING: All factors processed, but no analysis results were generated or saved.\")\n",
        "\n",
        "            # ExcelWriter context manager handles saving on exit IF overall_success is True implicitly\n",
        "\n",
        "    # --- End of Excel Writer context ---\n",
        "    except ImportError:\n",
        "        print(\"\\nERROR: Could not prepare Excel file. `openpyxl` library not found.\")\n",
        "        print(\"Please install it: pip install openpyxl\")\n",
        "    except Exception as e_main_loop:\n",
        "        print(f\"\\nERROR occurred during factor analysis loop or Excel writing: {e_main_loop}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# --- End of Script ---\n",
        "print(\"\\n=============================================\")\n",
        "print(\"=== Combined Factor Analysis Script Finished ===\")\n",
        "print(\"=============================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
