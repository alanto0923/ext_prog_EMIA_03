{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Am0RlC30KL"
      },
      "source": [
        "# Data Acquisition (Research Papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krQd3D9r0oAt",
        "outputId": "579c9047-755d-4274-9202-2fece4273164"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: xmltodict in /opt/anaconda3/lib/python3.12/site-packages (0.14.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests xmltodict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBMVU7IF3xMX",
        "outputId": "b32d29f2-1c64-4e36-a44d-c69ee669fb20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.3)\n",
            "Requirement already satisfied: PyPDF2 in /opt/anaconda3/lib/python3.12/site-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.5)\n",
            "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.3.3)\n",
            "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.12/site-packages (0.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests PyPDF2 nltk spacy gensim textblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c_eI7j-4Dsm",
        "outputId": "318d3612-5976-49ce-f4d8-22e07c3f5762"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-15 19:22:24,289 - WARNING - Found existing database 'arxiv_papers.db'. Deleting it to ensure the correct schema is applied.\n",
            "2025-07-15 19:22:24,290 - INFO - --- Starting search for keyword: 'alpha factors' ---\n",
            "2025-07-15 19:22:25,495 - INFO - Successfully fetched 5 entries for query: 'alpha factors'\n",
            "2025-07-15 19:22:25,500 - INFO - Processing paper ID: 2406.18394v5\n",
            "2025-07-15 19:22:29,559 - INFO - Downloaded PDF: arxiv_pdfs/2406.18394v5.pdf\n",
            "2025-07-15 19:22:32,129 - INFO - Stored/Updated paper 2406.18394v5 in the database.\n",
            "2025-07-15 19:22:32,129 - INFO - Processing paper ID: 2409.05144v3\n",
            "2025-07-15 19:22:34,416 - INFO - Downloaded PDF: arxiv_pdfs/2409.05144v3.pdf\n",
            "2025-07-15 19:22:39,018 - INFO - Stored/Updated paper 2409.05144v3 in the database.\n",
            "2025-07-15 19:22:39,018 - INFO - Processing paper ID: 1210.5413v1\n",
            "2025-07-15 19:22:41,796 - INFO - Downloaded PDF: arxiv_pdfs/1210.5413v1.pdf\n",
            "2025-07-15 19:22:43,262 - INFO - Stored/Updated paper 1210.5413v1 in the database.\n",
            "2025-07-15 19:22:43,263 - INFO - Processing paper ID: 2401.02710v2\n",
            "2025-07-15 19:22:45,237 - INFO - Downloaded PDF: arxiv_pdfs/2401.02710v2.pdf\n",
            "2025-07-15 19:22:46,468 - INFO - Stored/Updated paper 2401.02710v2 in the database.\n",
            "2025-07-15 19:22:46,469 - INFO - Processing paper ID: 1301.0533v1\n",
            "2025-07-15 19:22:47,816 - INFO - Downloaded PDF: arxiv_pdfs/1301.0533v1.pdf\n",
            "2025-07-15 19:22:50,543 - INFO - Stored/Updated paper 1301.0533v1 in the database.\n",
            "2025-07-15 19:22:50,543 - INFO - --- Starting search for keyword: 'alpha generation' ---\n",
            "2025-07-15 19:22:51,612 - INFO - Successfully fetched 5 entries for query: 'alpha generation'\n",
            "2025-07-15 19:22:51,614 - INFO - Processing paper ID: 1209.1519v1\n",
            "2025-07-15 19:22:53,603 - INFO - Downloaded PDF: arxiv_pdfs/1209.1519v1.pdf\n",
            "2025-07-15 19:22:55,762 - INFO - Stored/Updated paper 1209.1519v1 in the database.\n",
            "2025-07-15 19:22:55,763 - INFO - Processing paper ID: 2306.12964v1\n",
            "2025-07-15 19:22:57,734 - INFO - Downloaded PDF: arxiv_pdfs/2306.12964v1.pdf\n",
            "2025-07-15 19:23:00,456 - INFO - Stored/Updated paper 2306.12964v1 in the database.\n",
            "2025-07-15 19:23:00,457 - INFO - Processing paper ID: 1405.4505v2\n",
            "2025-07-15 19:23:01,558 - INFO - Downloaded PDF: arxiv_pdfs/1405.4505v2.pdf\n",
            "2025-07-15 19:23:03,553 - INFO - Stored/Updated paper 1405.4505v2 in the database.\n",
            "2025-07-15 19:23:03,554 - INFO - Processing paper ID: 1303.1293v1\n",
            "2025-07-15 19:23:04,959 - INFO - Downloaded PDF: arxiv_pdfs/1303.1293v1.pdf\n",
            "2025-07-15 19:23:05,996 - INFO - Stored/Updated paper 1303.1293v1 in the database.\n",
            "2025-07-15 19:23:05,996 - INFO - Processing paper ID: 1806.08469v1\n",
            "2025-07-15 19:23:07,634 - INFO - Downloaded PDF: arxiv_pdfs/1806.08469v1.pdf\n",
            "2025-07-15 19:23:08,993 - INFO - Stored/Updated paper 1806.08469v1 in the database.\n",
            "2025-07-15 19:23:08,993 - INFO - --- Starting search for keyword: 'alpha mining' ---\n",
            "2025-07-15 19:23:10,167 - INFO - Successfully fetched 5 entries for query: 'alpha mining'\n",
            "2025-07-15 19:23:10,168 - INFO - Processing paper ID: 2308.00016v1\n",
            "2025-07-15 19:23:12,911 - INFO - Downloaded PDF: arxiv_pdfs/2308.00016v1.pdf\n",
            "2025-07-15 19:23:14,833 - INFO - Stored/Updated paper 2308.00016v1 in the database.\n",
            "2025-07-15 19:23:14,833 - INFO - Processing paper ID: 2402.07080v2\n",
            "2025-07-15 19:23:16,275 - INFO - Downloaded PDF: arxiv_pdfs/2402.07080v2.pdf\n",
            "2025-07-15 19:23:18,985 - INFO - Stored/Updated paper 2402.07080v2 in the database.\n",
            "2025-07-15 19:23:18,985 - INFO - Processing paper ID: 2502.16789v2\n",
            "2025-07-15 19:23:20,981 - INFO - Downloaded PDF: arxiv_pdfs/2502.16789v2.pdf\n",
            "2025-07-15 19:23:23,904 - INFO - Stored/Updated paper 2502.16789v2 in the database.\n",
            "2025-07-15 19:23:23,905 - INFO - Processing paper ID: 2402.09746v1\n",
            "2025-07-15 19:23:25,393 - INFO - Downloaded PDF: arxiv_pdfs/2402.09746v1.pdf\n",
            "2025-07-15 19:23:26,156 - INFO - Stored/Updated paper 2402.09746v1 in the database.\n",
            "2025-07-15 19:23:26,156 - INFO - Processing paper ID: 2505.11122v1\n",
            "2025-07-15 19:23:28,812 - INFO - Downloaded PDF: arxiv_pdfs/2505.11122v1.pdf\n",
            "2025-07-15 19:23:33,417 - INFO - Stored/Updated paper 2505.11122v1 in the database.\n",
            "2025-07-15 19:23:33,417 - INFO - --- Starting search for keyword: 'factor investing' ---\n",
            "2025-07-15 19:23:34,826 - INFO - Successfully fetched 5 entries for query: 'factor investing'\n",
            "2025-07-15 19:23:34,828 - INFO - Processing paper ID: 2210.12462v1\n",
            "2025-07-15 19:23:36,686 - INFO - Downloaded PDF: arxiv_pdfs/2210.12462v1.pdf\n",
            "2025-07-15 19:23:41,129 - INFO - Stored/Updated paper 2210.12462v1 in the database.\n",
            "2025-07-15 19:23:41,129 - INFO - Processing paper ID: 2507.00332v1\n",
            "2025-07-15 19:23:42,607 - INFO - Downloaded PDF: arxiv_pdfs/2507.00332v1.pdf\n",
            "2025-07-15 19:23:43,567 - INFO - Stored/Updated paper 2507.00332v1 in the database.\n",
            "2025-07-15 19:23:43,567 - INFO - Processing paper ID: 2305.16364v1\n",
            "2025-07-15 19:23:45,679 - INFO - Downloaded PDF: arxiv_pdfs/2305.16364v1.pdf\n",
            "2025-07-15 19:23:49,014 - INFO - Stored/Updated paper 2305.16364v1 in the database.\n",
            "2025-07-15 19:23:49,014 - INFO - Processing paper ID: 2505.01921v2\n",
            "2025-07-15 19:23:52,586 - INFO - Downloaded PDF: arxiv_pdfs/2505.01921v2.pdf\n",
            "2025-07-15 19:23:57,288 - INFO - Stored/Updated paper 2505.01921v2 in the database.\n",
            "2025-07-15 19:23:57,289 - INFO - Processing paper ID: 2007.04838v1\n",
            "2025-07-15 19:24:00,090 - INFO - Downloaded PDF: arxiv_pdfs/2007.04838v1.pdf\n",
            "2025-07-15 19:24:12,300 - INFO - Stored/Updated paper 2007.04838v1 in the database.\n",
            "2025-07-15 19:24:12,301 - INFO - --- Starting search for keyword: 'formulaic alphas' ---\n",
            "2025-07-15 19:24:13,597 - INFO - Successfully fetched 5 entries for query: 'formulaic alphas'\n",
            "2025-07-15 19:24:13,599 - INFO - Processing paper ID: 2002.08245v2\n",
            "2025-07-15 19:24:15,340 - INFO - Downloaded PDF: arxiv_pdfs/2002.08245v2.pdf\n",
            "2025-07-15 19:24:17,502 - INFO - Stored/Updated paper 2002.08245v2 in the database.\n",
            "2025-07-15 19:24:17,503 - INFO - Processing paper ID: 2401.02710v2\n",
            "2025-07-15 19:24:19,144 - INFO - Downloaded PDF: arxiv_pdfs/2401.02710v2.pdf\n",
            "2025-07-15 19:24:20,397 - INFO - Stored/Updated paper 2401.02710v2 in the database.\n",
            "2025-07-15 19:24:20,398 - INFO - Processing paper ID: 2306.12964v1\n",
            "2025-07-15 19:24:22,594 - INFO - Downloaded PDF: arxiv_pdfs/2306.12964v1.pdf\n",
            "2025-07-15 19:24:25,509 - INFO - Stored/Updated paper 2306.12964v1 in the database.\n",
            "2025-07-15 19:24:25,509 - INFO - Processing paper ID: 2406.16505v2\n",
            "2025-07-15 19:24:27,353 - INFO - Downloaded PDF: arxiv_pdfs/2406.16505v2.pdf\n",
            "2025-07-15 19:24:29,108 - INFO - Stored/Updated paper 2406.16505v2 in the database.\n",
            "2025-07-15 19:24:29,108 - INFO - Processing paper ID: 2402.07080v2\n",
            "2025-07-15 19:24:30,492 - INFO - Downloaded PDF: arxiv_pdfs/2402.07080v2.pdf\n",
            "2025-07-15 19:24:33,221 - INFO - Stored/Updated paper 2402.07080v2 in the database.\n",
            "2025-07-15 19:24:33,222 - INFO - --- Starting search for keyword: 'stock price prediction' ---\n",
            "2025-07-15 19:24:34,066 - INFO - Successfully fetched 5 entries for query: 'stock price prediction'\n",
            "2025-07-15 19:24:34,068 - INFO - Processing paper ID: 2405.10584v1\n",
            "2025-07-15 19:24:38,073 - INFO - Downloaded PDF: arxiv_pdfs/2405.10584v1.pdf\n",
            "2025-07-15 19:24:41,823 - INFO - Stored/Updated paper 2405.10584v1 in the database.\n",
            "2025-07-15 19:24:41,824 - INFO - Processing paper ID: 1603.07893v3\n",
            "2025-07-15 19:24:42,719 - INFO - Downloaded PDF: arxiv_pdfs/1603.07893v3.pdf\n",
            "2025-07-15 19:24:43,109 - INFO - Stored/Updated paper 1603.07893v3 in the database.\n",
            "2025-07-15 19:24:43,110 - INFO - Processing paper ID: 2002.06975v1\n",
            "2025-07-15 19:24:44,485 - INFO - Downloaded PDF: arxiv_pdfs/2002.06975v1.pdf\n",
            "2025-07-15 19:24:46,693 - INFO - Stored/Updated paper 2002.06975v1 in the database.\n",
            "2025-07-15 19:24:46,694 - INFO - Processing paper ID: 2308.11117v1\n",
            "2025-07-15 19:24:48,303 - INFO - Downloaded PDF: arxiv_pdfs/2308.11117v1.pdf\n",
            "2025-07-15 19:24:50,668 - INFO - Stored/Updated paper 2308.11117v1 in the database.\n",
            "2025-07-15 19:24:50,668 - INFO - Processing paper ID: 2308.13642v1\n",
            "2025-07-15 19:24:52,725 - INFO - Downloaded PDF: arxiv_pdfs/2308.13642v1.pdf\n",
            "2025-07-15 19:24:54,077 - INFO - Stored/Updated paper 2308.13642v1 in the database.\n",
            "2025-07-15 19:24:54,078 - INFO - --- Starting search for keyword: 'artifical intelligence trading' ---\n",
            "2025-07-15 19:24:55,306 - INFO - Successfully fetched 0 entries for query: 'artifical intelligence trading'\n",
            "2025-07-15 19:24:55,313 - INFO - --- Starting search for keyword: 'AI stock selection' ---\n",
            "2025-07-15 19:24:56,367 - INFO - Successfully fetched 0 entries for query: 'AI stock selection'\n",
            "2025-07-15 19:24:56,368 - INFO - --- Starting search for keyword: 'large language models finance' ---\n",
            "2025-07-15 19:24:57,367 - INFO - Successfully fetched 0 entries for query: 'large language models finance'\n",
            "2025-07-15 19:24:57,370 - INFO - --- Starting search for keyword: 'reinforcement learning portfolio' ---\n",
            "2025-07-15 19:24:58,215 - INFO - Successfully fetched 3 entries for query: 'reinforcement learning portfolio'\n",
            "2025-07-15 19:24:58,217 - INFO - Processing paper ID: 2503.04143v1\n",
            "2025-07-15 19:25:00,483 - INFO - Downloaded PDF: arxiv_pdfs/2503.04143v1.pdf\n",
            "2025-07-15 19:25:03,817 - INFO - Stored/Updated paper 2503.04143v1 in the database.\n",
            "2025-07-15 19:25:03,817 - INFO - Processing paper ID: 2207.02458v1\n",
            "2025-07-15 19:25:06,340 - INFO - Downloaded PDF: arxiv_pdfs/2207.02458v1.pdf\n",
            "2025-07-15 19:25:08,295 - INFO - Stored/Updated paper 2207.02458v1 in the database.\n",
            "2025-07-15 19:25:08,296 - INFO - Processing paper ID: 2501.17992v1\n",
            "2025-07-15 19:25:10,038 - INFO - Downloaded PDF: arxiv_pdfs/2501.17992v1.pdf\n",
            "2025-07-15 19:25:15,799 - INFO - Stored/Updated paper 2501.17992v1 in the database.\n",
            "2025-07-15 19:25:15,799 - INFO - --- Starting search for keyword: 'portfolio optimization' ---\n",
            "2025-07-15 19:25:16,998 - INFO - Successfully fetched 5 entries for query: 'portfolio optimization'\n",
            "2025-07-15 19:25:17,000 - INFO - Processing paper ID: 0711.2718v1\n",
            "2025-07-15 19:25:18,320 - INFO - Downloaded PDF: arxiv_pdfs/0711.2718v1.pdf\n",
            "2025-07-15 19:25:20,232 - INFO - Stored/Updated paper 0711.2718v1 in the database.\n",
            "2025-07-15 19:25:20,232 - INFO - Processing paper ID: 2406.10250v2\n",
            "2025-07-15 19:25:23,247 - INFO - Downloaded PDF: arxiv_pdfs/2406.10250v2.pdf\n",
            "2025-07-15 19:25:24,647 - INFO - Stored/Updated paper 2406.10250v2 in the database.\n",
            "2025-07-15 19:25:24,648 - INFO - Processing paper ID: 2112.15499v2\n",
            "2025-07-15 19:25:26,736 - INFO - Downloaded PDF: arxiv_pdfs/2112.15499v2.pdf\n",
            "2025-07-15 19:25:31,282 - INFO - Stored/Updated paper 2112.15499v2 in the database.\n",
            "2025-07-15 19:25:31,282 - INFO - Processing paper ID: 2208.14749v2\n",
            "2025-07-15 19:25:32,618 - INFO - Downloaded PDF: arxiv_pdfs/2208.14749v2.pdf\n",
            "2025-07-15 19:25:35,767 - INFO - Stored/Updated paper 2208.14749v2 in the database.\n",
            "2025-07-15 19:25:35,767 - INFO - Processing paper ID: 0606015v1\n",
            "2025-07-15 19:25:36,849 - ERROR - Error downloading PDF for paper 0606015v1: 404 Client Error: Not Found for url: http://arxiv.org/pdf/0606015v1\n",
            "2025-07-15 19:25:36,852 - WARNING - Could not extract text for paper 0606015v1. Skipping NLP and storage.\n",
            "2025-07-15 19:25:36,853 - INFO - --- Starting search for keyword: 'trading strategies' ---\n",
            "2025-07-15 19:25:37,912 - INFO - Successfully fetched 5 entries for query: 'trading strategies'\n",
            "2025-07-15 19:25:37,914 - INFO - Processing paper ID: 0801.4047v2\n",
            "2025-07-15 19:25:39,595 - INFO - Downloaded PDF: arxiv_pdfs/0801.4047v2.pdf\n",
            "2025-07-15 19:25:41,011 - INFO - Stored/Updated paper 0801.4047v2 in the database.\n",
            "2025-07-15 19:25:41,011 - INFO - Processing paper ID: 1705.08022v1\n",
            "2025-07-15 19:25:42,833 - INFO - Downloaded PDF: arxiv_pdfs/1705.08022v1.pdf\n",
            "2025-07-15 19:25:45,385 - INFO - Stored/Updated paper 1705.08022v1 in the database.\n",
            "2025-07-15 19:25:45,386 - INFO - Processing paper ID: 1912.04221v1\n",
            "2025-07-15 19:25:46,845 - INFO - Downloaded PDF: arxiv_pdfs/1912.04221v1.pdf\n",
            "2025-07-15 19:25:48,536 - INFO - Stored/Updated paper 1912.04221v1 in the database.\n",
            "2025-07-15 19:25:48,536 - INFO - Processing paper ID: 2103.13507v1\n",
            "2025-07-15 19:25:50,378 - INFO - Downloaded PDF: arxiv_pdfs/2103.13507v1.pdf\n",
            "2025-07-15 19:25:53,125 - INFO - Stored/Updated paper 2103.13507v1 in the database.\n",
            "2025-07-15 19:25:53,126 - INFO - Processing paper ID: 0503444v1\n",
            "2025-07-15 19:25:54,213 - ERROR - Error downloading PDF for paper 0503444v1: 404 Client Error: Not Found for url: http://arxiv.org/pdf/0503444v1\n",
            "2025-07-15 19:25:54,216 - WARNING - Could not extract text for paper 0503444v1. Skipping NLP and storage.\n",
            "2025-07-15 19:25:54,216 - INFO - --- Starting search for keyword: 'algorithmic trading' ---\n",
            "2025-07-15 19:25:55,025 - INFO - Successfully fetched 5 entries for query: 'algorithmic trading'\n",
            "2025-07-15 19:25:55,028 - INFO - Processing paper ID: 2006.05515v1\n",
            "2025-07-15 19:25:56,465 - INFO - Downloaded PDF: arxiv_pdfs/2006.05515v1.pdf\n",
            "2025-07-15 19:25:57,773 - INFO - Stored/Updated paper 2006.05515v1 in the database.\n",
            "2025-07-15 19:25:57,773 - INFO - Processing paper ID: 2010.11388v1\n",
            "2025-07-15 19:25:59,801 - INFO - Downloaded PDF: arxiv_pdfs/2010.11388v1.pdf\n",
            "2025-07-15 19:26:04,079 - INFO - Stored/Updated paper 2010.11388v1 in the database.\n",
            "2025-07-15 19:26:04,079 - INFO - Processing paper ID: 2101.08813v1\n",
            "2025-07-15 19:26:05,835 - INFO - Downloaded PDF: arxiv_pdfs/2101.08813v1.pdf\n",
            "2025-07-15 19:26:07,846 - INFO - Stored/Updated paper 2101.08813v1 in the database.\n",
            "2025-07-15 19:26:07,846 - INFO - Processing paper ID: 2505.14050v1\n",
            "2025-07-15 19:26:11,682 - INFO - Downloaded PDF: arxiv_pdfs/2505.14050v1.pdf\n",
            "2025-07-15 19:26:13,280 - INFO - Stored/Updated paper 2505.14050v1 in the database.\n",
            "2025-07-15 19:26:13,280 - INFO - Processing paper ID: 2411.05013v1\n",
            "2025-07-15 19:26:15,157 - INFO - Downloaded PDF: arxiv_pdfs/2411.05013v1.pdf\n",
            "2025-07-15 19:26:19,102 - INFO - Stored/Updated paper 2411.05013v1 in the database.\n",
            "2025-07-15 19:26:19,102 - INFO - --- Starting search for keyword: 'quantitative trading' ---\n",
            "2025-07-15 19:26:20,573 - INFO - Successfully fetched 5 entries for query: 'quantitative trading'\n",
            "2025-07-15 19:26:20,576 - INFO - Processing paper ID: 2201.11070v3\n",
            "2025-07-15 19:26:22,602 - INFO - Downloaded PDF: arxiv_pdfs/2201.11070v3.pdf\n",
            "2025-07-15 19:26:31,055 - INFO - Stored/Updated paper 2201.11070v3 in the database.\n",
            "2025-07-15 19:26:31,056 - INFO - Processing paper ID: 2304.06037v2\n",
            "2025-07-15 19:26:32,650 - INFO - Downloaded PDF: arxiv_pdfs/2304.06037v2.pdf\n",
            "2025-07-15 19:26:34,302 - INFO - Stored/Updated paper 2304.06037v2 in the database.\n",
            "2025-07-15 19:26:34,303 - INFO - Processing paper ID: 2411.17900v1\n",
            "2025-07-15 19:26:38,282 - INFO - Downloaded PDF: arxiv_pdfs/2411.17900v1.pdf\n",
            "2025-07-15 19:26:40,160 - INFO - Stored/Updated paper 2411.17900v1 in the database.\n",
            "2025-07-15 19:26:40,161 - INFO - Processing paper ID: 2310.05551v3\n",
            "2025-07-15 19:26:42,242 - INFO - Downloaded PDF: arxiv_pdfs/2310.05551v3.pdf\n",
            "2025-07-15 19:26:44,510 - INFO - Stored/Updated paper 2310.05551v3 in the database.\n",
            "2025-07-15 19:26:44,510 - INFO - Processing paper ID: 1609.09601v1\n",
            "2025-07-15 19:26:46,631 - INFO - Downloaded PDF: arxiv_pdfs/1609.09601v1.pdf\n",
            "2025-07-15 19:26:48,797 - INFO - Stored/Updated paper 1609.09601v1 in the database.\n",
            "2025-07-15 19:26:48,798 - INFO - --- Starting search for keyword: 'mixture of experts trading' ---\n",
            "2025-07-15 19:26:50,003 - INFO - Successfully fetched 0 entries for query: 'mixture of experts trading'\n",
            "2025-07-15 19:26:50,006 - INFO - Closing database connection.\n",
            "2025-07-15 19:26:50,009 - INFO - --- Starting Topic Modeling on all collected papers ---\n",
            "2025-07-15 19:26:50,062 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n",
            "2025-07-15 19:26:50,100 - INFO - built Dictionary<16309 unique tokens: ['01', '1', '10', '100', '101']...> from 51 documents (total 170049 corpus positions)\n",
            "2025-07-15 19:26:50,101 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<16309 unique tokens: ['01', '1', '10', '100', '101']...> from 51 documents (total 170049 corpus positions)\", 'datetime': '2025-07-15T19:26:50.101269', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'created'}\n",
            "2025-07-15 19:26:50,126 - INFO - using symmetric alpha at 0.2\n",
            "2025-07-15 19:26:50,126 - INFO - using symmetric eta at 0.2\n",
            "2025-07-15 19:26:50,127 - INFO - using serial LDA version on this node\n",
            "2025-07-15 19:26:50,129 - INFO - running online (multi-pass) LDA training, 5 topics, 10 passes over the supplied corpus of 51 documents, updating model once every 51 documents, evaluating perplexity every 51 documents, iterating 50x with a convergence threshold of 0.001000\n",
            "2025-07-15 19:26:50,270 - INFO - -10.500 per-word bound, 1448.2 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:50,274 - INFO - PROGRESS: pass 0, at document #51/51\n",
            "2025-07-15 19:26:50,392 - INFO - topic #0 (0.200): 0.008*\"alpha\" + 0.007*\"trading\" + 0.007*\"model\" + 0.007*\"stock\" + 0.006*\"learning\" + 0.006*\"market\" + 0.005*\"data\" + 0.005*\"portfolio\" + 0.004*\"using\" + 0.004*\"alphas\"\n",
            "2025-07-15 19:26:50,395 - INFO - topic #1 (0.200): 0.012*\"alpha\" + 0.006*\"data\" + 0.006*\"trading\" + 0.005*\"stock\" + 0.005*\"model\" + 0.005*\"market\" + 0.004*\"time\" + 0.004*\"models\" + 0.004*\"learning\" + 0.003*\"search\"\n",
            "2025-07-15 19:26:50,397 - INFO - topic #2 (0.200): 0.009*\"stock\" + 0.007*\"price\" + 0.007*\"trading\" + 0.006*\"model\" + 0.006*\"portfolio\" + 0.005*\"learning\" + 0.005*\"market\" + 0.004*\"data\" + 0.004*\"et\" + 0.004*\"0\"\n",
            "2025-07-15 19:26:50,398 - INFO - topic #3 (0.200): 0.007*\"stock\" + 0.006*\"learning\" + 0.006*\"alpha\" + 0.006*\"data\" + 0.005*\"model\" + 0.004*\"portfolio\" + 0.004*\"market\" + 0.004*\"trading\" + 0.004*\"set\" + 0.004*\"et\"\n",
            "2025-07-15 19:26:50,399 - INFO - topic #4 (0.200): 0.007*\"alpha\" + 0.007*\"model\" + 0.006*\"market\" + 0.005*\"learning\" + 0.005*\"using\" + 0.005*\"portfolio\" + 0.005*\"data\" + 0.005*\"stock\" + 0.004*\"factor\" + 0.004*\"trading\"\n",
            "2025-07-15 19:26:50,401 - INFO - topic diff=2.152399, rho=1.000000\n",
            "2025-07-15 19:26:50,531 - INFO - -8.441 per-word bound, 347.6 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:50,532 - INFO - PROGRESS: pass 1, at document #51/51\n",
            "2025-07-15 19:26:50,606 - INFO - topic #0 (0.200): 0.009*\"trading\" + 0.007*\"alpha\" + 0.007*\"stock\" + 0.007*\"learning\" + 0.006*\"model\" + 0.005*\"market\" + 0.005*\"data\" + 0.004*\"portfolio\" + 0.004*\"alphas\" + 0.004*\"1\"\n",
            "2025-07-15 19:26:50,608 - INFO - topic #1 (0.200): 0.018*\"alpha\" + 0.006*\"trading\" + 0.005*\"alphas\" + 0.005*\"data\" + 0.005*\"search\" + 0.004*\"market\" + 0.004*\"stock\" + 0.004*\"model\" + 0.004*\"mining\" + 0.004*\"learning\"\n",
            "2025-07-15 19:26:50,609 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.009*\"price\" + 0.007*\"trading\" + 0.007*\"portfolio\" + 0.007*\"model\" + 0.006*\"learning\" + 0.005*\"market\" + 0.005*\"et\" + 0.005*\"data\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:50,611 - INFO - topic #3 (0.200): 0.006*\"stock\" + 0.006*\"learning\" + 0.005*\"alpha\" + 0.005*\"data\" + 0.005*\"model\" + 0.004*\"set\" + 0.004*\"deep\" + 0.004*\"portfolio\" + 0.004*\"market\" + 0.004*\"trading\"\n",
            "2025-07-15 19:26:50,611 - INFO - topic #4 (0.200): 0.007*\"model\" + 0.007*\"alpha\" + 0.006*\"market\" + 0.006*\"data\" + 0.005*\"portfolio\" + 0.005*\"learning\" + 0.005*\"factor\" + 0.005*\"stock\" + 0.005*\"using\" + 0.004*\"probability\"\n",
            "2025-07-15 19:26:50,612 - INFO - topic diff=0.673316, rho=0.577350\n",
            "2025-07-15 19:26:50,771 - INFO - -8.240 per-word bound, 302.4 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:50,772 - INFO - PROGRESS: pass 2, at document #51/51\n",
            "2025-07-15 19:26:50,833 - INFO - topic #0 (0.200): 0.010*\"trading\" + 0.007*\"learning\" + 0.007*\"stock\" + 0.005*\"alpha\" + 0.005*\"market\" + 0.005*\"model\" + 0.004*\"1\" + 0.004*\"portfolio\" + 0.004*\"data\" + 0.004*\"alphas\"\n",
            "2025-07-15 19:26:50,834 - INFO - topic #1 (0.200): 0.022*\"alpha\" + 0.007*\"alphas\" + 0.006*\"trading\" + 0.006*\"search\" + 0.005*\"market\" + 0.005*\"mining\" + 0.005*\"data\" + 0.004*\"stock\" + 0.004*\"et\" + 0.004*\"factor\"\n",
            "2025-07-15 19:26:50,835 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"model\" + 0.007*\"trading\" + 0.006*\"learning\" + 0.006*\"market\" + 0.005*\"et\" + 0.005*\"data\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:50,836 - INFO - topic #3 (0.200): 0.005*\"stock\" + 0.005*\"learning\" + 0.004*\"set\" + 0.004*\"data\" + 0.004*\"model\" + 0.004*\"alpha\" + 0.003*\"deep\" + 0.003*\"operator\" + 0.003*\"portfolio\" + 0.003*\"market\"\n",
            "2025-07-15 19:26:50,836 - INFO - topic #4 (0.200): 0.007*\"model\" + 0.006*\"alpha\" + 0.006*\"data\" + 0.006*\"market\" + 0.005*\"portfolio\" + 0.005*\"learning\" + 0.005*\"stock\" + 0.005*\"factor\" + 0.005*\"probability\" + 0.005*\"using\"\n",
            "2025-07-15 19:26:50,837 - INFO - topic diff=0.442484, rho=0.500000\n",
            "2025-07-15 19:26:50,947 - INFO - -8.159 per-word bound, 285.8 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:50,948 - INFO - PROGRESS: pass 3, at document #51/51\n",
            "2025-07-15 19:26:51,046 - INFO - topic #0 (0.200): 0.011*\"trading\" + 0.008*\"learning\" + 0.007*\"stock\" + 0.004*\"1\" + 0.004*\"market\" + 0.004*\"model\" + 0.004*\"alpha\" + 0.004*\"reinforcement\" + 0.004*\"monoidal\" + 0.004*\"portfolio\"\n",
            "2025-07-15 19:26:51,048 - INFO - topic #1 (0.200): 0.024*\"alpha\" + 0.008*\"alphas\" + 0.006*\"search\" + 0.006*\"trading\" + 0.006*\"mining\" + 0.006*\"market\" + 0.005*\"data\" + 0.005*\"factor\" + 0.004*\"performance\" + 0.004*\"stock\"\n",
            "2025-07-15 19:26:51,049 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"model\" + 0.007*\"trading\" + 0.006*\"learning\" + 0.006*\"market\" + 0.005*\"et\" + 0.005*\"data\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:51,049 - INFO - topic #3 (0.200): 0.004*\"stock\" + 0.004*\"set\" + 0.004*\"operator\" + 0.003*\"learning\" + 0.003*\"data\" + 0.003*\"model\" + 0.003*\"alpha\" + 0.002*\"training\" + 0.002*\"deep\" + 0.002*\"1\"\n",
            "2025-07-15 19:26:51,052 - INFO - topic #4 (0.200): 0.008*\"model\" + 0.006*\"data\" + 0.005*\"portfolio\" + 0.005*\"alpha\" + 0.005*\"market\" + 0.005*\"learning\" + 0.005*\"probability\" + 0.005*\"stock\" + 0.005*\"factor\" + 0.005*\"models\"\n",
            "2025-07-15 19:26:51,053 - INFO - topic diff=0.304390, rho=0.447214\n",
            "2025-07-15 19:26:51,155 - INFO - -8.118 per-word bound, 277.8 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:51,156 - INFO - PROGRESS: pass 4, at document #51/51\n",
            "2025-07-15 19:26:51,222 - INFO - topic #0 (0.200): 0.011*\"trading\" + 0.008*\"learning\" + 0.007*\"stock\" + 0.004*\"1\" + 0.004*\"reinforcement\" + 0.004*\"monoidal\" + 0.004*\"attacks\" + 0.004*\"market\" + 0.004*\"action\" + 0.004*\"arbitrage\"\n",
            "2025-07-15 19:26:51,224 - INFO - topic #1 (0.200): 0.026*\"alpha\" + 0.009*\"alphas\" + 0.007*\"search\" + 0.006*\"trading\" + 0.006*\"mining\" + 0.006*\"market\" + 0.005*\"factor\" + 0.005*\"performance\" + 0.005*\"stock\" + 0.005*\"data\"\n",
            "2025-07-15 19:26:51,225 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"trading\" + 0.007*\"model\" + 0.006*\"learning\" + 0.006*\"market\" + 0.005*\"et\" + 0.005*\"data\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:51,225 - INFO - topic #3 (0.200): 0.005*\"operator\" + 0.004*\"set\" + 0.004*\"stock\" + 0.003*\"data\" + 0.003*\"points\" + 0.003*\"learning\" + 0.003*\"model\" + 0.002*\"point\" + 0.002*\"operators\" + 0.002*\"training\"\n",
            "2025-07-15 19:26:51,229 - INFO - topic #4 (0.200): 0.008*\"model\" + 0.006*\"data\" + 0.006*\"portfolio\" + 0.005*\"probability\" + 0.005*\"market\" + 0.005*\"learning\" + 0.005*\"factor\" + 0.005*\"stock\" + 0.005*\"models\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:51,230 - INFO - topic diff=0.214529, rho=0.408248\n",
            "2025-07-15 19:26:51,416 - INFO - -8.094 per-word bound, 273.2 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:51,417 - INFO - PROGRESS: pass 5, at document #51/51\n",
            "2025-07-15 19:26:51,479 - INFO - topic #0 (0.200): 0.011*\"trading\" + 0.008*\"learning\" + 0.007*\"stock\" + 0.005*\"1\" + 0.004*\"reinforcement\" + 0.004*\"monoidal\" + 0.004*\"attacks\" + 0.004*\"arbitrage\" + 0.004*\"action\" + 0.004*\"market\"\n",
            "2025-07-15 19:26:51,481 - INFO - topic #1 (0.200): 0.027*\"alpha\" + 0.009*\"alphas\" + 0.007*\"search\" + 0.006*\"trading\" + 0.006*\"mining\" + 0.006*\"market\" + 0.005*\"factor\" + 0.005*\"performance\" + 0.005*\"stock\" + 0.005*\"formulaic\"\n",
            "2025-07-15 19:26:51,482 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"trading\" + 0.007*\"model\" + 0.006*\"learning\" + 0.006*\"market\" + 0.006*\"et\" + 0.005*\"data\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:51,483 - INFO - topic #3 (0.200): 0.005*\"operator\" + 0.004*\"set\" + 0.003*\"stock\" + 0.003*\"points\" + 0.003*\"point\" + 0.003*\"shift\" + 0.003*\"operators\" + 0.003*\"weighted\" + 0.002*\"data\" + 0.002*\"properties\"\n",
            "2025-07-15 19:26:51,484 - INFO - topic #4 (0.200): 0.008*\"model\" + 0.006*\"data\" + 0.006*\"portfolio\" + 0.005*\"probability\" + 0.005*\"market\" + 0.005*\"models\" + 0.005*\"learning\" + 0.005*\"factor\" + 0.005*\"time\" + 0.005*\"stock\"\n",
            "2025-07-15 19:26:51,485 - INFO - topic diff=0.152481, rho=0.377964\n",
            "2025-07-15 19:26:51,587 - INFO - -8.080 per-word bound, 270.5 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:51,588 - INFO - PROGRESS: pass 6, at document #51/51\n",
            "2025-07-15 19:26:51,659 - INFO - topic #0 (0.200): 0.012*\"trading\" + 0.008*\"learning\" + 0.006*\"stock\" + 0.005*\"1\" + 0.005*\"reinforcement\" + 0.005*\"monoidal\" + 0.004*\"attacks\" + 0.004*\"arbitrage\" + 0.004*\"action\" + 0.004*\"market\"\n",
            "2025-07-15 19:26:51,660 - INFO - topic #1 (0.200): 0.027*\"alpha\" + 0.009*\"alphas\" + 0.007*\"search\" + 0.006*\"mining\" + 0.006*\"trading\" + 0.006*\"market\" + 0.005*\"factor\" + 0.005*\"formulaic\" + 0.005*\"performance\" + 0.005*\"stock\"\n",
            "2025-07-15 19:26:51,661 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"trading\" + 0.007*\"model\" + 0.006*\"learning\" + 0.006*\"market\" + 0.006*\"et\" + 0.005*\"data\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:51,662 - INFO - topic #3 (0.200): 0.006*\"operator\" + 0.004*\"set\" + 0.003*\"points\" + 0.003*\"point\" + 0.003*\"shift\" + 0.003*\"operators\" + 0.003*\"stock\" + 0.003*\"weighted\" + 0.003*\"properties\" + 0.003*\"space\"\n",
            "2025-07-15 19:26:51,663 - INFO - topic #4 (0.200): 0.008*\"model\" + 0.006*\"data\" + 0.006*\"portfolio\" + 0.006*\"probability\" + 0.005*\"market\" + 0.005*\"models\" + 0.005*\"time\" + 0.005*\"factor\" + 0.005*\"learning\" + 0.005*\"stock\"\n",
            "2025-07-15 19:26:51,664 - INFO - topic diff=0.110159, rho=0.353553\n",
            "2025-07-15 19:26:51,771 - INFO - -8.070 per-word bound, 268.7 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:51,772 - INFO - PROGRESS: pass 7, at document #51/51\n",
            "2025-07-15 19:26:51,826 - INFO - topic #0 (0.200): 0.012*\"trading\" + 0.008*\"learning\" + 0.006*\"stock\" + 0.005*\"1\" + 0.005*\"monoidal\" + 0.005*\"attacks\" + 0.005*\"reinforcement\" + 0.004*\"arbitrage\" + 0.004*\"action\" + 0.003*\"deep\"\n",
            "2025-07-15 19:26:51,828 - INFO - topic #1 (0.200): 0.027*\"alpha\" + 0.009*\"alphas\" + 0.007*\"search\" + 0.006*\"mining\" + 0.006*\"trading\" + 0.006*\"formulaic\" + 0.005*\"market\" + 0.005*\"factor\" + 0.005*\"performance\" + 0.005*\"stock\"\n",
            "2025-07-15 19:26:51,829 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"trading\" + 0.007*\"model\" + 0.006*\"market\" + 0.006*\"learning\" + 0.006*\"et\" + 0.005*\"data\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:51,829 - INFO - topic #3 (0.200): 0.006*\"operator\" + 0.004*\"set\" + 0.003*\"points\" + 0.003*\"point\" + 0.003*\"shift\" + 0.003*\"operators\" + 0.003*\"weighted\" + 0.003*\"properties\" + 0.003*\"space\" + 0.003*\"type\"\n",
            "2025-07-15 19:26:51,831 - INFO - topic #4 (0.200): 0.008*\"model\" + 0.006*\"data\" + 0.006*\"portfolio\" + 0.006*\"probability\" + 0.005*\"market\" + 0.005*\"models\" + 0.005*\"time\" + 0.005*\"factor\" + 0.005*\"1\" + 0.005*\"learning\"\n",
            "2025-07-15 19:26:51,831 - INFO - topic diff=0.080905, rho=0.333333\n",
            "2025-07-15 19:26:51,979 - INFO - -8.064 per-word bound, 267.5 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:51,980 - INFO - PROGRESS: pass 8, at document #51/51\n",
            "2025-07-15 19:26:52,039 - INFO - topic #0 (0.200): 0.012*\"trading\" + 0.008*\"learning\" + 0.006*\"stock\" + 0.005*\"1\" + 0.005*\"monoidal\" + 0.005*\"attacks\" + 0.005*\"reinforcement\" + 0.004*\"arbitrage\" + 0.004*\"action\" + 0.003*\"attack\"\n",
            "2025-07-15 19:26:52,041 - INFO - topic #1 (0.200): 0.027*\"alpha\" + 0.009*\"alphas\" + 0.007*\"search\" + 0.006*\"mining\" + 0.006*\"trading\" + 0.006*\"formulaic\" + 0.005*\"factor\" + 0.005*\"market\" + 0.005*\"performance\" + 0.005*\"stock\"\n",
            "2025-07-15 19:26:52,043 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"trading\" + 0.007*\"model\" + 0.007*\"market\" + 0.006*\"learning\" + 0.006*\"et\" + 0.005*\"data\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:52,044 - INFO - topic #3 (0.200): 0.006*\"operator\" + 0.004*\"set\" + 0.004*\"points\" + 0.003*\"point\" + 0.003*\"shift\" + 0.003*\"operators\" + 0.003*\"weighted\" + 0.003*\"properties\" + 0.003*\"space\" + 0.003*\"type\"\n",
            "2025-07-15 19:26:52,045 - INFO - topic #4 (0.200): 0.008*\"model\" + 0.006*\"data\" + 0.006*\"portfolio\" + 0.006*\"probability\" + 0.005*\"market\" + 0.005*\"models\" + 0.005*\"time\" + 0.005*\"1\" + 0.005*\"factor\" + 0.005*\"training\"\n",
            "2025-07-15 19:26:52,045 - INFO - topic diff=0.060309, rho=0.316228\n",
            "2025-07-15 19:26:52,162 - INFO - -8.059 per-word bound, 266.7 perplexity estimate based on a held-out corpus of 51 documents with 170049 words\n",
            "2025-07-15 19:26:52,162 - INFO - PROGRESS: pass 9, at document #51/51\n",
            "2025-07-15 19:26:52,212 - INFO - topic #0 (0.200): 0.012*\"trading\" + 0.008*\"learning\" + 0.006*\"stock\" + 0.005*\"1\" + 0.005*\"monoidal\" + 0.005*\"attacks\" + 0.005*\"reinforcement\" + 0.004*\"arbitrage\" + 0.004*\"action\" + 0.004*\"attack\"\n",
            "2025-07-15 19:26:52,213 - INFO - topic #1 (0.200): 0.027*\"alpha\" + 0.009*\"alphas\" + 0.007*\"search\" + 0.006*\"mining\" + 0.006*\"formulaic\" + 0.006*\"trading\" + 0.006*\"factor\" + 0.005*\"performance\" + 0.005*\"market\" + 0.005*\"stock\"\n",
            "2025-07-15 19:26:52,214 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"trading\" + 0.007*\"model\" + 0.007*\"market\" + 0.006*\"learning\" + 0.006*\"et\" + 0.005*\"data\" + 0.005*\"quantum\"\n",
            "2025-07-15 19:26:52,216 - INFO - topic #3 (0.200): 0.006*\"operator\" + 0.004*\"set\" + 0.004*\"points\" + 0.003*\"point\" + 0.003*\"shift\" + 0.003*\"operators\" + 0.003*\"weighted\" + 0.003*\"properties\" + 0.003*\"space\" + 0.003*\"type\"\n",
            "2025-07-15 19:26:52,217 - INFO - topic #4 (0.200): 0.008*\"model\" + 0.006*\"data\" + 0.006*\"portfolio\" + 0.006*\"probability\" + 0.005*\"time\" + 0.005*\"models\" + 0.005*\"market\" + 0.005*\"1\" + 0.005*\"training\" + 0.005*\"factor\"\n",
            "2025-07-15 19:26:52,218 - INFO - topic diff=0.045706, rho=0.301511\n",
            "2025-07-15 19:26:52,219 - INFO - LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=16309, num_topics=5, decay=0.5, chunksize=2000> in 2.09s', 'datetime': '2025-07-15T19:26:52.219271', 'gensim': '4.3.3', 'python': '3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]', 'platform': 'macOS-15.1-arm64-arm-64bit', 'event': 'created'}\n",
            "2025-07-15 19:26:52,220 - INFO - topic #0 (0.200): 0.012*\"trading\" + 0.008*\"learning\" + 0.006*\"stock\" + 0.005*\"1\" + 0.005*\"monoidal\"\n",
            "2025-07-15 19:26:52,221 - INFO - topic #1 (0.200): 0.027*\"alpha\" + 0.009*\"alphas\" + 0.007*\"search\" + 0.006*\"mining\" + 0.006*\"formulaic\"\n",
            "2025-07-15 19:26:52,221 - INFO - topic #2 (0.200): 0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"trading\" + 0.007*\"model\"\n",
            "2025-07-15 19:26:52,222 - INFO - topic #3 (0.200): 0.006*\"operator\" + 0.004*\"set\" + 0.004*\"points\" + 0.003*\"point\" + 0.003*\"shift\"\n",
            "2025-07-15 19:26:52,223 - INFO - topic #4 (0.200): 0.008*\"model\" + 0.006*\"data\" + 0.006*\"portfolio\" + 0.006*\"probability\" + 0.005*\"time\"\n",
            "2025-07-15 19:26:52,224 - INFO - \n",
            "Pipeline finished. Processed a total of 51 papers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Identified Topics ---\n",
            "(0, '0.012*\"trading\" + 0.008*\"learning\" + 0.006*\"stock\" + 0.005*\"1\" + 0.005*\"monoidal\"')\n",
            "(1, '0.027*\"alpha\" + 0.009*\"alphas\" + 0.007*\"search\" + 0.006*\"mining\" + 0.006*\"formulaic\"')\n",
            "(2, '0.010*\"stock\" + 0.010*\"price\" + 0.007*\"portfolio\" + 0.007*\"trading\" + 0.007*\"model\"')\n",
            "(3, '0.006*\"operator\" + 0.004*\"set\" + 0.004*\"points\" + 0.003*\"point\" + 0.003*\"shift\"')\n",
            "(4, '0.008*\"model\" + 0.006*\"data\" + 0.006*\"portfolio\" + 0.006*\"probability\" + 0.005*\"time\"')\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# 1. IMPORTS AND SETUP\n",
        "# ==============================================================================\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import xmltodict\n",
        "import PyPDF2\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from gensim import corpora, models\n",
        "import sqlite3\n",
        "import logging\n",
        "\n",
        "# Configure logging for clear output\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CONSTANTS AND CONFIGURATION\n",
        "# ==============================================================================\n",
        "ARXIV_API_URL = \"http://export.arxiv.org/api/query\"\n",
        "PDF_DIR = \"arxiv_pdfs\"\n",
        "DB_FILE = \"arxiv_papers.db\"\n",
        "os.makedirs(PDF_DIR, exist_ok=True)\n",
        "\n",
        "# Define search keywords\n",
        "SEARCH_KEYWORDS = [\n",
        "    \"alpha factors\", 'alpha generation', 'alpha mining', 'factor investing', \n",
        "    'formulaic alphas', 'stock price prediction', 'artifical intelligence trading', \n",
        "    'AI stock selection', 'large language models finance', 'reinforcement learning portfolio', \n",
        "    'portfolio optimization', 'trading strategies', 'algorithmic trading', \n",
        "    'quantitative trading', 'mixture of experts trading'\n",
        "]\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. NLTK AND SPACY MODEL LOADING\n",
        "# ==============================================================================\n",
        "# Download NLTK data quietly\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Load spaCy model, downloading if necessary\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    logging.info(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    os.system(f\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. CORE FUNCTIONS (DECOMPOSED AND REFACTORED)\n",
        "# ==============================================================================\n",
        "\n",
        "def parse_arxiv_entry(entry: dict) -> dict:\n",
        "    \"\"\"Parses a single raw XML entry from arXiv into a clean dictionary.\"\"\"\n",
        "    paper = {}\n",
        "    paper['id'] = entry.get('id', '').split('/')[-1] if entry.get('id') else None\n",
        "    paper['title'] = entry.get('title', 'No Title').replace('\\n', ' ').strip()\n",
        "    paper['abstract'] = entry.get('summary', 'No Abstract').replace('\\n', ' ').strip()\n",
        "    \n",
        "    # Handle both single author (dict) and multiple authors (list of dicts)\n",
        "    authors = entry.get('author', [])\n",
        "    if isinstance(authors, dict):\n",
        "        authors = [authors]\n",
        "    paper['authors'] = [author.get('name') for author in authors if isinstance(author, dict)]\n",
        "    \n",
        "    paper['categories'] = [cat.get('@term') for cat in entry.get('category', []) if isinstance(cat, dict)]\n",
        "    # FIX: Use underscore to match database schema convention\n",
        "    paper['journal_ref'] = entry.get('arxiv:journal_ref', {}).get('#text')\n",
        "    paper['doi'] = entry.get('arxiv:doi', {}).get('#text')\n",
        "    paper['published'] = entry.get('published')\n",
        "    return paper\n",
        "\n",
        "def fetch_arxiv_metadata(query: str, max_results: int = 5, retry_count: int = 3) -> list:\n",
        "    \"\"\"\n",
        "    Fetches paper metadata from the arXiv API for a given query.\n",
        "    Handles network errors with retries and exponential backoff.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"search_query\": f'all:\"{query}\"',\n",
        "        \"start\": 0,\n",
        "        \"max_results\": max_results,\n",
        "    }\n",
        "    for attempt in range(retry_count):\n",
        "        try:\n",
        "            response = requests.get(ARXIV_API_URL, params=params)\n",
        "            response.raise_for_status()  # Raises HTTPError for bad responses (4xx or 5xx)\n",
        "            \n",
        "            xml_dict = xmltodict.parse(response.content)\n",
        "            entries = xml_dict.get('feed', {}).get('entry', [])\n",
        "            \n",
        "            logging.info(f\"Successfully fetched {len(entries)} entries for query: '{query}'\")\n",
        "            return [parse_arxiv_entry(e) for e in entries]\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logging.warning(f\"Network error on attempt {attempt + 1} for '{query}': {e}\")\n",
        "        except (xmltodict.expat.ExpatError, KeyError) as e:\n",
        "            logging.warning(f\"XML parsing error on attempt {attempt + 1} for '{query}': {e}\")\n",
        "\n",
        "        if attempt < retry_count - 1:\n",
        "            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "        else:\n",
        "            logging.error(f\"Failed to fetch metadata for '{query}' after {retry_count} attempts.\")\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "\n",
        "def download_pdf(pdf_url: str, paper_id: str) -> str | None:\n",
        "    \"\"\"Downloads a PDF given its URL and saves it locally. Returns the file path.\"\"\"\n",
        "    file_path = os.path.join(PDF_DIR, f\"{paper_id}.pdf\")\n",
        "    try:\n",
        "        response = requests.get(pdf_url, stream=True, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        with open(file_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        logging.info(f\"Downloaded PDF: {file_path}\")\n",
        "        return file_path\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error downloading PDF for paper {paper_id}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str | None:\n",
        "    \"\"\"Extracts text content from a PDF.\"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            text = \"\".join(page.extract_text() for page in reader.pages if page.extract_text())\n",
        "            return text if text.strip() else None\n",
        "    except (FileNotFoundError, PyPDF2.errors.PdfReadError) as e:\n",
        "        logging.error(f\"Error extracting text from PDF {pdf_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_keywords(text: str) -> list:\n",
        "    \"\"\"Extracts keywords from text using spaCy for better linguistic understanding.\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    doc = nlp(text.lower())\n",
        "    keywords = [token.lemma_ for token in doc if token.pos_ in (\"NOUN\", \"PROPN\", \"ADJ\") and not token.is_stop and token.is_alpha]\n",
        "    # Return unique keywords while preserving order\n",
        "    return list(dict.fromkeys(keywords))[:20]\n",
        "\n",
        "\n",
        "def summarize_text(text: str, num_sentences: int = 3) -> str:\n",
        "    \"\"\"Summarizes text by taking the first few sentences.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    return \"\".join(sent.text for sent in sentences[:num_sentences]) if sentences else text\n",
        "\n",
        "\n",
        "def analyze_sentiment(text: str) -> float | None:\n",
        "    \"\"\"Analyzes sentiment using TextBlob. Returns polarity score or None.\"\"\"\n",
        "    if not text:\n",
        "        return None # FIX: Return None for missing data to store as NULL\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity\n",
        "\n",
        "\n",
        "def init_database(conn: sqlite3.Connection):\n",
        "    \"\"\"Creates the 'papers' table if it doesn't exist.\"\"\"\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE IF NOT EXISTS papers (\n",
        "            id TEXT PRIMARY KEY,\n",
        "            title TEXT,\n",
        "            abstract TEXT,\n",
        "            authors TEXT,\n",
        "            categories TEXT,\n",
        "            journal_ref TEXT,\n",
        "            doi TEXT,\n",
        "            published TEXT,\n",
        "            pdf_path TEXT,\n",
        "            full_text TEXT,\n",
        "            keywords TEXT,\n",
        "            summary TEXT,\n",
        "            sentiment REAL\n",
        "        )\n",
        "    ''')\n",
        "    conn.commit()\n",
        "\n",
        "\n",
        "def store_paper_in_db(paper_data: dict, conn: sqlite3.Connection):\n",
        "    \"\"\"Stores a single processed paper dictionary into the database.\"\"\"\n",
        "    cursor = conn.cursor()\n",
        "    columns = [\n",
        "        'id', 'title', 'abstract', 'authors', 'categories', 'journal_ref', 'doi',\n",
        "        'published', 'pdf_path', 'full_text', 'keywords', 'summary', 'sentiment'\n",
        "    ]\n",
        "    \n",
        "    placeholders = \", \".join([\"?\"] * len(columns))\n",
        "    query = f\"INSERT OR REPLACE INTO papers ({', '.join(columns)}) VALUES ({placeholders})\"\n",
        "    \n",
        "    values = []\n",
        "    for col in columns:\n",
        "        value = paper_data.get(col)\n",
        "        if isinstance(value, list):\n",
        "            value = json.dumps(value)  # Serialize lists to JSON strings\n",
        "        values.append(value)\n",
        "\n",
        "    try:\n",
        "        cursor.execute(query, tuple(values))\n",
        "        conn.commit()\n",
        "        logging.info(f\"Stored/Updated paper {paper_data['id']} in the database.\")\n",
        "    except sqlite3.Error as e:\n",
        "        logging.error(f\"Database error for paper {paper_data.get('id')}: {e}\")\n",
        "        conn.rollback()\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. MAIN ORCHESTRATION FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function to orchestrate the entire data pipeline.\n",
        "    \"\"\"\n",
        "    # ======================================================================\n",
        "    # === DATABASE RESET LOGIC (GUARANTEES A FRESH START) ===\n",
        "    # This block solves the \"table has no column\" error by deleting the\n",
        "    # old database file before a new one is created.\n",
        "    if os.path.exists(DB_FILE):\n",
        "        logging.warning(\n",
        "            f\"Found existing database '{DB_FILE}'. Deleting it to ensure the correct schema is applied.\"\n",
        "        )\n",
        "        os.remove(DB_FILE)\n",
        "    # ======================================================================\n",
        "\n",
        "    all_processed_papers = []\n",
        "    \n",
        "    # Use a try...finally block to ensure the database connection is closed\n",
        "    conn = sqlite3.connect(DB_FILE)\n",
        "    try:\n",
        "        init_database(conn) # Ensure table exists\n",
        "        \n",
        "        for keyword in SEARCH_KEYWORDS:\n",
        "            logging.info(f\"--- Starting search for keyword: '{keyword}' ---\")\n",
        "            paper_metadata_list = fetch_arxiv_metadata(keyword, max_results=5)\n",
        "            \n",
        "            for metadata in paper_metadata_list:\n",
        "                paper_id = metadata['id']\n",
        "                if not paper_id:\n",
        "                    logging.warning(\"Skipping entry with no ID.\")\n",
        "                    continue\n",
        "\n",
        "                # --- Process a single paper ---\n",
        "                logging.info(f\"Processing paper ID: {paper_id}\")\n",
        "                pdf_url = f\"https://arxiv.org/pdf/{paper_id}.pdf\"\n",
        "                metadata['pdf_path'] = download_pdf(pdf_url, paper_id)\n",
        "\n",
        "                if metadata['pdf_path']:\n",
        "                    metadata['full_text'] = extract_text_from_pdf(metadata['pdf_path'])\n",
        "                \n",
        "                if metadata.get('full_text'):\n",
        "                    # Perform NLP analysis\n",
        "                    full_text = metadata['full_text']\n",
        "                    metadata['keywords'] = extract_keywords(full_text)\n",
        "                    metadata['summary'] = summarize_text(full_text)\n",
        "                    metadata['sentiment'] = analyze_sentiment(full_text)\n",
        "                    \n",
        "                    # Store the complete paper record\n",
        "                    store_paper_in_db(metadata, conn)\n",
        "                    all_processed_papers.append(metadata)\n",
        "                else:\n",
        "                    logging.warning(f\"Could not extract text for paper {paper_id}. Skipping NLP and storage.\")\n",
        "\n",
        "    finally:\n",
        "        logging.info(\"Closing database connection.\")\n",
        "        conn.close()\n",
        "\n",
        "    # --- Final Analysis: Topic Modeling ---\n",
        "    logging.info(\"--- Starting Topic Modeling on all collected papers ---\")\n",
        "    # FIX: Use the complete list `all_processed_papers`\n",
        "    texts = [p.get('full_text') for p in all_processed_papers if p.get('full_text')]\n",
        "\n",
        "    if texts:\n",
        "        tokenized_texts = [[word for word in text.lower().split() if word.isalnum() and word not in STOP_WORDS] for text in texts]\n",
        "        dictionary = corpora.Dictionary(tokenized_texts)\n",
        "        corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
        "        \n",
        "        if corpus:\n",
        "            lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
        "            topics = lda_model.print_topics(num_words=5)\n",
        "            print(\"\\n--- Identified Topics ---\")\n",
        "            for topic in topics:\n",
        "                print(topic)\n",
        "        else:\n",
        "            print(\"\\nCould not create a corpus for topic modeling (likely all documents were empty).\")\n",
        "    else:\n",
        "        print(\"\\nNo text available to perform topic modeling.\")\n",
        "        \n",
        "    logging.info(f\"\\nPipeline finished. Processed a total of {len(all_processed_papers)} papers.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. SCRIPT ENTRY POINT\n",
        "# ==============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of papers stored in the database: 48\n"
          ]
        }
      ],
      "source": [
        "import sqlite3\n",
        "import os\n",
        "\n",
        "DB_FILE = \"arxiv_papers.db\"\n",
        "\n",
        "def check_paper_count():\n",
        "    \"\"\"Connects to the database and prints the number of papers.\"\"\"\n",
        "    if not os.path.exists(DB_FILE):\n",
        "        print(f\"Error: Database file '{DB_FILE}' not found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Connect to the SQLite database\n",
        "        conn = sqlite3.connect(DB_FILE)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Execute the query to count all rows in the 'papers' table\n",
        "        cursor.execute(\"SELECT COUNT(*) FROM papers\")\n",
        "\n",
        "        # Fetch the result. fetchone() returns a single tuple, e.g., (75,)\n",
        "        count_result = cursor.fetchone()\n",
        "        \n",
        "        # The count is the first element of the tuple\n",
        "        paper_count = count_result[0]\n",
        "\n",
        "        print(f\"Total number of papers stored in the database: {paper_count}\")\n",
        "\n",
        "    except sqlite3.Error as e:\n",
        "        print(f\"Database error: {e}\")\n",
        "    finally:\n",
        "        # Make sure to close the connection\n",
        "        if 'conn' in locals() and conn:\n",
        "            conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    check_paper_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaIndex & Ollama: Multimodal RAG Agent for Financial Alpha Generation from PDFs (for Phase I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index in /opt/anaconda3/lib/python3.12/site-packages (0.12.30)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.30 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.12.30)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.3.33)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.73.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (3.10.5)\n",
            "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2.1.1)\n",
            "Requirement already satisfied: dataclasses-json in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (4.13.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.30->llama-index) (1.14.1)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.18)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.12)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.11.0)\n",
            "Requirement already satisfied: griffe in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.7.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.1.4)\n",
            "Requirement already satisfied: platformdirs in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.5)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2024.8.30)\n",
            "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (4.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.2)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.12 in /opt/anaconda3/lib/python3.12/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.12)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.26.20)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.30->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2023.3)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-cloud-services>=0.6.12->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.30->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.16.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.30->llama-index) (2.1.3)\n",
            "Requirement already satisfied: llama-index-core in /opt/anaconda3/lib/python3.12/site-packages (0.12.30)\n",
            "Requirement already satisfied: llama-index-readers-file in /opt/anaconda3/lib/python3.12/site-packages (0.4.7)\n",
            "Requirement already satisfied: llama-index-llms-ollama in /opt/anaconda3/lib/python3.12/site-packages (0.5.4)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /opt/anaconda3/lib/python3.12/site-packages (0.5.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (3.10.5)\n",
            "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2.1.1)\n",
            "Requirement already satisfied: dataclasses-json in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (3.9.1)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (4.13.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file) (4.12.3)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file) (5.4.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file) (0.0.26)\n",
            "Requirement already satisfied: ollama>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-llms-ollama) (0.4.7)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.30.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-embeddings-huggingface) (4.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.11.0)\n",
            "Requirement already satisfied: griffe in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core) (1.7.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core) (3.1.4)\n",
            "Requirement already satisfied: platformdirs in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file) (2.5)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core) (2024.9.11)\n",
            "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (4.2.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (1.0.2)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (3.7)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (1.26.20)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.1)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n",
            "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core) (2.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-huggingface\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf_keras in /opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
            "Requirement already satisfied: tensorflow<2.20,>=2.19 in /opt/anaconda3/lib/python3.12/site-packages (from tf_keras) (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.2.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.4.0)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.25.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.9.2)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.11.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.5.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf_keras) (0.44.0)\n",
            "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (2.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.0)\n",
            "Requirement already satisfied: frontend in /opt/anaconda3/lib/python3.12/site-packages (0.0.3)\n",
            "Requirement already satisfied: starlette>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from frontend) (0.46.2)\n",
            "Requirement already satisfied: uvicorn>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from frontend) (0.34.1)\n",
            "Requirement already satisfied: itsdangerous>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from frontend) (2.2.0)\n",
            "Requirement already satisfied: aiofiles in /opt/anaconda3/lib/python3.12/site-packages (from frontend) (24.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /opt/anaconda3/lib/python3.12/site-packages (from starlette>=0.12.0->frontend) (4.2.0)\n",
            "Requirement already satisfied: click>=7.0 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn>=0.7.1->frontend) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /opt/anaconda3/lib/python3.12/site-packages (from uvicorn>=0.7.1->frontend) (0.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette>=0.12.0->frontend) (1.3.0)\n",
            "Collecting PyMuPDF\n",
            "  Using cached pymupdf-1.26.3-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
            "Using cached pymupdf-1.26.3-cp39-abi3-macosx_11_0_arm64.whl (22.4 MB)\n",
            "Installing collected packages: PyMuPDF\n",
            "  Attempting uninstall: PyMuPDF\n",
            "    Found existing installation: PyMuPDF 1.26.3\n",
            "    Uninstalling PyMuPDF-1.26.3:\n",
            "      Successfully uninstalled PyMuPDF-1.26.3\n",
            "Successfully installed PyMuPDF-1.26.3\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
            "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (4.0.2)\n",
            "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.51.3)\n",
            "Requirement already satisfied: ollama in /opt/anaconda3/lib/python3.12/site-packages (0.4.7)\n",
            "Requirement already satisfied: langchain in /opt/anaconda3/lib/python3.12/site-packages (0.3.23)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /opt/anaconda3/lib/python3.12/site-packages (from ollama) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ollama) (2.11.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.51)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (4.2.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.2)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (3.7)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (8.2.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (1.26.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (2.1)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-ollama in /opt/anaconda3/lib/python3.12/site-packages (0.4.0)\n",
            "Requirement already satisfied: llama-index-embeddings-clip in /opt/anaconda3/lib/python3.12/site-packages (0.4.0)\n",
            "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (11.2.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-multi-modal-llms-ollama) (0.12.30)\n",
            "Requirement already satisfied: ollama>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-multi-modal-llms-ollama) (0.4.7)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (3.10.5)\n",
            "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2.1.1)\n",
            "Requirement already satisfied: dataclasses-json in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (3.9.1)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.26.4)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (4.13.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.14.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.11.0)\n",
            "Requirement already satisfied: griffe in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.7.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (3.1.4)\n",
            "Requirement already satisfied: platformdirs in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (4.3.7)\n",
            "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (4.2.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.0.2)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (3.7)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (0.14.0)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2024.9.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.26.20)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /opt/anaconda3/lib/python3.12/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (24.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-multi-modal-llms-ollama) (2.1.3)\n",
            "Requirement already satisfied: llama-index in /opt/anaconda3/lib/python3.12/site-packages (0.12.30)\n",
            "Requirement already satisfied: llama-index-core in /opt/anaconda3/lib/python3.12/site-packages (0.12.30)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-ollama in /opt/anaconda3/lib/python3.12/site-packages (0.4.0)\n",
            "Requirement already satisfied: llama-index-embeddings-clip in /opt/anaconda3/lib/python3.12/site-packages (0.4.0)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /opt/anaconda3/lib/python3.12/site-packages (0.5.3)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.1)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.6.11)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.3.33)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.7)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (3.10.5)\n",
            "Requirement already satisfied: banks<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2.1.1)\n",
            "Requirement already satisfied: dataclasses-json in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (3.3)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (11.2.1)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (4.13.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-core) (1.14.1)\n",
            "Requirement already satisfied: ollama>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-multi-modal-llms-ollama) (0.4.7)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.30.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-embeddings-huggingface) (4.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.11.0)\n",
            "Requirement already satisfied: griffe in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core) (1.7.2)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core) (3.1.4)\n",
            "Requirement already satisfied: platformdirs in /opt/anaconda3/lib/python3.12/site-packages (from banks<3.0.0,>=2.0.0->llama-index-core) (4.3.7)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.73.0)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.18)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.4.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.12)\n",
            "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (4.2.0)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (1.0.2)\n",
            "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (3.7)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx->llama-index-core) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic>=2.8.0->llama-index-core) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31.0->llama-index-core) (1.26.20)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.51.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.6.0)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.1)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.0.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.5)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.12 in /opt/anaconda3/lib/python3.12/site-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.12)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.5.3)\n",
            "Requirement already satisfied: colorama>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from griffe->banks<3.0.0,>=2.0.0->llama-index-core) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2023.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from llama-cloud-services>=0.6.12->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.16.0)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/v3/j0t3d9ws5bzdcq6f1g3c9thr0000gn/T/pip-req-build-o7c0x1q5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/v3/j0t3d9ws5bzdcq6f1g3c9thr0000gn/T/pip-req-build-o7c0x1q5\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: ftfy in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (4.66.5)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (2.6.0)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (from clip==1.0) (0.21.0)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (4.13.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
            "Requirement already satisfied: torchaudio in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: PyMuPDF in /opt/anaconda3/lib/python3.12/site-packages (1.26.3)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
            "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
            "Requirement already satisfied: clip-openai in /opt/anaconda3/lib/python3.12/site-packages (1.0.post20230121)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: ftfy in /opt/anaconda3/lib/python3.12/site-packages (from clip-openai) (6.3.1)\n",
            "Requirement already satisfied: regex in /opt/anaconda3/lib/python3.12/site-packages (from clip-openai) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from clip-openai) (4.66.5)\n",
            "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from ftfy->clip-openai) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tf_keras\n",
        "!pip install frontend\n",
        "!pip install PyMuPDF --force-reinstall\n",
        "!pip install torch torchvision sentence-transformers transformers ollama langchain\n",
        "!pip install llama-index-multi-modal-llms-ollama llama-index-embeddings-clip Pillow\n",
        "!pip install llama-index llama-index-core llama-index-multi-modal-llms-ollama llama-index-embeddings-clip llama-index-embeddings-huggingface\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install PyMuPDF torch torchvision clip-openai \n",
        "# Note: 'clip-openai' is the package installed via git previously. 'langchain' might be needed for the output parser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama imported successfully!\n",
            "Ollama LLM instantiated!\n"
          ]
        }
      ],
      "source": [
        "# (Checkpoint) To verify if the local Ollama LLM successfully installed and it's available on your computer\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "print(\"Ollama imported successfully!\")  # If this prints, the import works\n",
        "\n",
        "# If the import works, then try a minimal Ollama interaction:\n",
        "\n",
        "# You can type this command (in the terminal of your computer) to use this LLM : ollama run llama3.2\n",
        "try:\n",
        "  llm = Ollama(model=\"llama4\") # or another model available to you\n",
        "  print(\"Ollama LLM instantiated!\")\n",
        "except Exception as e:\n",
        "  print(f\"Error instantiating Ollama: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-15 20:48:31,217 - INFO - Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up global settings...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-15 20:48:46,241 - INFO - 2 prompts are loaded, with the keys: ['query', 'text']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding models set.\n",
            "Starting multimodal script...\n",
            "Event loop already running...\n",
            "Found 120 PDF files in 'arxiv_pdfs'.\n",
            "Attempting to load index from 'storage_multimodal'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-15 20:48:52,008 - INFO - Loading all indices.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index loaded successfully from storage. (Took 4.15s)\n",
            "Query Engine created. Retriever top_k=3\n",
            "\n",
            "Configuration: Will send images to LLM: True\n",
            "\n",
            "--- Generating Result for Query: 'Find all documents (with texts, images, charts or graphs), related to the generation of alpha factors.' ---\n",
            "--- Retrieving top 3 nodes for query: 'Find all documents (with texts, images, charts or graphs), related to the generation of alpha factor...' ---\n",
            "--- Retrieved 5 nodes total ---\n",
            "    Found TextNode: c44a71a0-eb5d-48e1-bf72-185716de7526 (Score: 0.6640)\n",
            "    Found TextNode: 21064109-3ad5-4020-8b09-829a1b1dc383 (Score: 0.6489)\n",
            "    Found TextNode: ec116cd3-55ed-4e1c-8752-5f01c2ed2f17 (Score: 0.6340)\n",
            "    Found ImageNode: 9da09395-8ce9-4b41-abb6-0c884f7fb965 (Score: 0.2907)\n",
            "    Found ImageNode: 0693c029-bd50-45f2-b002-ca7f3fd9a165 (Score: 0.2893)\n",
            "---> Retrieved 2 images for context.\n",
            "--- Encoding retrieved images to base64... ---\n",
            "--- Encoded 2 images successfully. ---\n",
            "Attaching 2 base64 encoded images to the request (send_images_to_llm=True).\n",
            "--- Sending Prompt via underlying ollama client ---\n",
            "Prompt length (approx chars): 6565\n",
            "Number of base64 images being sent: 2\n",
            "---------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-15 20:49:04,334 - INFO - HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted content using attribute access.\n",
            "--- Raw LLM Response ---\n",
            "Here's a list of unique seed alphas related to generating alpha factors for daily stock market data in Hong Kong:\n",
            "\n",
            "```\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Momentum\",\n",
            "      \"name\": \"Price Momentum (10 days)\",\n",
            "      \"code\": \"((CLOSE - DELAY(CLOSE, 10)) / DELAY(CLOSE, 10))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Momentum\",\n",
            "      \"name\": \"Moving Average Crossover (10 vs 50 day)\",\n",
            "      \"code\": \"(SMA(CLOSE, 10) - SMA(CLOSE, 50))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Momentum\",\n",
            "      \"name\": \"Volume Momentum\",\n",
            "      \"code\": \"VOLUME - DELAY(VOLUME, 50)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Mean Reversion (20 days)\",\n",
            "      \"code\": \"(MEAN(CLOSE, 20) - CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Moving Average Reversion\",\n",
            "      \"code\": \"(SMA(CLOSE, 20) - CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Stochastic Oscillator (%K, 14-day)\",\n",
            "      \"code\": \"(CLOSE - MIN(LOW, 14)) / (MAX(HIGH, 14) - MIN(LOW, 14))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Average True Range (ATR, 14-day)\",\n",
            "      \"code\": \"(ATR(14))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Daily High-Low Range\",\n",
            "      \"code\": \"(HIGH - LOW)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Normalized Bollinger Band Width (20-day, 2 StdDev)\",\n",
            "      \"code\": \"((SMA(CLOSE, 20) + 2 * STD(CLOSE, 20)) - (SMA(CLOSE, 20) - 2 * STD(CLOSE, 20))) / SMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Volume Rate of Change (VROC, 10-day)\",\n",
            "      \"code\": \"((VOLUME / DELAY(VOLUME, 10)) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Trading Volume\",\n",
            "      \"code\": \"VOLUME\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Technical\",\n",
            "      \"name\": \"Moving Average (MA)\",\n",
            "      \"code\": \"SMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Technical\",\n",
            "      \"name\": \"Exponential Moving Average (MA)\",\n",
            "      \"code\": \"EMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Technical\",\n",
            "      \"name\": \"Relative Strength Index (RSI)\",\n",
            "      \"code\": \"RSI(14)\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n",
            "------------------------\n",
            "Extracted JSON via markdown.\n",
            "Successfully parsed JSON and validated structure.\n",
            "--- Alpha generation call completed. Retrieved 2 image node(s) during the process. ---\n",
            "\n",
            "--- Final Result ---\n",
            "Generated Alphas (JSON Output):\n",
            "{\n",
            "  \"alphas\": [\n",
            "    {\n",
            "      \"domain\": \"Momentum\",\n",
            "      \"name\": \"Price Momentum (10 days)\",\n",
            "      \"code\": \"((CLOSE - DELAY(CLOSE, 10)) / DELAY(CLOSE, 10))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Momentum\",\n",
            "      \"name\": \"Moving Average Crossover (10 vs 50 day)\",\n",
            "      \"code\": \"(SMA(CLOSE, 10) - SMA(CLOSE, 50))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Momentum\",\n",
            "      \"name\": \"Volume Momentum\",\n",
            "      \"code\": \"VOLUME - DELAY(VOLUME, 50)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Mean Reversion (20 days)\",\n",
            "      \"code\": \"(MEAN(CLOSE, 20) - CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Moving Average Reversion\",\n",
            "      \"code\": \"(SMA(CLOSE, 20) - CLOSE)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Mean Reversion\",\n",
            "      \"name\": \"Stochastic Oscillator (%K, 14-day)\",\n",
            "      \"code\": \"(CLOSE - MIN(LOW, 14)) / (MAX(HIGH, 14) - MIN(LOW, 14))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Average True Range (ATR, 14-day)\",\n",
            "      \"code\": \"(ATR(14))\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Daily High-Low Range\",\n",
            "      \"code\": \"(HIGH - LOW)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Volatility\",\n",
            "      \"name\": \"Normalized Bollinger Band Width (20-day, 2 StdDev)\",\n",
            "      \"code\": \"((SMA(CLOSE, 20) + 2 * STD(CLOSE, 20)) - (SMA(CLOSE, 20) - 2 * STD(CLOSE, 20))) / SMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Volume Rate of Change (VROC, 10-day)\",\n",
            "      \"code\": \"((VOLUME / DELAY(VOLUME, 10)) - 1)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Liquidity\",\n",
            "      \"name\": \"Trading Volume\",\n",
            "      \"code\": \"VOLUME\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Technical\",\n",
            "      \"name\": \"Moving Average (MA)\",\n",
            "      \"code\": \"SMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Technical\",\n",
            "      \"name\": \"Exponential Moving Average (MA)\",\n",
            "      \"code\": \"EMA(CLOSE, 20)\"\n",
            "    },\n",
            "    {\n",
            "      \"domain\": \"Technical\",\n",
            "      \"name\": \"Relative Strength Index (RSI)\",\n",
            "      \"code\": \"RSI(14)\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "--------------------\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "# (Keep all imports the same as the previous working version)\n",
        "import torch\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "import base64\n",
        "import io\n",
        "import os\n",
        "import glob\n",
        "import traceback\n",
        "import sys\n",
        "import pkg_resources\n",
        "import pymupdf\n",
        "from PIL import Image\n",
        "try:\n",
        "    from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "except ImportError:\n",
        "    # Dummy classes if langchain not installed\n",
        "    class ResponseSchema:\n",
        "        def __init__(self, name, description): self.name = name; self.description = description\n",
        "    class StructuredOutputParser:\n",
        "        @staticmethod\n",
        "        def from_response_schemas(schemas):\n",
        "            class DummyParser:\n",
        "                 def get_format_instructions(self): return \"Format instructions N/A.\"\n",
        "            return DummyParser()\n",
        "\n",
        "from llama_index.core import Settings, StorageContext, load_index_from_storage, VectorStoreIndex, Document\n",
        "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
        "from llama_index.core.schema import ImageDocument, ImageNode as LlamaIndexImageNode\n",
        "from llama_index.multi_modal_llms.ollama import OllamaMultiModal\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.embeddings.clip import ClipEmbedding\n",
        "from typing import List, Dict, Sequence, Tuple\n",
        "\n",
        "# --- Global Settings & Instantiation ---\n",
        "# (Keep setup the same)\n",
        "print(\"Setting up global settings...\")\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "image_embed_model = ClipEmbedding(embed_batch_size=1)\n",
        "print(\"Embedding models set.\")\n",
        "\n",
        "# --- DEFINE LLM PARAMETERS ---\n",
        "LLM_MODEL = \"llama4\"  \n",
        "LLM_MAX_NEW_TOKENS = 2000\n",
        "LLM_TEMPERATURE = 0.1\n",
        "LLM_TIMEOUT = 720.0\n",
        "\n",
        "multi_modal_llm = OllamaMultiModal(\n",
        "    model=LLM_MODEL,\n",
        "    max_new_tokens=LLM_MAX_NEW_TOKENS,\n",
        "    temperature=LLM_TEMPERATURE,\n",
        "    request_timeout=LLM_TIMEOUT,\n",
        ")\n",
        "\n",
        "# --- Helper Function ---\n",
        "# (Keep pil_image_to_base64 the same)\n",
        "def pil_image_to_base64(pil_image: Image.Image, format=\"JPEG\") -> str:\n",
        "    buffered = io.BytesIO()\n",
        "    if format == \"PNG\" and pil_image.mode in (\"RGBA\", \"LA\"):\n",
        "        pil_image.save(buffered, format=\"PNG\")\n",
        "    elif pil_image.mode == \"P\":\n",
        "        pil_image = pil_image.convert(\"RGB\")\n",
        "        pil_image.save(buffered, format=\"JPEG\", quality=90)\n",
        "    else:\n",
        "        if pil_image.mode != \"RGB\":\n",
        "            pil_image = pil_image.convert(\"RGB\")\n",
        "        pil_image.save(buffered, format=\"JPEG\", quality=90)\n",
        "    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "    return img_str\n",
        "\n",
        "# --- Functions ---\n",
        "\n",
        "# (Keep process_pdf_for_multimodal the same as your last working version)\n",
        "def process_pdf_for_multimodal(pdf_path) -> List:\n",
        "    \"\"\" Extracts text/images from PDF using PyMuPDF with logging. \"\"\"\n",
        "    all_documents = []\n",
        "    page_image_counts = {}\n",
        "    total_images_processed = 0\n",
        "    pdf_basename = os.path.basename(pdf_path)\n",
        "    try:\n",
        "        doc = pymupdf.open(pdf_path)\n",
        "    except Exception as e:\n",
        "        print(f\"!!! ERROR opening PDF {pdf_basename}: {type(e).__name__}: {e}. Skipping.\")\n",
        "        return all_documents\n",
        "    print(f\"  Processing {doc.page_count} pages in {pdf_basename}...\")\n",
        "    for page_num, page in enumerate(doc):\n",
        "        page_id = f\"{pdf_basename}_page_{page_num+1}\"\n",
        "        shared_metadata = {\"source_file\": pdf_basename, \"page_number\": page_num + 1}\n",
        "        images_on_page = 0\n",
        "        try: # Text Extraction\n",
        "            page_text = page.get_text(sort=True)\n",
        "            if page_text.strip():\n",
        "                all_documents.append(Document(text=page_text, id_=f\"{page_id}_text\", metadata=shared_metadata.copy()))\n",
        "        except Exception as e: print(f\"    Page {page_num+1}: Error extracting text: {e}\")\n",
        "        try: # Image Extraction\n",
        "            image_list = page.get_images(full=True)\n",
        "            # if image_list: print(f\"    Page {page_num+1}: Found {len(image_list)} potential image(s).\") # Optional log\n",
        "            for img_index, img_info in enumerate(image_list):\n",
        "                xref = img_info[0]\n",
        "                if xref == 0: continue\n",
        "                try:\n",
        "                    base_image = doc.extract_image(xref)\n",
        "                    if base_image and base_image.get(\"image\"):\n",
        "                        image_bytes_from_pdf = base_image[\"image\"]\n",
        "                        # print(f\"      Page {page_num+1}, Img {img_index} (xref {xref}): Extracted {len(image_bytes_from_pdf)} bytes. Validating...\") # Optional log\n",
        "                        try:\n",
        "                            img_io = io.BytesIO(image_bytes_from_pdf)\n",
        "                            img_pil = Image.open(img_io); img_pil.verify()\n",
        "                            # print(f\"      Page {page_num+1}, Img {img_index} (xref {xref}): Validation OK.\") # Optional log\n",
        "                            all_documents.append(ImageDocument(image=image_bytes_from_pdf, id_=f\"{page_id}_img_{img_index}\", metadata=shared_metadata.copy()))\n",
        "                            images_on_page += 1; total_images_processed += 1\n",
        "                        except Exception as img_err: print(f\"    ! Page {page_num+1}, Img {img_index} (xref {xref}): ERROR validating: {type(img_err).__name__}: {img_err}. Skipping.\")\n",
        "                    # else: print(f\"    ! Page {page_num+1}, Img {img_index} (xref {xref}): No image data extracted.\") # Optional log\n",
        "                except Exception as extract_err: print(f\"    ! Page {page_num+1}, Img {img_index} (xref {xref}): ERROR calling extract_image: {type(extract_err).__name__}: {extract_err}. Skipping.\")\n",
        "        except Exception as e: print(f\"    ! Page {page_num+1}: ERROR during image loop: {type(e).__name__}: {e}\")\n",
        "        if images_on_page > 0: page_image_counts[page_num + 1] = images_on_page\n",
        "    doc.close()\n",
        "    print(f\"  ---> Finished {pdf_basename}. Validated {total_images_processed} images. Pages: {list(page_image_counts.keys()) if page_image_counts else 'None'}\")\n",
        "    return all_documents\n",
        "\n",
        "# (Keep prepare_multimodal_context_and_images the same as your last working version)\n",
        "async def prepare_multimodal_context_and_images(query: str, query_engine):\n",
        "    \"\"\" Retrieves context and image docs. \"\"\"\n",
        "    text_context = \"\"\n",
        "    retrieved_image_docs = []\n",
        "    try:\n",
        "        retriever = query_engine.retriever\n",
        "        top_k_val = getattr(retriever, 'similarity_top_k', 'Default') # Access from retriever\n",
        "        print(f\"--- Retrieving top {top_k_val} nodes for query: '{query[:100]}...' ---\")\n",
        "        retrieved_nodes = await retriever.aretrieve(query)\n",
        "        print(f\"--- Retrieved {len(retrieved_nodes)} nodes total ---\")\n",
        "        if not retrieved_nodes:\n",
        "            print(\"Warning: No documents or images retrieved for the query.\")\n",
        "            return \"No relevant context found.\", []\n",
        "        doc_texts = []\n",
        "        for RNode in retrieved_nodes:\n",
        "            node = RNode.node\n",
        "            score_str = f\"{RNode.score:.4f}\" if RNode.score is not None else \"N/A\"\n",
        "            source_info = f\"Source: {node.metadata.get('source_file', 'Unknown')}, Page: {node.metadata.get('page_number', 'N/A')}, Score: {score_str}\"\n",
        "            if isinstance(node, (ImageDocument, LlamaIndexImageNode)):\n",
        "                 retrieved_image_docs.append(node)\n",
        "                 doc_texts.append(f\"--- Retrieved Image ---\\n[{source_info}]\\n--- End Image ---\")\n",
        "                 print(f\"    Found ImageNode: {node.id_} (Score: {score_str})\")\n",
        "            else:\n",
        "                 try: node_content = node.get_content(metadata_mode='all')[:200] + \"...\"\n",
        "                 except Exception: node_content = \"[Could not get node content]...\"\n",
        "                 doc_texts.append(f\"--- Retrieved Document ---\\n[{node_content}]\\n[{source_info}]\\n--- End Document ---\")\n",
        "                 print(f\"    Found TextNode: {node.id_} (Score: {score_str})\")\n",
        "        text_context = \"\\n\\n\".join(doc_texts).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during retrieval: {e}\"); traceback.print_exc()\n",
        "        return \"Error retrieving context.\", []\n",
        "    print(f\"---> Retrieved {len(retrieved_image_docs)} images for context.\")\n",
        "    return text_context, retrieved_image_docs\n",
        "\n",
        "\n",
        "# *************************************************************************\n",
        "# *** MODIFIED generate_seed_alphas to de-emphasize context slightly ***\n",
        "# *************************************************************************\n",
        "async def generate_seed_alphas(\n",
        "    query: str,\n",
        "    query_engine,\n",
        "    llm_instance,\n",
        "    send_images_to_llm: bool = False # *** ADDED FLAG to control image sending ***\n",
        "    ) -> Tuple[Dict, List]:\n",
        "    \"\"\"\n",
        "    Generates seed alphas using Multimodal LLM with retrieved text and image context.\n",
        "    Includes options to de-emphasize context and returns retrieved images list.\n",
        "    Correctly handles Ollama response object.\n",
        "    \"\"\"\n",
        "    parsed_data_result = {\"alphas\": []}\n",
        "    retrieved_images_for_return = []\n",
        "\n",
        "    # 1. Get context string and LlamaIndex ImageDocument objects\n",
        "    context_str, retrieved_llamaindex_image_docs = await prepare_multimodal_context_and_images(query, query_engine)\n",
        "    retrieved_images_for_return = retrieved_llamaindex_image_docs\n",
        "\n",
        "    # 2. Define the expected JSON output structure (remains the same)\n",
        "    response_schemas = [\n",
        "        ResponseSchema(name=\"alphas\", description=\"A list of alpha objects...\"),\n",
        "    ]\n",
        "\n",
        "    # 3. Construct the LLM Prompt string\n",
        "    # *** ADDED clarifying sentence in Instructions ***\n",
        "    prompt_content = f\"\"\"Based on the following context documents, generate *high-quality, independent and unique seed alphas* related to: {query}.\n",
        "\n",
        "            Context Documents (Use these for inspiration or validation, but prioritize the core instructions and examples below):\n",
        "            ```\n",
        "            {context_str if context_str else \"No text context retrieved.\"}\n",
        "            ```\n",
        "\n",
        "            Instructions:\n",
        "            1. Categorize the alphas into one of these financial domains: Momentum, Mean Reversion, Volatility, Fundamental, Liquidity, Quality, Growth, Technical, Macro Economics.\n",
        "            2. Provide a descriptive 'name' for each alpha.\n",
        "            3. Provide the calculation 'code' (formula) for each alpha.\n",
        "            4. Focus on alphas factors suitable for *daily stock market data in Hong Kong*.\n",
        "            5. The 'code' (formula) *MUST ONLY* use 'OPEN', 'LOW', 'HIGH', 'CLOSE', 'VOLUME', standard arithmetic/logical operators, or functions commonly found in libraries like TA-Lib (e.g., SMA, EMA, RSI, ATR, STD, MIN, MAX, DELAY, MEAN). Assume DELAY(X, n) means value of X n periods ago. Use standard function names.\n",
        "            6. Ensure the 'name' of each alpha factor accurately reflects its 'code' (formula).\n",
        "            7. Verify the correctness of each generated alpha factor's formula logic. Avoid trivial or redundant alphas.\n",
        "            8. **Crucially, rely primarily on the examples below and the general task description to generate diverse alphas. Treat the 'Context Documents' above as supplementary.**\n",
        "            9. **YOU MUST NOT GENERATE ANY PYTHON CODES.**\n",
        "\n",
        "            Output Format:\n",
        "            Return the result as a *single, valid JSON object (dictionary)*. The JSON object *must* strictly adhere to the following structure, with no extra text, comments, or explanations before or after the JSON block:\n",
        "\n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Example Domain\",\n",
        "                \"name\": \"Example Alpha Name\",\n",
        "                \"code\": \"Example Formula (e.g., CLOSE - DELAY(CLOSE, 1))\"\n",
        "                }},\n",
        "                // ... more alpha objects can follow here\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "\n",
        "            Example Alphas (Illustrative - Generate new ones based on context and query):\n",
        "            \n",
        "            ```json\n",
        "            {{\n",
        "            \"alphas\": [\n",
        "                {{\n",
        "                \"domain\": \"Momentum\",\n",
        "                \"name\": \"Price Momentum (10 days)\",\n",
        "                \"code\": \"((CLOSE - DELAY(CLOSE, 10)) / DELAY(CLOSE, 10))\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Momentum\",\n",
        "                \"name\": \"Moving Average Crossover (10 vs 50 day)\",\n",
        "                \"code\": \"(SMA(CLOSE, 10) - SMA(CLOSE, 50))\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Momentum\",\n",
        "                \"name\": \"Volume Momentum\",\n",
        "                \"code\": \"VOLUME - DELAY(VOLUME, 50))\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Mean Reversion\",\n",
        "                \"name\": \"Mean Reversion (20 days)\",\n",
        "                \"code\": \"(MEAN(CLOSE, 20) - CLOSE)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Mean Reversion\",\n",
        "                \"name\": \"Moving Average Reversion\",\n",
        "                \"code\": \"(SMA(CLOSE, 20) - CLOSE)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Mean Reversion\",\n",
        "                \"name\": \"Stochastic Oscillator (%K, 14-day)\",\n",
        "                \"code\": \"(CLOSE - MIN(LOW, 14)) / (MAX(HIGH, 14) - MIN(LOW, 14))\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Volatility\",\n",
        "                \"name\": \"Average True Range (ATR, 14-day)\",\n",
        "                \"code\": \"(ATR(14))\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Volatility\",\n",
        "                \"name\": \"Daily High-Low Range\",\n",
        "                \"code\": \"(HIGH - LOW)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Volatility\",\n",
        "                \"name\": \"Normalized Bollinger Band Width (20-day, 2 StdDev)\",\n",
        "                \"code\": \"((SMA(CLOSE, 20) + 2 * STD(CLOSE, 20)) - (SMA(CLOSE, 20) - 2 * STD(CLOSE, 20))) / SMA(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Liquidity\",\n",
        "                \"name\": \"Volume Rate of Change (VROC, 10-day)\",\n",
        "                \"code\": \"((VOLUME / DELAY(VOLUME, 10)) - 1)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Liquidity\",\n",
        "                \"name\": \"Trading Volume\",\n",
        "                \"code\": \"VOLUME\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Technical\",\n",
        "                \"name\": \"Moving Average (MA)\",\n",
        "                \"code\": \"SMA(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Technical\",\n",
        "                \"name\": \"Exponential Moving Average (MA)\",\n",
        "                \"code\": \"EMA(CLOSE, 20)\"\n",
        "                }},\n",
        "                {{\n",
        "                \"domain\": \"Technical\",\n",
        "                \"name\": \"Relative Strength Index (RSI)\",\n",
        "                \"code\": \"RSI(14)\"\n",
        "                }},\n",
        "                // ... more examples\n",
        "            ]\n",
        "            }}\n",
        "            ```\n",
        "            Ensure the entire response is only the valid JSON object shown above. If no relevant alphas can be generated, return an empty list within the JSON: {{\"alphas\": []}}. Ensure all keys and string values within the JSON are enclosed in double quotes. Do not add any introductory text like \"Here is the JSON...\" before the opening brace {{.\n",
        "            \"\"\"\n",
        "\n",
        "    # 4. Prepare messages and images for the underlying ollama client\n",
        "    ollama_messages = []\n",
        "    base64_images = [] # Images encoded, ready to be potentially sent\n",
        "    encoded_image_count = 0\n",
        "    if retrieved_llamaindex_image_docs:\n",
        "        print(\"--- Encoding retrieved images to base64... ---\")\n",
        "        # (Keep the working image decoding/encoding loop)\n",
        "        for img_doc in retrieved_llamaindex_image_docs:\n",
        "            # ... (logic to get img_bytes_to_encode from img_doc.image (str or bytes)) ...\n",
        "            img_bytes_to_encode = None\n",
        "            img_data_from_doc = img_doc.image\n",
        "            if isinstance(img_data_from_doc, str):\n",
        "                try:\n",
        "                    if len(img_data_from_doc) < 260 and os.path.exists(img_data_from_doc): continue # Skip paths\n",
        "                    img_bytes_to_encode = base64.b64decode(img_data_from_doc)\n",
        "                except Exception: continue # Skip decode errors\n",
        "            elif isinstance(img_data_from_doc, bytes):\n",
        "                img_bytes_to_encode = img_data_from_doc\n",
        "            else: continue # Skip other types\n",
        "\n",
        "            if img_bytes_to_encode:\n",
        "                try:\n",
        "                    pil_img = Image.open(io.BytesIO(img_bytes_to_encode))\n",
        "                    img_format = \"JPEG\" if pil_img.mode != \"PNG\" else \"PNG\"\n",
        "                    b64_string = pil_image_to_base64(pil_img, format=img_format)\n",
        "                    base64_images.append(b64_string) # Add to list of *potentially* sent images\n",
        "                    encoded_image_count += 1\n",
        "                except Exception as encode_err:\n",
        "                    print(f\"    Warning: Failed Pillow/base64 encoding for {img_doc.id_}: {encode_err}\")\n",
        "        print(f\"--- Encoded {encoded_image_count} images successfully. ---\")\n",
        "\n",
        "    # Prepare user message content\n",
        "    user_message_dict = { \"role\": \"user\", \"content\": prompt_content }\n",
        "\n",
        "    # *** Conditionally add images based on the flag ***\n",
        "    images_actually_sent = []\n",
        "    if base64_images and send_images_to_llm:\n",
        "        print(f\"Attaching {len(base64_images)} base64 encoded images to the request (send_images_to_llm=True).\")\n",
        "        user_message_dict[\"images\"] = base64_images\n",
        "        images_actually_sent = base64_images # Track what was sent\n",
        "    elif base64_images and not send_images_to_llm:\n",
        "         print(\"INFO: Images were retrieved and encoded, but configured NOT to be sent to the LLM (send_images_to_llm=False).\")\n",
        "         # Do not add the 'images' key to user_message_dict\n",
        "\n",
        "    ollama_messages.append(user_message_dict)\n",
        "\n",
        "    # 5. Query the Multimodal LLM using the underlying client and Parse the Response\n",
        "    print(\"--- Sending Prompt via underlying ollama client ---\")\n",
        "    print(f\"Prompt length (approx chars): {len(prompt_content)}\")\n",
        "    print(f\"Number of base64 images being sent: {len(images_actually_sent)}\") # Log count *actually* sent\n",
        "    print(\"---------------------------------------------------------------------------\")\n",
        "\n",
        "    try:\n",
        "        # (Keep Ollama client checking and calling logic the same)\n",
        "        if not hasattr(llm_instance, '_client') and not hasattr(llm_instance, '_aclient'):\n",
        "             raise AttributeError(\"LLM instance missing compatible Ollama client.\")\n",
        "        llm_options = { \"temperature\": LLM_TEMPERATURE, \"num_predict\": LLM_MAX_NEW_TOKENS }\n",
        "        if hasattr(llm_instance, '_aclient'):\n",
        "            response_obj = await llm_instance._aclient.chat( model=llm_instance.model, messages=ollama_messages, options=llm_options )\n",
        "        # ... (elif for sync client) ...\n",
        "        else: raise AttributeError(\"No Ollama client found.\")\n",
        "\n",
        "        # (Keep the corrected response object handling logic the same)\n",
        "        completion_text = None\n",
        "        if response_obj and hasattr(response_obj, 'message') and response_obj.message and \\\n",
        "           hasattr(response_obj.message, 'content') and isinstance(response_obj.message.content, str):\n",
        "            completion_text = response_obj.message.content.strip()\n",
        "            print(\"Successfully extracted content using attribute access.\")\n",
        "        else:\n",
        "            print(f\"Warning: Unexpected response structure: {response_obj}\")\n",
        "            return parsed_data_result, retrieved_images_for_return\n",
        "\n",
        "        print(f\"--- Raw LLM Response ---\")\n",
        "        print(completion_text)\n",
        "        print(\"------------------------\")\n",
        "\n",
        "        # (Keep JSON Parsing logic the same)\n",
        "        json_string = None\n",
        "        json_match = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', completion_text, re.DOTALL | re.IGNORECASE)\n",
        "        if json_match: json_string = json_match.group(1); print(\"Extracted JSON via markdown.\")\n",
        "        elif completion_text.startswith('{') and completion_text.endswith('}'): json_string = completion_text; print(\"Assuming raw response is JSON.\")\n",
        "        else: # Loose extraction\n",
        "             json_start = completion_text.find('{'); json_end = completion_text.rfind('}')\n",
        "             if json_start != -1 and json_end > json_start: json_string = completion_text[json_start:json_end+1]; print(\"Warning: Using loose JSON extraction.\")\n",
        "             else: print(\"ERROR: No JSON block found.\"); return parsed_data_result, retrieved_images_for_return\n",
        "\n",
        "        if json_string:\n",
        "            try:\n",
        "                parsed_data = json.loads(json_string)\n",
        "                if isinstance(parsed_data, dict) and isinstance(parsed_data.get(\"alphas\"), list):\n",
        "                     print(\"Successfully parsed JSON and validated structure.\")\n",
        "                     parsed_data_result = parsed_data\n",
        "                else: print(\"Error: Parsed JSON lacks expected structure.\")\n",
        "            except json.JSONDecodeError as json_err: print(f\"Error decoding JSON: {json_err}\\nString: {json_string}\")\n",
        "            except Exception as parse_error: print(f\"Error parsing JSON: {parse_error}\\nString: {json_string}\")\n",
        "\n",
        "        return parsed_data_result, retrieved_images_for_return\n",
        "\n",
        "    # (Keep except blocks the same)\n",
        "    except AttributeError as attr_err: print(f\"Attribute Error during LLM call: {attr_err}\"); traceback.print_exc(); return parsed_data_result, retrieved_images_for_return\n",
        "    except Exception as llm_error: print(f\"Error during Ollama call: {llm_error}\"); traceback.print_exc(); return parsed_data_result, retrieved_images_for_return\n",
        "\n",
        "\n",
        "# --- Main Function ---\n",
        "async def main():\n",
        "    \"\"\"Main execution function: Sets up MULTIMODAL index, runs query, generates alphas.\"\"\"\n",
        "    data_dir = \"arxiv_pdfs\"\n",
        "    persist_dir = \"storage_multimodal\"\n",
        "    # *** REDUCED TOP_K to limit context ***\n",
        "    similarity_top_k_setting = 3 # Retrieve fewer documents/images\n",
        "\n",
        "    # --- Environment Setup & PDF Check --- (Keep as is)\n",
        "    if not os.path.exists(data_dir): print(f\"Error: PDF dir '{data_dir}' not found.\"); return None\n",
        "    pdf_files = glob.glob(os.path.join(data_dir, \"*.pdf\"))\n",
        "    print(f\"Found {len(pdf_files)} PDF files in '{data_dir}'.\")\n",
        "\n",
        "    index = None; query_engine = None; llm_to_use = multi_modal_llm\n",
        "\n",
        "    # --- Load or Create MULTIMODAL Index ---\n",
        "    try: # Load index\n",
        "        if not os.path.exists(persist_dir): raise FileNotFoundError(\"Persistence dir not found.\")\n",
        "        print(f\"Attempting to load index from '{persist_dir}'...\"); t_start_load = asyncio.get_event_loop().time()\n",
        "        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
        "        index = load_index_from_storage(storage_context, image_embed_model=image_embed_model)\n",
        "        print(f\"Index loaded successfully from storage. (Took {asyncio.get_event_loop().time() - t_start_load:.2f}s)\")\n",
        "        query_engine = index.as_query_engine(llm=llm_to_use, image_embed_model=image_embed_model, similarity_top_k = similarity_top_k_setting)\n",
        "        print(f\"Query Engine created. Retriever top_k={query_engine.retriever.similarity_top_k}\")\n",
        "    except Exception as load_error: # Create index\n",
        "        print(f\"Info: Failed to load index ({type(load_error).__name__}).\")\n",
        "        if not pdf_files: print(\"Error: No PDFs found and cannot load index.\"); return None\n",
        "        print(f\"Creating new index from '{data_dir}'...\")\n",
        "        try:\n",
        "            print(\"--- Processing PDFs (with enhanced logging)... ---\"); t_start_process = asyncio.get_event_loop().time()\n",
        "            all_documents = []\n",
        "            # Consider using asyncio.gather for parallel processing if many PDFs\n",
        "            for pdf_path in pdf_files:\n",
        "                processed_docs = process_pdf_for_multimodal(pdf_path)\n",
        "                all_documents.extend(processed_docs)\n",
        "            print(f\"--- Finished processing PDFs. (Took {asyncio.get_event_loop().time() - t_start_process:.2f}s) ---\")\n",
        "            if not all_documents: print(\"Error: No documents processed.\"); return None\n",
        "            text_count = sum(1 for d in all_documents if isinstance(d, Document) and not isinstance(d, ImageDocument))\n",
        "            img_count = sum(1 for d in all_documents if isinstance(d, ImageDocument))\n",
        "            print(f\"Processed {len(all_documents)} total objects ({text_count} text, {img_count} image).\")\n",
        "            if img_count == 0: print(\"Warning: No images processed.\")\n",
        "\n",
        "            print(\"Building Multimodal Index (this may take time)...\"); t_start_build = asyncio.get_event_loop().time()\n",
        "            storage_context = StorageContext.from_defaults()\n",
        "            index = MultiModalVectorStoreIndex.from_documents(all_documents, storage_context=storage_context, image_embed_model=image_embed_model, show_progress=True)\n",
        "            print(f\"Index created successfully. (Took {asyncio.get_event_loop().time() - t_start_build:.2f}s)\")\n",
        "            print(f\"Persisting index to '{persist_dir}'...\"); t_start_persist = asyncio.get_event_loop().time()\n",
        "            os.makedirs(persist_dir, exist_ok=True)\n",
        "            index.storage_context.persist(persist_dir=persist_dir)\n",
        "            print(f\"Index persisted. (Took {asyncio.get_event_loop().time() - t_start_persist:.2f}s)\")\n",
        "            query_engine = index.as_query_engine(llm=llm_to_use, image_embed_model=image_embed_model, similarity_top_k = similarity_top_k_setting)\n",
        "            print(f\"Query Engine created. Retriever top_k={query_engine.retriever.similarity_top_k}\")\n",
        "        except Exception as create_error: print(f\"Fatal Error creating index: {create_error}\"); traceback.print_exc(); return None\n",
        "\n",
        "    # --- Run the Generation ---\n",
        "    final_result_dict = {\"alphas\": []}\n",
        "    if query_engine:\n",
        "        # *** Control whether to send images to LLM ***\n",
        "        # Set to True if using a multimodal LLM (like llava) and you WANT it to see images\n",
        "        # Set to False if using text-only LLM (like llama3.2) or if you want to force reliance on text context only\n",
        "        SEND_IMAGES = True\n",
        "        print(f\"\\nConfiguration: Will send images to LLM: {SEND_IMAGES}\")\n",
        "\n",
        "        # Define query\n",
        "        query = \"Find all documents (with texts, images, charts or graphs), related to the generation of alpha factors.\"\n",
        "        print(f\"\\n--- Generating Result for Query: '{query}' ---\")\n",
        "        try:\n",
        "            # Pass the SEND_IMAGES flag to the generation function\n",
        "            final_result_dict, retrieved_images_list = await generate_seed_alphas(\n",
        "                query, query_engine, llm_to_use, send_images_to_llm=SEND_IMAGES\n",
        "            )\n",
        "            print(f\"--- Alpha generation call completed. Retrieved {len(retrieved_images_list)} image node(s) during the process. ---\")\n",
        "\n",
        "        except Exception as gen_err:\n",
        "             print(f\"Error during alpha generation process: {gen_err}\")\n",
        "             traceback.print_exc()\n",
        "             # Keep final_result_dict as the default empty list\n",
        "\n",
        "        return final_result_dict\n",
        "    else:\n",
        "        print(\"Error: Query engine not initialized.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Script Entry Point ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting multimodal script...\")\n",
        "    final_json_result = None\n",
        "    try:\n",
        "        loop = asyncio.get_running_loop(); print(\"Event loop already running...\"); final_json_result = await main()\n",
        "    except RuntimeError: print(\"No event loop running...\"); final_json_result = asyncio.run(main())\n",
        "    except Exception as main_err: print(f\"\\n!!! Error during main execution: {main_err} !!!\"); traceback.print_exc()\n",
        "\n",
        "    # --- Result Handling ---\n",
        "    print(\"\\n--- Final Result ---\")\n",
        "    # (Keep result printing logic the same)\n",
        "    if final_json_result and isinstance(final_json_result, dict) and 'alphas' in final_json_result:\n",
        "        print(\"Generated Alphas (JSON Output):\")\n",
        "        try: print(json.dumps(final_json_result, indent=2))\n",
        "        except Exception as json_dump_err: print(f\"Error formatting JSON: {json_dump_err}\\nRaw: {final_json_result}\")\n",
        "        print(\"--------------------\")\n",
        "    else:\n",
        "        print(\"Script finished, but final result was not expected alpha dictionary.\")\n",
        "        if final_json_result is not None: print(f\"Final result type: {type(final_json_result)}, value: {final_json_result}\")\n",
        "        else: print(\"Final result was None (likely due to error).\")\n",
        "        print(\"Check logs above.\")\n",
        "\n",
        "    print(\"\\nScript finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the results from LLM to a dataframe, then to a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ],
      "source": [
        "print(type(final_json_result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>domain</th>\n",
              "      <th>name</th>\n",
              "      <th>code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Price Momentum (10 days)</td>\n",
              "      <td>((CLOSE - DELAY(CLOSE, 10)) / DELAY(CLOSE, 10))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Moving Average Crossover (10 vs 50 day)</td>\n",
              "      <td>(SMA(CLOSE, 10) - SMA(CLOSE, 50))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Volume Momentum</td>\n",
              "      <td>VOLUME - DELAY(VOLUME, 50)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Mean Reversion (20 days)</td>\n",
              "      <td>(MEAN(CLOSE, 20) - CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Moving Average Reversion</td>\n",
              "      <td>(SMA(CLOSE, 20) - CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Stochastic Oscillator (%K, 14-day)</td>\n",
              "      <td>(CLOSE - MIN(LOW, 14)) / (MAX(HIGH, 14) - MIN(LOW, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Average True Range (ATR, 14-day)</td>\n",
              "      <td>(ATR(14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Daily High-Low Range</td>\n",
              "      <td>(HIGH - LOW)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Normalized Bollinger Band Width (20-day, 2 StdDev)</td>\n",
              "      <td>((SMA(CLOSE, 20) + 2 * STD(CLOSE, 20)) - (SMA(CLOSE, 20) - 2 * STD(CLOSE, 20))) / SMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Volume Rate of Change (VROC, 10-day)</td>\n",
              "      <td>((VOLUME / DELAY(VOLUME, 10)) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Trading Volume</td>\n",
              "      <td>VOLUME</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Moving Average (MA)</td>\n",
              "      <td>SMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Exponential Moving Average (MA)</td>\n",
              "      <td>EMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Relative Strength Index (RSI)</td>\n",
              "      <td>RSI(14)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            domain                                                name  \\\n",
              "0         Momentum                            Price Momentum (10 days)   \n",
              "1         Momentum             Moving Average Crossover (10 vs 50 day)   \n",
              "2         Momentum                                     Volume Momentum   \n",
              "3   Mean Reversion                            Mean Reversion (20 days)   \n",
              "4   Mean Reversion                            Moving Average Reversion   \n",
              "5   Mean Reversion                  Stochastic Oscillator (%K, 14-day)   \n",
              "6       Volatility                    Average True Range (ATR, 14-day)   \n",
              "7       Volatility                                Daily High-Low Range   \n",
              "8       Volatility  Normalized Bollinger Band Width (20-day, 2 StdDev)   \n",
              "9        Liquidity                Volume Rate of Change (VROC, 10-day)   \n",
              "10       Liquidity                                      Trading Volume   \n",
              "11       Technical                                 Moving Average (MA)   \n",
              "12       Technical                     Exponential Moving Average (MA)   \n",
              "13       Technical                       Relative Strength Index (RSI)   \n",
              "\n",
              "                                                                                                code  \n",
              "0                                                    ((CLOSE - DELAY(CLOSE, 10)) / DELAY(CLOSE, 10))  \n",
              "1                                                                  (SMA(CLOSE, 10) - SMA(CLOSE, 50))  \n",
              "2                                                                         VOLUME - DELAY(VOLUME, 50)  \n",
              "3                                                                          (MEAN(CLOSE, 20) - CLOSE)  \n",
              "4                                                                           (SMA(CLOSE, 20) - CLOSE)  \n",
              "5                                            (CLOSE - MIN(LOW, 14)) / (MAX(HIGH, 14) - MIN(LOW, 14))  \n",
              "6                                                                                          (ATR(14))  \n",
              "7                                                                                       (HIGH - LOW)  \n",
              "8   ((SMA(CLOSE, 20) + 2 * STD(CLOSE, 20)) - (SMA(CLOSE, 20) - 2 * STD(CLOSE, 20))) / SMA(CLOSE, 20)  \n",
              "9                                                                 ((VOLUME / DELAY(VOLUME, 10)) - 1)  \n",
              "10                                                                                            VOLUME  \n",
              "11                                                                                    SMA(CLOSE, 20)  \n",
              "12                                                                                    EMA(CLOSE, 20)  \n",
              "13                                                                                           RSI(14)  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "original_dfs = []\n",
        "\n",
        "\n",
        "alphas = final_json_result.get(\"alphas\", [])\n",
        "original_df_new = pd.DataFrame(alphas)  # Directly create DataFrame from the list of dictionaries\n",
        "original_dfs.append(original_df_new)\n",
        "\n",
        "if original_dfs:\n",
        "    original_combined_df = pd.concat(original_dfs, ignore_index=True)\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "original_combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (Add-on - not compulsory) Address the case when the formulas in code columns have outer quotes with the formulas.\n",
        "import pandas as pd\n",
        "import re  # Import the regular expression library\n",
        "\n",
        "def remove_outer_quotes(code_string):\n",
        "    \"\"\"Removes outer quotes from a string if they exist.\"\"\"\n",
        "    if code_string and code_string.startswith('\"') and code_string.endswith('\"'):\n",
        "        return code_string[1:-1]\n",
        "    return code_string\n",
        "\n",
        "# Apply the function to the 'code' column\n",
        "original_combined_df['code'] = original_combined_df['code'].apply(remove_outer_quotes)\n",
        "\n",
        "# Print the modified DataFrame\n",
        "original_combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>domain</th>\n",
              "      <th>name</th>\n",
              "      <th>code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Price Momentum (10 days)</td>\n",
              "      <td>((CLOSE - DELAY(CLOSE, 10)) / DELAY(CLOSE, 10))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Moving Average Crossover (10 vs 50 day)</td>\n",
              "      <td>(SMA(CLOSE, 10) - SMA(CLOSE, 50))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Momentum</td>\n",
              "      <td>Volume Momentum</td>\n",
              "      <td>VOLUME - DELAY(VOLUME, 50)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Mean Reversion (20 days)</td>\n",
              "      <td>(MEAN(CLOSE, 20) - CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Moving Average Reversion</td>\n",
              "      <td>(SMA(CLOSE, 20) - CLOSE)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Mean Reversion</td>\n",
              "      <td>Stochastic Oscillator (%K, 14-day)</td>\n",
              "      <td>(CLOSE - MIN(LOW, 14)) / (MAX(HIGH, 14) - MIN(LOW, 14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Average True Range (ATR, 14-day)</td>\n",
              "      <td>(ATR(14))</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Daily High-Low Range</td>\n",
              "      <td>(HIGH - LOW)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Volatility</td>\n",
              "      <td>Normalized Bollinger Band Width (20-day, 2 StdDev)</td>\n",
              "      <td>((SMA(CLOSE, 20) + 2 * STD(CLOSE, 20)) - (SMA(CLOSE, 20) - 2 * STD(CLOSE, 20))) / SMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Volume Rate of Change (VROC, 10-day)</td>\n",
              "      <td>((VOLUME / DELAY(VOLUME, 10)) - 1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Liquidity</td>\n",
              "      <td>Trading Volume</td>\n",
              "      <td>VOLUME</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Moving Average (MA)</td>\n",
              "      <td>SMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Exponential Moving Average (MA)</td>\n",
              "      <td>EMA(CLOSE, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Technical</td>\n",
              "      <td>Relative Strength Index (RSI)</td>\n",
              "      <td>RSI(14)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            domain                                                name  \\\n",
              "0         Momentum                            Price Momentum (10 days)   \n",
              "1         Momentum             Moving Average Crossover (10 vs 50 day)   \n",
              "2         Momentum                                     Volume Momentum   \n",
              "3   Mean Reversion                            Mean Reversion (20 days)   \n",
              "4   Mean Reversion                            Moving Average Reversion   \n",
              "5   Mean Reversion                  Stochastic Oscillator (%K, 14-day)   \n",
              "6       Volatility                    Average True Range (ATR, 14-day)   \n",
              "7       Volatility                                Daily High-Low Range   \n",
              "8       Volatility  Normalized Bollinger Band Width (20-day, 2 StdDev)   \n",
              "9        Liquidity                Volume Rate of Change (VROC, 10-day)   \n",
              "10       Liquidity                                      Trading Volume   \n",
              "11       Technical                                 Moving Average (MA)   \n",
              "12       Technical                     Exponential Moving Average (MA)   \n",
              "13       Technical                       Relative Strength Index (RSI)   \n",
              "\n",
              "                                                                                                code  \n",
              "0                                                    ((CLOSE - DELAY(CLOSE, 10)) / DELAY(CLOSE, 10))  \n",
              "1                                                                  (SMA(CLOSE, 10) - SMA(CLOSE, 50))  \n",
              "2                                                                         VOLUME - DELAY(VOLUME, 50)  \n",
              "3                                                                          (MEAN(CLOSE, 20) - CLOSE)  \n",
              "4                                                                           (SMA(CLOSE, 20) - CLOSE)  \n",
              "5                                            (CLOSE - MIN(LOW, 14)) / (MAX(HIGH, 14) - MIN(LOW, 14))  \n",
              "6                                                                                          (ATR(14))  \n",
              "7                                                                                       (HIGH - LOW)  \n",
              "8   ((SMA(CLOSE, 20) + 2 * STD(CLOSE, 20)) - (SMA(CLOSE, 20) - 2 * STD(CLOSE, 20))) / SMA(CLOSE, 20)  \n",
              "9                                                                 ((VOLUME / DELAY(VOLUME, 10)) - 1)  \n",
              "10                                                                                            VOLUME  \n",
              "11                                                                                    SMA(CLOSE, 20)  \n",
              "12                                                                                    EMA(CLOSE, 20)  \n",
              "13                                                                                           RSI(14)  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "original_combined_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_combined_df.to_csv(\"HK_final_comprehensive_result_4.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
