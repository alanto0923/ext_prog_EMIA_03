AlphaAgent: LLM-Driven Alpha Mining with Regularized
Exploration to Counteract Alpha Decay
Ziyi Tang
Sun Yat-sen University
Guangzhou, ChinaZechuan Chen
Sun Yat-sen University
Guangzhou, ChinaJiarui Yang
Sun Yat-sen University
Guangzhou, China
Jiayao Mai
University of New South Wales
Sydney, AustraliaYongsen Zheng
Nanyang Technological University
50 Nanyang Avenue , SingaporeKeze Wang
Sun Yat-sen University
Guangzhou, China
Jinrui Chen
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, ChinaLiang Lin
Sun Yat-sen University
Guangzhou, China
Abstract
Alpha mining, a critical component in quantitative investment, fo-
cuses on discovering predictive signals for future asset returns in
increasingly complex financial markets. However, the pervasive
issue of alpha decayâ€”where factors lose their predictive power
over timeâ€”poses a significant challenge for alpha mining. Tra-
ditional methods like genetic programming face rapid alpha de-
cay from overfitting and complexity, while approaches driven by
Large Language Models (LLMs), despite their promise, often rely
too heavily on existing knowledge, creating homogeneous factors
that worsen crowding and accelerate decay. To address this chal-
lenge, we propose AlphaAgent, an autonomous framework that
effectively integrates LLM agents with ad hoc regularizations for
mining decay-resistant alpha factors. AlphaAgent employs three
key mechanisms: (i) originality enforcement through a similarity
measure based on abstract syntax trees (ASTs) against existing
alphas, (ii) hypothesisâ€”factor alignment via LLM-evaluated seman-
tic consistency between market hypotheses and generated factors,
and (iii) complexity control via AST-based structural constraints,
preventing over-engineered constructions that are prone to over-
fitting. These mechanisms collectively guide the alpha generation
process to balance originality, financial rationale, and adaptability
to evolving market conditions, mitigating the risk of alpha decay.
Extensive evaluations show that AlphaAgent outperforms tradi-
tional and LLM-based methods in mitigating alpha decay across
bull and bear markets, consistently delivering significant alpha in
Chinese CSI 500 and U.S. S&P 500 markets over the past four years.
Notably, AlphaAgent showcases remarkable resistance to alpha
decay, elevating the potential for yielding powerful factors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
Conferenceâ€™17, July 2017, Washington, DC, USA
Â©2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-XXXX-X/18/06
https://doi.org/XXXXXXX.XXXXXXXCCS Concepts
â€¢Computing methodologies â†’Multi-agent planning ;Natural
language generation ;â€¢Applied computing â†’Forecasting .
Keywords
Alpha Mining, Quantitative Investment, Large Language Models,
Autonomous Agents
ACM Reference Format:
Ziyi Tang, Zechuan Chen, Jiarui Yang, Jiayao Mai, Yongsen Zheng, Keze
Wang, Jinrui Chen, and Liang Lin. 2025. AlphaAgent: LLM-Driven Alpha
Mining with Regularized Exploration to Counteract Alpha Decay. In .ACM,
New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX
1 Introduction
Factor investing is a key strategy in modern quantitative finance,
focusing on the systematic identification and exploitation of alpha
factors (a.k.a., alphas) â€” quantifiable characteristics that can pre-
dict asset returns. However, alpha decay, or the decline of factor
returns over time, arises from two main challenges. First, overfitting
through excessive data mining ("p-hacking") leads to the emergence
of spurious factors that appear significant in backtests but decay
rapidly in real-world applications [ 10]. Second, factor crowding
occurs when too many investors adopt similar strategies, which can
accelerate alpha decay and trigger sudden reversals during market
stress [ 2,5,9]. This was evident in early 2024 with the size factorâ€™s
underperformance in Chinaâ€™s A-share market [ 6,34], highlighting
the risks of concentrated positioning in popular factors. Thus, it is
rather crucial to counteract alpha decay during alpha mining.
Traditional methods for alpha mining mainly build on genetic
programming (GP) [ 7,18,21,22,37,40] and reinforcement learn-
ing (RL). However, they struggle to effectively address the alpha
decay challenge. Traditional GP and RL approaches tend to over-
emphasize the optimization of historical performance metrics while
neglecting the underlying financial and economic rationale. With-
out sufficient consideration of financial soundness and economic
intuition, these methods often produce alpha factors that show
strong historical performance but experience rapid alpha decayarXiv:2502.16789v1  [cs.CE]  24 Feb 2025Conferenceâ€™17, July 2017, Washington, DC, USA Trovato et al.
when deployed in live markets. Large Language Models (LLMs)
offer promising potential in factor mining due to their extensive
understanding of financial knowledge, which could help generate
and evaluate factors beyond pure statistical significance. Despite
their versatility, LLMsâ€™ direct application to alpha factor mining
remains underutilized, revealing significant challenges in address-
ing alpha decay. The fundamental limitation lies in the lack of
effective constraints in current LLM-based factor mining frame-
works. Without proper theoretical and empirical constraints to
guide factor mining, LLMs tend to overly rely on well-documented
financial knowledge and established factors during construction,
such as traditional technical indicators (e.g., RSI [ 28]) and widely
studied market anomalies (e.g., momentum, value, and size effects).
This constrained-free approach exacerbates factor crowding - LLMs
predominantly generate factors that capture the same market in-
efficiencies as existing ones, which are already heavily exploited
by market participants. In rapidly evolving markets like the U.S.
stock market, LLM-generated factors thus fail to identify novel
sources of alpha, leading to suboptimal investment performance.
This highlights the need for innovative approaches that constrain
LLMsâ€™ factor generation process by encouraging the exploration
of unique factor constructions that balance theoretical soundness
with market adaptability.
To address these issues, we propose a novel paradigm that ef-
fectively constrains LLM-based factor generation to mitigate alpha
decay, addressing key limitations of conventional approaches. At
its core, AlphaAgent introduces three critical regularization mecha-
nisms to guide LLM-based factor generation: (1) complexity control
through symbolic expression trees and parameter counting, (2)
hypothesis alignment via LLM-evaluated semantic consistency be-
tween market hypotheses and generated factors, and (3) novelty
enforcement through a similarity measure based on abstract syn-
tax trees (ASTs) against existing alpha libraries (e.g., Alpha101).
AlphaAgent formalizes factor construction through an operator
library and abstract syntax trees (ASTs), implementing a pairwise
subtree isomorphism detection mechanism to quantify factor orig-
inality while using LLMs to verify financial intuition alignment
through consistency scoring between hypotheses, descriptions, and
expressions. These constraints guide LLMs to explore novel market
inefficiencies while maintaining theoretical soundness, alleviating
the alpha decay challenge. Based on these constraints, AlphaA-
gent builds an autonomous workflow encompassing hypothesis
proposal, factor construction, factor development, backtesting, and
feedback mechanisms. The framework begins with the idea agent
for the hypothesis proposal, with the first market insight provided
by domain experts. Grounded in these hypotheses, the factor agent
constructs parsimonious and original factors to explore unexploited
market inefficiencies, applying regularization mechanisms to bal-
ance complexity, novelty, and hypothesis alignment. The eval agent
then rigorously validates factorsâ€™ executability and numerical sta-
bility while backtesting assesses their predictive effectiveness on
historical data. The feedback mechanism evaluates performance
metrics and theoretical soundness, guiding iterative refinement.
This closed-loop process progressively derives a family of factors
that capture emerging rather than overcrowded market inefficien-
cies, promoting alpha mining by balancing theoretical soundness
with factor originality.Extensive experiments demonstrate AlphaAgentâ€™s effectiveness
in generating factors resistant to alpha decay. Through comprehen-
sive evaluations in the Chinese CSI 500 and U.S. S&P 500 markets
from January 2021 to December 2024, our framework demonstrates
stable factor performance across different market regimes. Through
comprehensive evaluations in the Chinese CSI 500 and U.S. S&P
500 markets from January 2021 to December 2024, our framework
achieves an average annual excess return of 11.0% (IR=1.5) and 8.74%
(IR=1.05) respectively after accounting for transaction costs, while
maintaining robust performance across different market regimes.
While traditional factors exhibit substantial decay in predictive
power due to market crowding and overfitting, AlphaAgentâ€™s al-
phas maintain stable predictive effectiveness throughout the past 5
years, demonstrating stronger resistance to alpha decay. The frame-
work also displays efficiency in mining alphas, achieving an 81%
higher effective factor ratio (hit ratio) while consuming 30% fewer
tokens. These results substantially outperform traditional alpha
mining approaches and existing LLM-driven ones in terms of both
performance and performance persistence.
Our main contributions can be summarized as follows:
â€¢We propose a systematic regularization mechanism to coun-
teract alpha decay, combining originality enforcement, hy-
pothesis alignment, and complexity control, facilitating the
exploration of original and theoretically grounded alphas.
â€¢We implement a closed-loop multi-agent framework that
evolves alphas with three agents that iteratively perform
hypothesis generation, factor construction, and evaluation.
â€¢Extensive experiments reveal AlphaAgentâ€™s superior perfor-
mance and resilience against alpha decay, attaining an 81%
improvement in hit ratio.
2 Related Work
Alpha factors, or mathematical expressions designed to predict fu-
ture asset returns, have been a central focus in quantitative finance
since Fama and Frenchâ€™s pioneering work on their three-factor
model [ 10]. Traditional methods for alpha mining primarily rely on
genetic programming (GP) for exploring the vast search space of fac-
tor formulation [ 18,21,22,40], yet they struggle to generate factors
resistant to alpha decay. AlphaEvolve [ 7] enriches traditional GP
by incorporating parameter learning and matrix operations while
maintaining GPâ€™s explicit formula structure. [ 24] strengthens GP
with sparsity constraints during mutation, which guides the search
toward alpha factors with lower complexity. Another branch of tra-
ditional alpha factor mining relies on Reinforcement learning (RL)
to optimize factor formulation through policy learning [ 19,25,37].
AlphaGen [ 37] mines formulaic alphas with deep reinforcement
learning, using combination model performance as a reward signal
to guide exploration within the alpha factor search space. Shi et
al. [25] propose an RL-based framework that simultaneously discov-
ers and combines multiple alpha factors with fixed weights to form
a unified signal. Building upon PPO [ 23], an RL-based approach [ 39]
is implemented that discards the critic network and introduces a
reward-shaping mechanism aiming to generate more profitable
and stable alphas for quantitative investment. However, these tra-
ditional paradigms face significant challenges in addressing alphaAlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay Conferenceâ€™17, July 2017, Washington, DC, USA
decay. Both GP and RL approaches tend to over-emphasize histor-
ical performance optimization, leading to either overly complex,
overfitted factors or factors lacking economic rationale [ 24,25,39].
These limitations result in rapid alpha decay when factors are de-
ployed in live markets.
Recent advances in Large Language Models (LLMs) offer a promis-
ing direction to address the alpha decay challenge, as LLMs excel
at capturing evolving patterns and incorporating domain knowl-
edge to generate more resilient factors [ 13,26,31,33,41]. While
numerous studies have explored alpha mining using LLMs, the
critical issue of alpha decay has received limited attention. AutoAl-
pha [ 38] introduces an adaptive factor generation framework that
continuously evolves alphas based on recent market conditions.
LLMFactor [ 30] leverages knowledge-guided prompting to extract
economically interpretable factors from financial news and histor-
ical data. FAMA [ 16] further advances this direction by dynamic
factor combination and cross-sample selection, enabling perfor-
mance across different market regimes. RD-Agent [ 35] proposes a
data-centric feedback loop that allows continuous factor adapta-
tion to changing market conditions. However, existing approaches
still suffer from alpha decay as they lack effective regularization
mechanisms to prevent factors from overly relying on historical
patterns and existing market knowledge.
3 AlphaAgent
3.1 Problem Formulation
The alpha mining task considers a set of stocks S={ð‘ 1,...,ð‘ ð‘}, a
time windowT={ð‘¡1,...,ð‘¡ð‘‡}, and a feature matrix XâˆˆRð‘Ã—ð‘‡Ã—ð·,
whereð·denotes the dimension of the raw features. The objective
of alpha mining is to learn an alpha factor (or alpha) ð‘“that maps
a slice of input features Xð‘¡to a predictive signal ð‘Ÿð‘¡+1, namely the
subsequent return. Formally, an alpha can be written as ð‘“(Xð‘¡)â†’
ð‘Ÿð‘¡+1, whereð‘Ÿð‘¡+1is the return on the day ð‘¡+1. The alpha factor
mining problem can be formulated as an optimization task:
ð‘“âˆ—=arg max
ð‘“âˆˆFL ð‘“(X),yâˆ’ðœ†R(ð‘“), (1)
whereFdenotes the space of all possible factor expressions, yrep-
resents the ground-truth future returns (e.g., next-day returns), L
measures predictive effectiveness (such as the information ratio or
other performance metrics), Ris a regularization term encouraging
simplicity or novelty of the factor expression, and ðœ†is a balancing
parameter that trades off between performance and complexity.
Distinct from conventional pure data-driven methods, we pro-
pose to leverage Large Language Models (LLMs) as intelligent
agents for alpha factor generation. Traditional methods often strug-
gle to incorporate domain expertise effectively or tend to generate
factors that lack economic intuition. LLMs, with their strong natu-
ral language understanding and reasoning capabilities [ 27,31,41],
offer a promising solution by being able to comprehend and oper-
ationalize human market insights. However, LLMs are inherently
intractable due to their stochastic nature and limited ability to ex-
tract key information from lengthy contexts, which could lead to
inconsistent or irrelevant factor generation. To address these limita-
tions, we introduce two critical aspects in the regularization term to
ensure both the practical relevance and long-term effectiveness of
the generated factors. Specifically, we introduce market hypothesesâ„ŽâˆˆH to guide the LLM-based factor construction process with
domain-relevant insights (e.g., candlestick patterns, fundamental
analysis results, market microstructure theories), and we ensure
the generated factors maintain sufficient novelty. Concretely, we
reformulate the above objective as:
ð‘“âˆ—=arg max
ð‘“âˆˆFL ð‘“(X),yâˆ’ðœ†Rð‘”(ð‘“,â„Ž), (2)
where the regularization term Rð‘”(ð‘“,â„Ž)encompasses three com-
ponents: (1) the complexity of the factor expression, (2) the align-
ment between the factor and a market hypothesis â„ŽâˆˆH grounded
in external knowledge (e.g. from domain experts), and (3) the dis-
tinctiveness of the generated factor from existing ones. By incorpo-
rating these aspects into the regularization term, we ensure that
the LLM-generated factors not only maintain theoretical relevance
and practical interpretability through market hypotheses but also
exhibit sufficient novelty to potentially capture unexploited market
inefficiencies. Given the non-convex nature of the optimization
objective, we employ an alternating optimization strategy between
LandRð‘”to ensure convergence to a local minimum. The alter-
nating procedure continues until finding a locally optimal alphas
that balances predictive performance with the regularization con-
straints. This objective provides a flexible mechanism to balance
predictive ability, domain soundness, and factor uniqueness, ulti-
mately contributing to the long-term effectiveness of alpha factors.
The detailed formulation of Rð‘”(ð‘“,â„Ž)is delineated in Section 3.2.
3.2 Factor Generation Modeling
To operationalize the objective function defined in Eq. 2, a factor im-
plementation mechanism is required that ensures both robustness
and alignment with domain hypotheses. However, the inconsistent
quality of LLM-generated outputs poses significant challenges in
code-based factor construction. Approaches that generate code-
based factors may frequently encounter operational barriers such
as data format incompatibilities, inconsistencies across package
versions, and difficulties in maintaining semantic coherence in ex-
tended code implementations. These challenges create a fundamen-
tal tension between code executability and semantic consistency,
requiring LLMs to constantly balance these competing objectives
in factor generation.
3.2.1 Factor Parsing with Abstract Syntax Trees. To address these
limitations, we introduce an operator libraryOthat abstracts and
standardizes various mathematical and financial operations (e.g.,
rolling minima/maxima, moving averages, conditional checks). This
abstraction layer significantly streamlines the factor construction
process by providing LLMs with a consistent and well-defined set
of operations, thereby simplifying the semantic alignment between
operator compositions and market hypotheses. The Operator Li-
brary serves as an intermediate representation that bridges the gap
between high-level market insights and low-level implementation
details, enabling more robust and maintainable factor generation.
We define a parsing procedure:
G: H,Xâ†’ F, (3)
whereHrepresents the space of market hypotheses, with each
hypothesis often described in a semi-structured form (refer to Sec.
3.3).Xdenotes the space of raw features. The output space FConferenceâ€™17, July 2017, Washington, DC, USA Trovato et al.
Performance Metrics: Comprehensive 
assessment of return performance and 
risk exposure with tailored 
recommendations from EvalAgentInitial Idea: A number of market 
hypotheses generated by Idea Agent
{â„Ž1, â„Ž2,...,â„Žï¿½}
Factor Zoo: The k-th round factor 
generated by the Factor Agent 
{ï¿½1, ï¿½2,...,ï¿½ï¿½}
Human 
Knowledge
Research 
Report
Market 
Insight
Idea AgentThe Hypothesis: ... calculating the difference 
between the current price and the recent 
lowest or highest prices relative to volume ...
The Reason: ... utilizing price differentials 
and considering recent price movements for 
factor generation ...
Observation: ... Considering the time span 
from recent high/low prices to the present 
adds a temporal element to the analysis ...
Backtest
 Analysis Feedback
Knowledge gained after practice: ... it can potentially capture 
the momentum and volatility of the instrument ...
Eval AgentSelf-reflection
Factor AgentDescription: "This factor detects potential 
ascemding triangie canmdiestick petems in 
ohlc data, a bulish comntimation patem ..."
Formula: "ATF = ($low > TS_MIN($low, 10)) 
&& ($high == TS_MAX($high, 10)) ? 1 : 0"
Alpha 
Expression
Hypothesis Alignment
+
Complexity Constraints
Figure 1: The autonomous workflow of AlphaAgent, where three agents work collectively to mine alphas that balance financial
rationale, originality, and adaptability to evolving market conditions, counteract the risk of alpha decay in alpha mining tasks.
contains tree-structured factors through symbolic assembly from
atomic operators in O, where symbolic assembly defines the process
of composing and binding operators into factor expressions. By
referring toX, each factor can be bound to real data fields such as
$price ,$volume , and derivatives thereof.
We parse the textual hypothesis â„Žto a factorð‘“âˆˆF as an ab-
stract syntax tree (AST), denoted as ð‘‡(ð‘“), via the following steps:
(1) Identify key phrases in â„Ž(e.g., triangle patternâ€ ,breakoutâ€ ) and
map them to relevant operators in O; (2) Assign numeric parame-
ters (e.g., window size, threshold) for each operator based on â„Žor
default domain values; (3) Assemble the operators into an abstract
syntax tree ð‘‡(ð‘“), that captures the computational dependencies
and execution flow of the factor expression. In ð‘‡(ð‘“), leaf nodes
correspond to raw feature references (e.g., $high ,$low ), internal
nodes represent operator instances (e.g., TS_MIN(.) ,SMA(.) ), and
edges indicate the data flow between operations.
3.2.2 Interpretability and Complexity Control. Although objective
(1)focuses on maximizing predictive quality subject to domain
alignment, it is equally crucial to control the complexity of any
candidate expression. We incorporate a regularization term Rð‘”(ð‘“)
that penalizes overly complex syntax trees or large numbers of
free hyperparameters. This ensures that the final solution not only
adheres to the economic rationale encoded in â„Žbut also remains
interpretable and robust. For instance, we may define
Rð‘”(ð‘“,â„Ž)=ð›¼1Â·SL(ð‘“) +ð›¼2Â·PC(ð‘“) +ð›¼3Â·ER(ð‘“,â„Ž),(4)
where SL(ð‘“)measures symbolic length, PC(ð‘“)counts free parame-
ters (e.g., window lengths), and ð¸ð‘…(ð‘“,â„Ž)captures both the factorâ€™s
novelty compared to existing alpha factors and its alignment with
the given market hypothesis â„Ž. By tuning the weighting coeffi-
cients{ð›¼1,ð›¼2,ð›¼3}, we can obtain parsimonious yet powerful factor
specifications.
To quantitatively assess the originality of proposed alpha factors
and detect potential duplicates, we introduce a pair-wise factor
similarity metric based on AST matching. For any given factor ð‘“ð‘–,we first parse its expression into an AST representation ð‘‡(ð‘“ð‘–). To
compute the similarity between two ASTs ð‘“ð‘–andð‘“ð‘—, we identify
their largest common subtree by recursively comparing their AST
structuresð‘‡(ð‘“ð‘–)andð‘‡(ð‘“ð‘—). The similarity metric ð‘ is calculated as:
ð‘ (ð‘“ð‘–,ð‘“ð‘—)= max
ð‘¡ð‘–âŠ†ð‘‡(ð‘“ð‘–),ð‘¡ð‘—âŠ†ð‘‡(ð‘“ð‘—){|ð‘¡ð‘–|:ð‘¡ð‘–ð‘¡ð‘—},(5)
whereð‘¡ð‘–andð‘¡ð‘—are subtrees of ð‘‡(ð‘“ð‘–)andð‘‡(ð‘“ð‘—)respectively,|ð‘¡ð‘–|de-
notes the size of the subtree (number of nodes), and ð‘¡ð‘–ð‘¡ð‘—indicates
structural isomorphism between subtrees. With this similarity met-
ric, a newly proposed factor can be compared with existing alphas
that have been widely validated (see Fig. 2), such as Alpha101 [ 14].
Formally, we compute its maximum similarity score against an
existing alpha zoo Z={ðœ™1,ðœ™2,...,ðœ™ð‘}, providing a quantitative
measure of the factorâ€™s originality, written as:
ð‘†(ð‘“)=max
ðœ™âˆˆZð‘ (ð‘“,ðœ™),(6)
To ensure the semantic consistency between market hypotheses
and generated factors, we employ LLMs to evaluate two critical
alignments: (1) whether the factor description aligns with the mar-
ket hypothesis as a valid implementation, and (2) whether the factor
expression accurately reflects its description. For a given hypothe-
sisâ„Ž, factor description ð‘‘, and factor expression ð‘“, we formulate a
consistency scoring function:
C(â„Ž,ð‘‘,ð‘“)=ð‘1(â„Ž,ð‘‘)+ð‘2(ð‘‘,ð‘“), (7)
whereð‘1(â„Ž,ð‘‘)âˆˆ[ 0,1]evaluates whether the factor description ð‘‘
represents a valid implementation of hypothesis â„Ž,ð‘2(ð‘‘,ð‘“)âˆˆ[ 0,1]
measures the consistency between the factor description and its
mathematical expression, and ð›¼is a weighting parameter. For ex-
ample, if a factor claims to capture market liquidity dynamics in
its description but its expression contains no liquidity-related com-
ponents (such as trading volume, bid-ask spread, or market depth),
it would receive a low ð‘2score, indicating potential misalignment
between the claimed economic intuition and actual implementation.
This scoring mechanism helps filter out factors that either deviateAlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay Conferenceâ€™17, July 2017, Washington, DC, USA
Figure 2: This figure shows the factor ð‘“and an alpha zoo Z
each represented as an expression or AST whose leaf nodes
are depicted as light blue. The factorâ€™s originality score is cal-
culated by the maximum size of common subtrees between
its AST and each factor within the alpha zoo.
from the original market insight or contain mismatches between
their semantic meaning and mathematical formulation, thereby
reducing the risk of spurious factor generation.
Based on the above similarity metric and consistency evaluation,
we can now formulate ER(ð‘“,â„Ž)that quantifies both the originality
and hypothesis alignment of a generated factor:
ER(ð‘“,â„Ž)=ð›½1Â·ð‘†(ð‘“)+ð›½2Â·C(â„Ž,ð‘‘,ð‘“)+ð›½3Â·log(1+|Fð‘“|)
=max
ðœ™âˆˆZð‘†(ð‘“,ðœ™)+ð‘1(â„Ž,ð‘‘)+ð‘2(ð‘‘,ð‘“)+log(1+|Fð‘“|),(8)
whereð›½1,ð›½2,ð›½3are weighting coefficients, Fð‘“represents the set of
raw features used in factor ð‘“â€™s expression, and the logarithmic term
penalizes excessive feature usage to promote factor parsimony. A
lower ERscore indicates better factor quality, with the first term
penalizing similarity to existing factors, the second term ensuring
hypothesis alignment, and the third term controlling expression
complexity. By normalizing ð‘†(ð‘“,ðœ™)to[0,1]through appropriate
scaling, all terms in the equation become comparable in magnitude.
Through the above factor generation process, each candidate ð‘“
is guaranteed to satisfy the originality, hypothesis alignment, and
complexity constraints, while optional refinement heuristics can
iteratively enhance or simplify expressions. The overall framework,
described in subsequent sections, integrates eval agent for factor
evaluation, gathers feedback through reflective analysis, and ul-
timately establishes an autonomous framework to continuously
refine and uncover alpha factors that counteract alpha decay.
3.3 Autonomous Multi-Agent Framework
As illustrated in Figure 1, AlphaAgent implements a recurrent
framework for alpha factor mining through three specialized agentspowered by large language models (LLMs): idea agent ,factor agent ,
and eval agent . The idea agent synthesizes market hypotheses â„Ž
by integrating human knowledge, research reports, and market
insights. Each hypothesis captures a potential market inefficiency
pattern, such as value-momentum dynamics to behavioral biases
and market structure anomalies [ 2,14]. Next, the factor agent trans-
lates these hypotheses into factor expressions that capture their
underlying market dynamics. For each hypothesis â„Ž, the factor agent
generates multiple candidate implementations to quantify differ-
ent aspects of the hypothesized inefficiency, including a natural
language description of the factor logic and corresponding math-
ematical expressions using structural operators. The eval agent
evaluates them through backtesting on historical data, in-depth
search for similar existing factors, and analysis of performance
metrics. Performance feedback from the eval agent guides the next
iteration of hypothesis refinement and factor construction, forming
a closed loop for continuous alpha mining.
Idea Agent. Theidea agent serves as the foundation of our frame-
work by formalizing market hypotheses through a structured knowl-
edge integration process. Drawing from external knowledge, the
idea agent employs the chain-of-thought reasoning [ 27,32] to gener-
ate a market hypothesis with a systematic structure encompassing
five interconnected components: (1) knowledge synthesizes estab-
lished financial theories (e.g., market efficiency, behavioral finance),
empirical market intuitions (e.g., momentum, mean reversion), and
practitionersâ€™ conjectures derived from trading experience; (2) mar-
ket observations that provide empirical grounding through analysis
of current market conditions and emerging patterns; (3) justification
that establishes theoretical soundness by linking observed patterns
to underlying economic mechanisms; (4) hypothesis specifies tem-
poral characteristics of the hypothesized market behavior, encom-
passing pattern-driven triggers (e.g., â€œascending triangle breakoutâ€),
volume-price dynamics (e.g., â€œrising volume with price consolida-
tionâ€), and their anticipated market implications; and (5) specifica-
tionoutlines implementation constraints, such as optional numeric
or time-window parameters (e.g., â€œ10-day high/lowâ€). In the ini-
tialization phase, the idea agent generates the seed hypothesis â„Ž0
based on expert-provided research directions and market insights.
This initial hypothesis serves as an anchor point for the iterative
refinement process, where subsequent hypotheses are generated
through feedback from the eval agent . Through this structured ap-
proach, the idea agent constructs market hypotheses that unite
theoretical rigor with empirical validity, establishing a foundation
for systematic alpha discovery.
Factor Agent. The factor agent serves as the bridge between the-
oretical market hypotheses and their quantitative manifestations,
crafting alpha factor implementations through the regulated pro-
cess outlined in Section 3.2. To enhance factor quality, the agent
maintains an evolving knowledge base of both successful and failed
factor implementations. Failed cases are categorized based on their
failure modes, such as hypothesis misalignment and structural com-
plexity violations. These failure cases are then encoded into the
agentâ€™s knowledge base, allowing it to proactively avoid similar
pitfalls in subsequent iterations. The agent employs a multi-stageConferenceâ€™17, July 2017, Washington, DC, USA Trovato et al.
refinement pipeline: first generating multiple candidate implemen-
tations for each hypothesis, then applying filters based on complex-
ity and alignment metrics. During generation, the agent optimizes
a group of alpha factors by referencing similar historical cases from
its knowledge base, until they satisfy the originality, hypothesis
alignment, and complexity constraints. This experiential learning
mechanism enables the agent to continuously improve its genera-
tion capabilities, producing a variety of original, well-aligned, and
parsimonious factor implementations over time.
Eval Agent. The eval agent first conducts a multi-dimensional
evaluation of generated factors through a backtesting system. The
evaluation process encompasses three primary aspects: predictive
capability metrics that measure the factorâ€™s forecasting effective-
ness, return performance metrics that assess the factorâ€™s profit-
generating ability, and risk control metrics that evaluate its stability
and robustness under various market conditions. Beyond quanti-
tative assessment, the agent maintains an evaluation history to
track factorsâ€™ performance and identify emerging patterns in both
successes and failures. This accumulated evaluation knowledge is
systematically fed back to the idea agent , enabling its hypothesis
refinement. In this sense, the evaluation process not only validates
individual factors but also continuously provides insights for the
next round, forming a closed-loop mechanism to continuously op-
timize the overall alpha mining.
4 Experiments
4.1 Experiment Settings
4.1.1 Metrics. This study focuses on evaluating how AlphaAgent
counteracts the alpha decay challenge against established base-
line approaches using a comprehensive set of financial metrics.
The Information Coefficient (IC) and Rank Information Coefficient
(RankIC) measure forecasting precision through the correlation be-
tween predicted scores and actual returns, with IC using raw values
and RankIC using ranked values. Risk-adjusted performance indi-
cators include the Information Coefficient Information Ratio (ICIR),
which evaluates the consistency of IC performance by compar-
ing its mean to standard deviation, and the Information Ratio (IR),
which measures risk-adjusted excess returns relative to a bench-
mark. For absolute performance assessment, we use the Annualized
Return (AR) to quantify the yearly investment outcome normalized
from cumulative returns, and the Maximum Drawdown (MDD)
to capture the largest peak-to-trough decline in portfolio value.
With these multi-faceted evaluation metrics, we can assess both the
factorâ€™s long-term effectiveness and its resistance to alpha decay,
as persistent IC/RankIC values and stable ICIR indicate sustained
predictive power, while AR and MDD metrics reveal the practical
impact of any deterioration in the factorâ€™s effectiveness over time.
4.1.2 Backtest Settings. The backtesting experiments were con-
ducted using the Qlib framework [ 36], on CSI 500 of the Chinese
A-share market and S&P 500 of the U.S. stock market, spanning
2021 to 2024. Raw data employed to construct alpha factors include
only OHLCV (i.e., $open, $high, $low, $close, $volume). CSI 500
data is collected from Baostock [ 3], while S&P 500 data is from
Yahoo Finance [1]. See Table 1 for precise dataset splits.Asset Period Trading Days
Training: 2015-01 to 2019-12 1258
S&P 500 Validation: 2020-01 to 2020-12 253
Testing: 2021-01 to 2024-12 1004
Training: 2015-01 to 2019-12 1219
CSI 500 Validation: 2020-01 to 2020-12 243
Testing: 2021-01 to 2024-12 968
Table 1: Periods of Training, validation, and testing splits,
with their trading days for S&P 500 and CSI 500.
AlphaAgent employs GPT-3.5-turbo as the foundational LLM
to support agents. For RD-Agent [ 4], following its authors, GPT-4-
turbo is used. Four fundamental alphas, including intra-day return ,
daily return ,20-day relative volume , and normalized daily range ,
serve as the base alphas and will be concatenated with newly pro-
posed alphas to train a LightGBM model [ 15]. Before being fed
into LightGBM, both features and returns undergo cross-sectional
Z-score normalization to ensure comparability across stocks. The
LightGBM model, with a maximum depth of 4 layers, is responsible
for forecasting the next-day returns. In backtesting, a top- ð‘˜dropout
strategy is employed to select the 50 top-ranked stocks based on
the predicted returns and exclude the 5 lowest-ranked stocks. All
backtesting results account for transaction fees. For CSI 500, the
transaction fees are set at 0.0005 for buying and 0.0015 for selling.
For S&P 500, only selling fees are applied, with a rate of 0.0005.
4.1.3 Baselines. We compare AlphaAgent with several baseline
approaches: (1) Traditional time-series forecasting models includ-
ingLSTM [12] for capturing temporal dependencies and Trans-
former [29] for parallel sequence processing; (2) Tree-based model
LightGBM [15] for handling structured financial data; (3) Special-
ized financial models including StockMixer [11] and TRA [17],
which focus on integrating multiple trading strategies and handling
non-i.i.d. market patterns respectively; (4) Agent-based approaches
including AlphaForge [25] and RD-Agent [4], which leverage
deep learning and LLMs for alpha factor generation and optimiza-
tion; (5) Deep reasoning models, OpenAI- o1[20] and DeepSeek-
R1[8].
4.2 Overall Performance
In Table 2, we show a comparison of the overall performance of the
different methods for four years, from January 1, 2021, to December
31, 2024, in the CSI 500 (China) and S&P 500 (U.S.) stock markets.
Regarding AlphaForge, RD-Agent, Deepseek-R1, OpenAI-o1, and
AlphaAgent, we apply their respective output alphas directly in
this comparative analysis. For reasoning models Deepseek-R1 and
OpenAI-o1, we streamline the task prompt of AlphaAgent to facili-
tate alpha mining and evolution across five iterative rounds, with
each round providing corresponding backtesting results using the
identical configuration of AlphaAgent. For RD-Agent and AlphaA-
gent, we conduct 20 independent trials, with each trial comprising
five evolutionary rounds. In each trial, we insert the optimal factors
into their respective alpha zoos, yielding the optimal combination
as the final result. The performance of each method is evaluated by
five key metrics: IC, ICIR, AR, IR, and MDD. Bold numbers indicateAlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay Conferenceâ€™17, July 2017, Washington, DC, USA
Table 2: Performance Comparison of Different Methods on CSI 500 (China) and S&P 500 (U.S.). Bold and underlined numbers
represent the best and second-best performance across all compared approaches, respectively.
Market CSI 500 (2021-01-01 to 2024-12-31) S&P 500 (2021-01-01 to 2024-12-31)
Method IC ICIR AR IR MDD IC ICIR AR IR MDD
LSTM [12] 0.0175 0.1521 4.96% 0.6225 -9.68% 0.0028 0.0181 -1.51% -0.1671 -26.05%
Transformer [29] 0.0131 0.1074 4.11% 0.5074 -17.45% 0.0013 0.0129 -4.55% -0.4964 -34.96%
LightGBM [15] 0.0120 0.1209 -1.18% -0.1588 -18.97% 0.0011 0.0116 -2.64% -0.4224 -21.17%
TRA [17] 0.0198 0.1794 2.91% 0.4261 -12.73% -0.0003 -0.0027 -8.51% -1.1345 -49.55%
Stock-Mixer [11] 0.0000 0.0003 -0.35% -0.0496 -16.82% 0.0030 0.0312 -2.49% -0.3342 -29.43%
AlphaForge [25] 0.0146 0.1299 3.45% 0.3270 -17.67% 0.0017 0.0215 2.10% 0.2604 -19.57%
RD-Agent [4] 0.0113 0.0872 0.78% 0.0744 -20.85% 0.0019 0.0123 1.69% 0.1664 -23.18%
DeepSeek- R1[8]best-of-10 0.0132 0.1201 1.58% 0.2086 -14.95% 0.0048 0.0369 2.75% 0.2400 -15.34%
OpenAI- o1[20] best-of-10 0.0159 0.1502 0.46% 0.0632 -21.29% 0.0028 0.0217 2.29% 0.2021 -16.35%
AlphaAgent 0.0212 0.1938 11.00% 1.488 -9.36% 0.0056 0.0552 8.74% 1.0545 -9.10%
2021-01 2021-07 2022-01 2022-07 2023-01 2023-07 2024-01 2024-07 2025-010.1
0.00.10.20.30.4Cumulative Excess Return(a) CSI500
CSI500
Transformer + Alpha158
LightGBM + Alpha158
LSTM + Alpha158
DeepSeek-R1 best-of-10
AlphaAgent
2021-01 2021-07 2022-01 2022-07 2023-01 2023-07 2024-01 2024-07 2025-010.1
0.00.10.20.30.40.5Cumulative Excess Return(b) S&P500
S&P500
Transformer + Alpha158
LightGBM + Alpha158
LSTM + Alpha158
DeepSeek-R1 best-of-10
AlphaAgent
Figure 3: Cumulative Excess Returns of different approaches on CSI 500 and S&P 500.
the best-performing methodology in each dataset, while underlined
numbers represent sub-optimal performance.
Over the four-year period from 2021 to 2024, AlphaAgent con-
sistently outperformed other models across all key metrics in both
markets. For predictive power, it achieved the highest IC (0.0212)
and ICIR (0.1938) in CSI 500 and similarly led in S&P 500 with
IC of 0.0056 and ICIR of 0.0552. In terms of returns, AlphaAgent
generated the best annualized returns of 11.00% in CSI 500 and
8.74% in S&P 500, significantly ahead of the second-best performers
(LSTMâ€™s 4.96% and Deepseek-R1â€™s 2.75% respectively). The model
also demonstrated superior risk management, maintaining MDDs
below 10% in both markets (-9.36% in CSI 500 and -9.10% in S&P
500), while achieving the highest risk-adjusted returns with IRs of
1.488 and 1.0545 respectively.
Figure 3 (a) and (b) illustrate the cumulative excess returns of dif-
ferent models in the CSI 500 and S&P 500 markets from 2021 to 2024,
revealing distinct patterns of alpha decay across different models
and market environments. The time series models (LSTM and Trans-
former) exhibit second-tier performance in the CSI 500 market with
no significant excess returns before 2023 while suffering severe de-
cay in the S&P 500 market with consistently negative returns since
2023-02, particularly evident in Transformerâ€™s notable drawdown
after 2024-08. In contrast, AlphaAgent demonstrates remarkable
resilience in both markets, maintaining relatively stable quarterly
performance with approximately 45% cumulative excess returns
in CSI 500 and exceeding 37% in S&P 500 throughout the testing
period. Non-sequential models like LightGBM with Alpha158 show
complete alpha decay in the S&P 500 market as their cumulativeexcess returns oscillate around zero, while DeepSeek-R1â€™s perfor-
mance declines after 2023 despite its strong reasoning capabilities,
suggesting a lack of systematic constraints. This stark contrast in
alpha persistence between models and markets indicates that main-
taining consistent alpha generation is notably more challenging in
the more efficient U.S. market environment, though AlphaAgent
successfully overcomes this challenge.
4.3 Alpha Decay Analysis
Figure 4 compares the yearly performance between Alpha158 [ 36],
GP [18], a technical indicator RSI, and AlphaAgentâ€™s alphas over 5
years on CSI 500. Alpha158, GP, and RSI all exhibit substantial de-
clines in predictive power, with their ICs dropping from 0.022-0.036
to near zero and RankICs decreasing from 0.020-0.042 to around
zero, highlighting the widespread challenge of alpha decay in the
Chinese stock market. In contrast, AlphaAgentâ€™s alphas demon-
strate remarkable stability, maintaining predictive effectiveness
with IC values consistently around 0.02 and RankIC values around
0.025 throughout the period. This contrast highlights the superior
sustainability of AlphaAgent compared to traditional factors, which
exhibit stronger signs of alpha decay.
This analysis reveals fundamental differences between how tra-
ditional alphas and our approach face modern financial market
challenges. GPâ€™s rapid performance deterioration suggests potential
overfitting to historical patterns, making it particularly vulnerable
to changing market conditions. Meanwhile, the significant perfor-
mance degradation of Alpha158 and RSI Indicator exemplifies the
"alpha decay" phenomenon described in Sec. 1, to a great extent,Conferenceâ€™17, July 2017, Washington, DC, USA Trovato et al.
stemming from market crowding as these strategies become widely
adopted by investors. When multiple market participants simulta-
neously execute similar trading strategies based on the same factors,
their collective actions can diminish the factorsâ€™ predictive power.
In contrast, AlphaAgentâ€™s sustained performance suggests that its
factor modeling mechanism works to explore sustained and less
exploited alphas, effectively mitigating the crowding effect that
plagues traditional approaches.
2020 2021 2022 2023 2024
Year0.005
0.0000.0050.0100.0150.0200.0250.0300.0350.040
(a) Yearly IC
GP
RSI
Alpha158
AlphaAgent
2020 2021 2022 2023 2024
Year0.02
0.01
0.000.010.020.030.040.05
(b) Yearly RankIC
GP
RSI
Alpha158
AlphaAgent
Figure 4: Yearly IC and RankIC comparison on CSI 500. While
other factorsâ€™ predictive power rapidly decays over time, 15
alphas mined by AlphaAgent maintain stable performance.
4.4 Alpha Mining Efficiency Analysis
This subsection conducts an analysis that focuses on the quality
of generated alpha factors and the computational efficiency of the
generation process. Figure 5 illustrates the evolution of IC values
across five rounds for AlphaAgent and RD-Agent on CSI 500â€™s
test split. First, the results show that RD-Agent exhibits relatively
stable and smaller variance (shown by the consistent width of its
shaded region), likely due to its lack of exploration incentives for
LLM-based agents, indicating more homogeneous factor candidates.
Such candidates lead to factor crowding and accelerated alpha decay
as similar signals become widely exploited in the market. On the
contrary, AlphaAgent consistently maintains higher average IC val-
ues compared to RD-Agent throughout all rounds, demonstrating
the superior predictive power of its generated factors gained from
complexity and hypothesis-alignment constraints against overfit-
ting and financial rationality of the generated factors. A notable
observation is an increasing variance (shown by the expanding
shaded region) of AlphaAgentâ€™s IC values as the rounds progress,
suggesting its factor diversity and potentially a wider exploration
space brought by its originality penalties, which could lead to a
higher probability of discovering effective factors. AlphaAgentâ€™s
originality penalties drive broad exploration of the factor space,
while these complementary mechanisms ultimately contribute to
consistently higher average IC values.
4.5 Ablation Study
In Figure 6, we ablate AlphaAgentâ€™s core components, i.e., factor
modeling constraints and symbolic assembly, across three key met-
rics. This evaluation is based on 100 rounds of evolution, evenly
split between the CSI 500 and S&P 500 markets. The hit ratio mea-
sures the proportion of generated alphas achieving exceptional
Round 1 Round 2 Round 3 Round 4 Round 50.01200.01250.01300.01350.01400.01450.01500.0155
AlphaAgent Mean
AlphaAgent Â±1/2 Std. Dev.
RD-Agent Mean
RD-Agent Â±1/2 Std. Dev.Figure 5: The evolution of the IC values in the first five rounds
for AlphaAgent and RD-Agent.
returns per round (>4.0% annualized for CSI 500 and >1.8% for
S&P 500, representing the top 5% of all generated alphas). The dev
success rate captures the percentage of factors that can be suc-
cessfully executed without any code defects or numerical errors.
Token efficiency quantifies the number of viable factor candidates
generated per token, with the higher efficiency normalized to 1.0
while maintaining relative proportions.
Our ablation studies demonstrate AlphaAgentâ€™s effectiveness
across these three metrics. In terms of hit ratio, AlphaAgent achieves
0.29 compared to 0.16 without factor modeling constraints, repre-
senting an 81% improvement. This substantial increase indicates
that incorporating constraints significantly enhances the quality of
generated alpha factors, more likely to mine sustained and predic-
tive alphas. The development success rate comparison (0.83 vs 0.75)
evaluates the impact of symbolic assembly - it releases development
difficulties by reducing potential code defects and numerical errors.
When examining token efficiency, AlphaAgent achieves higher effi-
ciency (1.00) versus 0.81 without symbolic assembly, demonstrating
a 23% improvement in generating viable factor candidates per token.
These results collectively validate that our proposed mechanisms
effectively improve factor generation across multiple dimensions
while relaxing computational overhead.
Hit ratio Dev success rate T oken efficiency0.00.20.40.60.81.0
0.160.750.81
0.290.831.00
AlphaAgent ~ w/o factor constraints ~ w/o symbolic assembly
Figure 6: A comparison of RD-Agent and AlphaAgent regard-
ing hit ratio, development success rate, and token efficiency.
4.6 Implications
The results shown in Sec. 4.3 and Sec. 4.4 underscore a critical in-
sight into modern quantitative finance: the imperative for alpha
mining methods to possess continuous exploration capability be-
yond fitting historical patterns and relying on established theories.
This is particularly evident in todayâ€™s highly efficient markets whereAlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay Conferenceâ€™17, July 2017, Washington, DC, USA
traditional statistical arbitrage strategies face diminishing returns
due to increased competition and market adaptation. Our exper-
iments demonstrate that approaches incorporating active explo-
ration mechanisms can better identify and capitalize on emerging
market inefficiencies before they dissipate in rapidly evolving mar-
kets. With the emergence of LLMs and their growing application
in financial analysis, the landscape of alpha mining is undergoing
fundamental changes, requiring approaches to be more adaptive
and exploratory than ever before. These findings highlight the
need for future quantitative trading research to focus on advanced
exploration frameworks that balance the exploitation of known
patterns with the exploration of novel alpha sources, particularly in
an era where AI-driven market analysis is becoming increasingly
prevalent.
5 Conclusion
This paper introduces AlphaAgent, a novel LLM-driven framework
that effectively counteracts the critical challenge of alpha decay
with three key regularization mechanisms: originality enforcement,
complexity control, and hypothesis alignment. By incorporating
these mechanisms into an autonomous framework, AlphaAgent
produces decay-resistant and performant alpha factors while main-
taining theoretical soundness, achieving substantial excess returns
with remarkable consistency and decay resistance through various
market conditions. The framework also suggests a promising direc-
tion for the next-generation alpha mining framework that swiftly
adapts to market evolution while maintaining alpha sustainability.References
[1]Ran Aroussi. 2024. yfinance: Download market data from Yahoo! Financeâ€™s API.
https://pypi.org/project/yfinance/. Accessed: 2025-01-05.
[2]Clifford S Asness, Tobias J Moskowitz, and Lasse Heje Pedersen. 2013. Value and
momentum everywhere. The journal of finance 68, 3 (2013), 929â€“985.
[3]BaoStock. 2024. A tool for obtaining historical data of China stock market.
https://pypi.org/project/baostock/. Accessed: 2025-01-05.
[4]Haotian Chen, Xinjie Shen, Zeqi Ye, Wenjun Feng, Haoxue Wang, Xiao Yang, Xu
Yang, Weiqing Liu, and Jiang Bian. 2024. Towards Data-Centric Automatic R&D.
arXiv:2404.11276 [cs.AI]
[5]Yiyi Chen and Junheok Cheon. 2024. Alpha Decay in stock market prediction
using LSTM neural networks.
[6]CICC Research. 2024. CICC 2024 H2 Outlook | Quantitative Investment: The
Tide of Dividends May Recede, Awaiting the Rise of Growth. https://finance.
sina.com.cn/stock/stockzmt/2024-06-12/doc-inaymscf1478278.shtml. Accessed:
2025-02-07.
[7]Can Cui, Wei Wang, Meihui Zhang, Gang Chen, Zhaojing Luo, and Beng Chin
Ooi. 2021. AlphaEvolve: A Learning Framework to Discover Novel Alphas
in Quantitative Investment. In Proceedings of the 2021 International Conference
on Management of Data (Virtual Event, China) (SIGMOD â€™21) . Association for
Computing Machinery, New York, NY, USA, 2208â€“2216. https://doi.org/10.1145/
3448016.3457324
[8]DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, and
Ruoyu Zhang et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capabil-
ity in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] https:
//arxiv.org/abs/2501.12948
[9]Antoine Falck, Adam Rej, and David Thesmar. 2022. When do systematic strate-
gies decay? Quantitative Finance 22, 11 (2022), 1955â€“1969.
[10] Eugene F Fama and Kenneth R French. 1992. The cross-section of expected stock
returns. the Journal of Finance 47, 2 (1992), 427â€“465.
[11] Jinyong Fan and Yanyan Shen. 2024. StockMixer: A Simple Yet Strong MLP-Based
Architecture for Stock Price Forecasting. Proceedings of the AAAI Conference on
Artificial Intelligence 38, 8 (Mar. 2024), 8389â€“8397. https://doi.org/10.1609/aaai.
v38i8.28681
[12] Alex Graves and Alex Graves. 2012. Long short-term memory. Supervised sequence
labelling with recurrent neural networks (2012), 37â€“45.
[13] Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2023. Lan-
guage Models Can Teach Themselves to Program Better. In The Eleventh In-
ternational Conference on Learning Representations . https://openreview.net/
forum?id=SaRj2ka1XZ3
[14] Zura Kakushadze. 2016. 101 Formulaic Alphas. arXiv:1601.00991 [q-fin.PM]
https://arxiv.org/abs/1601.00991
[15] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boosting
Decision Tree. In Advances in Neural Information Processing Systems , I. Guyon,
U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_
files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf
[16] Zhiwei Li, Ran Song, Caihong Sun, Wei Xu, Zhengtao Yu, and Ji-Rong Wen.
2024. Can Large Language Models Mine Interpretable Financial Factors More
Effectively? A Neural-Symbolic Factor Mining Agent Model. In Findings of the
Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins,
and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok,
Thailand, 3891â€“3902. https://doi.org/10.18653/v1/2024.findings-acl.233
[17] Hengxu Lin, Dong Zhou, Weiqing Liu, and Jiang Bian. 2021. Learning Multiple
Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport.
InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &
Data Mining (KDD â€™21) . ACM.
[18] X. Lin, Y. Chen, Z. Li, and K. He. 2019. Revisiting Stock Alpha Mining Based
On Genetic Algorithm . Technical Report. Huatai Securities Research Center.
https://crm.htsc.com
[19] Xiao-Yang Liu, Hongyang Yang, Jiechao Gao, and Christina Dan Wang. 2022.
FinRL: deep reinforcement learning framework to automate trading in quantita-
tive finance. In Proceedings of the Second ACM International Conference on AI in
Finance (Virtual Event) (ICAIF â€™21) . Association for Computing Machinery, New
York, NY, USA, Article 1, 9 pages. https://doi.org/10.1145/3490354.3494366
[20] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, and Ahmed
El-Kishky et al. 2024. OpenAI o1 System Card. arXiv:2412.16720 [cs.AI] https:
//arxiv.org/abs/2412.16720
[21] RR Patil. 2023. AI-Infused algorithmic trading: genetic algorithms and machine
learning in high-frequency trading. International Journal for MultidiscMining
profitable alpha factors via convolution kernel learningiplinary Research 5, 5 (2023).
[22] Michael D. Schmidt and Hod Lipson. 2010. Age-fitness pareto optimization. In
Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computa-
tion(Portland, Oregon, USA) (GECCO â€™10) . Association for Computing Machinery,
New York, NY, USA, 543â€“544. https://doi.org/10.1145/1830483.1830584Conferenceâ€™17, July 2017, Washington, DC, USA Trovato et al.
[23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347 [cs.LG]
https://arxiv.org/abs/1707.06347
[24] Zhenyi Shen, Xiahong Mao, Xiaohu Yang, and Dan Zhao. 2023. Mining profitable
alpha factors via convolution kernel learning. Applied Intelligence 53, 23 (2023),
28460â€“28478.
[25] Hao Shi, Weili Song, Xinting Zhang, Jiahe Shi, Cuicui Luo, Xiang Ao, Hamid
Arian, and Luis Seco. 2024. AlphaForge: A Framework to Mine and Dynamically
Combine Formulaic Alpha Factors. arXiv:2406.18394 [q-fin.CP] https://arxiv.
org/abs/2406.18394
[26] Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths.
2024. Cognitive Architectures for Language Agents. arXiv:2309.02427 [cs.AI]
https://arxiv.org/abs/2309.02427
[27] Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen,
and Liang Lin. 2024. Towards CausalGPT: A Multi-Agent Approach for
Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs.
arXiv:2308.11914 [cs.AI] https://arxiv.org/abs/2308.11914
[28] Adrian Å¢Äƒran-MoroÅŸan. 2011. The relative strength index revisited. African
Journal of Business Management 5, 14 (2011), 5855â€“5862.
[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you
need. In Proceedings of the 31st International Conference on Neural Information
Processing Systems (Long Beach, California, USA) (NIPSâ€™17) . Curran Associates
Inc., Red Hook, NY, USA, 6000â€“6010.
[30] Meiyun Wang, Kiyoshi Izumi, and Hiroki Sakaji. 2024. LLMFactor: Extracting
Profitable Factors through Prompts for Explainable Stock Movement Prediction.
arXiv:2406.10811 [cs.CL] https://arxiv.org/abs/2406.10811
[31] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong
Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Aligning Large Language
Models with Human: A Survey. arXiv:2307.12966 [cs.CL] https://arxiv.org/abs/
2307.12966
[32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
Quoc V Le, Denny Zhou, et al .2022. Chain-of-thought prompting elicits reasoning
in large language models. Advances in Neural Information Processing Systems 35
(2022), 24824â€“24837.
[33] Lilian Weng. 2023. LLM-powered Autonomous Agents. lilianweng.github.io (Jun
2023). https://lilianweng.github.io/posts/2023-06-23-agent/
[34] Phillip Wool. 2024. China A-shares Q1 2024 Factor Review . Premia Partners. https:
//www.premia-partners.com/insight/china-a-shares-q1-2024-factor-review
[35] Xu Yang, Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, Xiao
Yang, Shizhao Sun, Weiqing Liu, and Jiang Bian. 2024. Collaborative Evolving
Strategy for Automatic Data-Centric Development. arXiv:2407.18690 [cs.AI]
[36] Xiao Yang, Weiqing Liu, Dong Zhou, Jiang Bian, and Tie-Yan Liu. 2020. Qlib:
An AI-oriented Quantitative Investment Platform. arXiv:2009.11189 [q-fin.GN]
https://arxiv.org/abs/2009.11189
[37] Shuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, and Qing He.
2023. Generating Synergistic Formulaic Alpha Collections via Reinforcement
Learning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining (Long Beach, CA, USA) (KDD â€™23) . Association for
Computing Machinery, New York, NY, USA, 5476â€“5486. https://doi.org/10.1145/
3580305.3599831
[38] Tianping Zhang, Yuanqi Li, Yifei Jin, and Jian Li. 2020. AutoAlpha: an Efficient
Hierarchical Evolutionary Algorithm for Mining Alpha Factors in Quantitative
Investment. arXiv:2002.08245 [q-fin.CP] https://arxiv.org/abs/2002.08245
[39] Junjie Zhao, Chengxi Zhang, Min Qin, and Peng Yang. 2024. QuantFactor REIN-
FORCE: Mining Steady Formulaic Alpha Factors with Variance-bounded REIN-
FORCE. arXiv:2409.05144 [q-fin.CP] https://arxiv.org/abs/2409.05144
[40] Su Zhaofan, Lin Jianwu, and Zhang Chengshan. 2022. Genetic algorithm based
quantitative factors construction. In 2022 IEEE 20th International Conference on
Industrial Informatics (INDIN) . IEEE, 650â€“655.
[41] Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schu-
urmans, and Hanjun Dai. 2024. Large Language Models can Learn Rules.
arXiv:2310.07064 [cs.AI] https://arxiv.org/abs/2310.07064